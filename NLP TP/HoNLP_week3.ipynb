{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f5cb8f6c-94df-4e39-b208-e54edc3a9df6",
      "metadata": {
        "id": "f5cb8f6c-94df-4e39-b208-e54edc3a9df6"
      },
      "source": [
        "# Hands-On NLP â€” Class 3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aaf60f2c",
      "metadata": {
        "id": "aaf60f2c"
      },
      "source": [
        "<span style=\"color:magenta\">Group Names:</span>\n",
        "\n",
        "* Name 1\n",
        "* Name 2\n",
        "* Name 3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e5beddc-b0f9-4cb4-92a7-e1270c541dd0",
      "metadata": {
        "id": "1e5beddc-b0f9-4cb4-92a7-e1270c541dd0"
      },
      "source": [
        "## Outline\n",
        "\n",
        "- Better vectors: Tf-idf\n",
        "\n",
        "- Ngrams\n",
        "\n",
        "- Text generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6366df5-2396-4222-8760-37060a1dc94c",
      "metadata": {
        "id": "a6366df5-2396-4222-8760-37060a1dc94c"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import collections\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn import decomposition, naive_bayes\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics import (\n",
        "    ConfusionMatrixDisplay,\n",
        "    accuracy_score,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        ")\n",
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8c44f31",
      "metadata": {
        "id": "a8c44f31"
      },
      "outputs": [],
      "source": [
        "tqdm.pandas()\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "sns.set_style(\"darkgrid\")\n",
        "sns.set_context(\"notebook\")\n",
        "\n",
        "pd.set_option(\"display.precision\", 4)\n",
        "\n",
        "print(\"sklearn's version:\", sklearn.__version__)    # 1.3.2\n",
        "print(\"nltk's version:\", nltk.__version__)          # 3.8.1\n",
        "print(\"pandas's version:\", pd.__version__)          # 2.1.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a0b2586-f2bf-49af-bd7b-a24ce918eee1",
      "metadata": {
        "id": "2a0b2586-f2bf-49af-bd7b-a24ce918eee1"
      },
      "outputs": [],
      "source": [
        "%config InlineBackend.figure_format=\"retina\"  # For high DPI display"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0cece58",
      "metadata": {
        "id": "c0cece58"
      },
      "source": [
        "## Getting the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "467e1198-37f5-4bc4-98cc-28fbf0b10c34",
      "metadata": {
        "id": "467e1198-37f5-4bc4-98cc-28fbf0b10c34"
      },
      "outputs": [],
      "source": [
        "DATA = Path(\"../data\")\n",
        "\n",
        "TEXT_P = DATA / \"texts\"\n",
        "\n",
        "CORPORA = [\n",
        "    \"mythology\",\n",
        "    \"woodworking\",\n",
        "    \"robotics\",\n",
        "    \"hsm\",\n",
        "    \"health\",\n",
        "    \"portuguese\",\n",
        "]\n",
        "\n",
        "EPS = np.finfo(float).eps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45164744-82eb-44ad-b6d9-1a9da7ff8fd6",
      "metadata": {
        "id": "45164744-82eb-44ad-b6d9-1a9da7ff8fd6"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "\n",
        "for i, corpus in enumerate(tqdm(CORPORA)):\n",
        "    print(corpus)\n",
        "    for fp in (TEXT_P / corpus).glob(\"*.txt\"):\n",
        "        with fp.open() as f:\n",
        "            text = f.read()\n",
        "        data.append(\n",
        "            {\n",
        "                \"id\": fp.stem,\n",
        "                \"text\": text,\n",
        "                \"category\": corpus,\n",
        "                \"cat_id\": i,\n",
        "            }\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d42f8c4-504f-43c0-b174-5396a852f426",
      "metadata": {
        "id": "6d42f8c4-504f-43c0-b174-5396a852f426"
      },
      "outputs": [],
      "source": [
        "all_df = pd.DataFrame.from_records(data, index=[\"id\"])\n",
        "all_df = all_df.drop(\"robotics.stackexchange_0000005103\")\n",
        "all_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ea5c0ff-1200-4d45-b7b3-8fded7343f8d",
      "metadata": {
        "id": "6ea5c0ff-1200-4d45-b7b3-8fded7343f8d"
      },
      "source": [
        "---\n",
        "Here the initialization of `all_df` is done and you can jump to your favorite classification\n",
        "___"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41a2eb62-3309-4636-bca7-8abb87d0a053",
      "metadata": {
        "id": "41a2eb62-3309-4636-bca7-8abb87d0a053"
      },
      "outputs": [],
      "source": [
        "all_df.groupby(\"category\").size().plot.bar()\n",
        "plt.grid(axis=\"x\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ff3abc6-c173-4705-b563-f194f7426990",
      "metadata": {
        "id": "5ff3abc6-c173-4705-b563-f194f7426990"
      },
      "source": [
        "## Vectorize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07eb09d9-a25f-4d44-b836-4244a8a2fedc",
      "metadata": {
        "id": "07eb09d9-a25f-4d44-b836-4244a8a2fedc"
      },
      "outputs": [],
      "source": [
        "corpus = [\n",
        "    \"This is the first document.\",\n",
        "    \"This document is the second document.\",\n",
        "    \"And this is the third one.\",\n",
        "    \"Is this the first document?\",\n",
        "]\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "xs = vectorizer.fit_transform(corpus).toarray()\n",
        "\n",
        "cv_df = pd.DataFrame(xs, index=corpus, columns=vectorizer.get_feature_names_out())\n",
        "cv_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bb0c06d",
      "metadata": {
        "id": "9bb0c06d"
      },
      "source": [
        "### TF-IDF\n",
        "\n",
        "- The counting of words is not the ideal way to represent a document\n",
        "\n",
        "- Instead of counting the number of occurences of a word, we could weight it by the number of documents it appears in\n",
        "\n",
        "- Term Frequency - Inverse Document Frequency"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4637a874",
      "metadata": {
        "id": "4637a874"
      },
      "source": [
        "#### From [Wikipedia](https://en.wikipedia.org/wiki/Tf-idf):\n",
        "\n",
        "**Term frequency**\n",
        "\n",
        "$$\\operatorname{tf}(t,d) = \\frac{f_{t,d}}{{\\sum_{t' \\in d}{f_{t',d}}}}$$\n",
        "\n",
        "**Inverse document frequency**\n",
        "\n",
        "$$\\operatorname{idf}(t, D) =  - \\log \\frac{|\\{d \\in D: t \\in d\\}|}{N} =\n",
        "    -\\log \\left({\\frac {n_{t}}{N}}\\right) $$\n",
        "\n",
        "N: total number of documents in the corpus $N = {|D|}$\n",
        "\n",
        "**Smoothed IDF:**\n",
        "\n",
        "$$\\operatorname{idf}(t, D) = - \\log \\left( \\frac {1 + n_t} {N}\\right)+ 1$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b3bc67e",
      "metadata": {
        "id": "6b3bc67e"
      },
      "source": [
        "#### From [scikit-learn documentation](https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting):\n",
        "\n",
        "In `TfidfVectorizer`, the implementation is slightly different:\n",
        "\n",
        "*   For the IDF:\n",
        "\n",
        "    *   Unsmoothed version (`smooth_idf=False`), though a $+1$ is applied to avoid a value of 0 for the IDF:\n",
        "    $$\\operatorname{idf}(t, D) = -\\log \\left({\\frac {n_{t}}{N}}\\right) + 1$$\n",
        "\n",
        "    *   Smoothed version (`smooth_idf=True`):\n",
        "    $$\\operatorname{idf}(t, D) = - \\log \\left( \\frac {1 + n_t} {1 + N}\\right)+ 1$$\n",
        "\n",
        "*   `TFIDF = tf(t, d) * idf(t, D)`\n",
        "\n",
        "*   `tf(t, d)` is the **count** of the term `t` in the document `d`, not the **frequency** (**no division!**)\n",
        "\n",
        "*   Then a global normalization is applied: `TFIDF = TFIDF / norm(TFIDF)`\n",
        "    \n",
        "    (instead of a normalization within `tf` only to obtain a frequency)\n",
        "\n",
        "*   `norm` is the L2 norm by default, but can be changed to L1 or `None`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "700d233a",
      "metadata": {
        "id": "700d233a"
      },
      "source": [
        "#### ðŸš§ TODO: Count vs. Freq vectorizers\n",
        "\n",
        "Compare the previous results of the `CountVectorizer` with the `TfidfVectorizer` on the same toy dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9096c2b1-4a5a-415d-94a3-b282c21c80ed",
      "metadata": {
        "id": "9096c2b1-4a5a-415d-94a3-b282c21c80ed"
      },
      "outputs": [],
      "source": [
        "# tf_idf = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5642f69-afc2-437e-b4dc-e78b8f1112a5",
      "metadata": {
        "id": "d5642f69-afc2-437e-b4dc-e78b8f1112a5"
      },
      "source": [
        "### Let's now classify the StackOverflow documents in their respective corpus label\n",
        "\n",
        "*   The complete corpus: `all_df`\n",
        "\n",
        "*   Let's use the different vectorizers for representation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04236c43",
      "metadata": {
        "id": "04236c43"
      },
      "outputs": [],
      "source": [
        "ys = all_df.cat_id.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a08f9f72-443a-416a-9537-2f000ce5117d",
      "metadata": {
        "id": "a08f9f72-443a-416a-9537-2f000ce5117d"
      },
      "outputs": [],
      "source": [
        "cv = CountVectorizer(max_features=5000)\n",
        "xs = cv.fit_transform(all_df.text).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7977ca29",
      "metadata": {
        "id": "7977ca29"
      },
      "outputs": [],
      "source": [
        "print(\"xs.shape =\", xs.shape)\n",
        "print(\"ys.shape =\", ys.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c757fdd0-615f-415b-8a59-cc4fbabba1c4",
      "metadata": {
        "id": "c757fdd0-615f-415b-8a59-cc4fbabba1c4"
      },
      "outputs": [],
      "source": [
        "train_xs, test_xs, train_ys, test_ys = train_test_split(\n",
        "    xs, ys, test_size=0.3, random_state=0, shuffle=True\n",
        ")\n",
        "print(train_xs.shape)\n",
        "print(test_xs.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1f450b8",
      "metadata": {
        "id": "b1f450b8"
      },
      "source": [
        "#### ðŸš§ TODO: `CountVectorizer` vs. `TfidfVectorizer` classifiers\n",
        "\n",
        "*   Limit to the 5000 most frequent words to reduce the dimensionality\n",
        "\n",
        "    (the sparse matrix `toarray()` method crashes if no reduction of features is applied)\n",
        "\n",
        "*   Use both to classify the data based on, e.g., the `MultinomialNB` classifier\n",
        "\n",
        "*   Compare their results in terms of accuracy and show their `classification_report`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7D8W5OWsLTs"
      },
      "source": [
        "#### ðŸš§ TODO: `MultinomialNB` vs. `GaussianNB` classifiers\n",
        "\n",
        "*   Use both to classify the data\n",
        "\n",
        "*   Compare their results in terms of accuracy and show their `classification_report`"
      ],
      "id": "s7D8W5OWsLTs"
    },
    {
      "cell_type": "markdown",
      "id": "5d928327-bf23-4554-8f5f-aaee99c74697",
      "metadata": {
        "id": "5d928327-bf23-4554-8f5f-aaee99c74697"
      },
      "source": [
        "#### ðŸš§ TODO: Confusion matrix for the MultinomialNB classifier\n",
        "\n",
        "- Translate this result into a sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53799ca9-8e2d-40b3-8922-6ad4e2ebc0d7",
      "metadata": {
        "id": "53799ca9-8e2d-40b3-8922-6ad4e2ebc0d7"
      },
      "source": [
        "#### ðŸš§ TODO: add the `stop_words=\"english\"` option to the `tf_idf` vectorizer\n",
        "\n",
        "*   Compare the size of the `sparse_xs` for the 2 conditions (stop words included or not)\n",
        "\n",
        "*   Retrain with the new vectorizer and the MultinomialNB classifier\n",
        "\n",
        "*   Compare the results with the previous vectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "152ccd31",
      "metadata": {
        "id": "152ccd31"
      },
      "source": [
        "#### ðŸš§ TODO: PCA\n",
        "\n",
        "*   Apply PCA to project on 2 dimensions to all vectors (train and test) and plot them\n",
        "\n",
        "*   Compare the plot with the previous vectorizer with, and without the `stop_words=\"english\"` option\n",
        "\n",
        "*   Explain the difference breafly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoNbueUOsLTt"
      },
      "source": [
        "#### ðŸš§ TODO: Train on PCA\n",
        "\n",
        "*   Apply PCA to project on 2 dimensions to all vectors (train and test), fitting the PCA only on the training set\n",
        "\n",
        "*   Train the MultinomialNB and the GaussianNB classifiers on the PCA vectors\n",
        "\n",
        "    Does it work with both classifiers? Explain breafly.\n",
        "\n",
        "*   Compare the results with the vectorizer with, and without, the `stop_words=\"english\"` option\n",
        "\n",
        "*   Analyze the difference breafly"
      ],
      "id": "MoNbueUOsLTt"
    },
    {
      "cell_type": "markdown",
      "id": "b9dada15-875a-4aea-8d31-01011ef6662b",
      "metadata": {
        "id": "b9dada15-875a-4aea-8d31-01011ef6662b"
      },
      "source": [
        "#### ðŸš§ TODO: PCA with more dimensions\n",
        "\n",
        "*   Try to find the best number of dimensions for the PCA with [`GridSearchCV`](sklearn.model_selection.GridSearchCV)\n",
        "\n",
        "*   Use pipeline to combine the PCA and the classifier and other potential preprocessing steps\n",
        "\n",
        "*   Boxplot accuracyies for different PCA dimension values using [`GridSearchCV`](sklearn.model_selection.GridSearchCV)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4874beb9-408c-467f-9a26-9c6e39c4c678",
      "metadata": {
        "id": "4874beb9-408c-467f-9a26-9c6e39c4c678"
      },
      "source": [
        "## N-grams features for text classification\n",
        "\n",
        "*   We will use bigram in this section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06ffb414-1b14-439f-9f1b-857fc3736956",
      "metadata": {
        "id": "06ffb414-1b14-439f-9f1b-857fc3736956"
      },
      "outputs": [],
      "source": [
        "vectorizer_2g = CountVectorizer(\n",
        "    analyzer=\"word\", ngram_range=(2, 2), max_features=5000,\n",
        ")\n",
        "x2gs = vectorizer_2g.fit_transform(all_df.text).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ad83427-2c40-448c-a68b-2e67af18c439",
      "metadata": {
        "id": "4ad83427-2c40-448c-a68b-2e67af18c439"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(vectorizer_2g.get_feature_names_out(), columns=[\"bigrams\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ffc1cd1-36a2-41e4-9d6a-9e51a550e6a3",
      "metadata": {
        "id": "4ffc1cd1-36a2-41e4-9d6a-9e51a550e6a3"
      },
      "outputs": [],
      "source": [
        "train_xs, test_xs, train_ys, test_ys = train_test_split(\n",
        "    x2gs, ys, test_size=0.3, random_state=0, shuffle=True\n",
        ")\n",
        "print(train_xs.shape)\n",
        "print(test_xs.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "096i7uxxsLTt"
      },
      "source": [
        "#### ðŸš§ TODO: Train classification models on n-grams\n",
        "\n",
        "*   Train on the `CountVectorizer` with bigrams using the `MultinomialNB` classifier\n",
        "\n",
        "*   Compare the results with the previous \"raw\" `CountVectorizer`"
      ],
      "id": "096i7uxxsLTt"
    },
    {
      "cell_type": "markdown",
      "id": "8c71a315-40b6-4739-bb20-a6a6a3724c7c",
      "metadata": {
        "id": "8c71a315-40b6-4739-bb20-a6a6a3724c7c"
      },
      "source": [
        "---\n",
        "## Building an n-gram generator\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "549f111f-d2ae-4ee4-bb99-689b2dd8ec68",
      "metadata": {
        "id": "549f111f-d2ae-4ee4-bb99-689b2dd8ec68"
      },
      "source": [
        "### Let's build character trigrams first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7042cea6-8886-4bc1-b824-75a84de70b75",
      "metadata": {
        "id": "7042cea6-8886-4bc1-b824-75a84de70b75"
      },
      "outputs": [],
      "source": [
        "LINE = \"This is cool!\"\n",
        "N = 3\n",
        "[LINE[i : i + N] for i in range(len(LINE) - N + 1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c7d6a3b-6032-498d-8317-2091c402c5e5",
      "metadata": {
        "id": "2c7d6a3b-6032-498d-8317-2091c402c5e5"
      },
      "outputs": [],
      "source": [
        "[tuple(LINE[i : i + N]) for i in range(len(LINE) - N + 1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61fa1606-e4c9-448d-b6a8-bf6d06339a71",
      "metadata": {
        "id": "61fa1606-e4c9-448d-b6a8-bf6d06339a71"
      },
      "outputs": [],
      "source": [
        "LINES = \"\"\"This is cool!\n",
        "This is amazing!\n",
        "But why is this his fish?\"\"\"\n",
        "\n",
        "re.split(\"\\n+\", LINES.lower())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83ac1be4-fb00-451a-a4ec-e402e6e7fd97",
      "metadata": {
        "id": "83ac1be4-fb00-451a-a4ec-e402e6e7fd97"
      },
      "outputs": [],
      "source": [
        "ngrams = collections.Counter()\n",
        "\n",
        "for line in re.split(\"\\n+\", LINES.lower()):\n",
        "    ngrams.update([line[i : i + N] for i in range(len(line) - N + 1)])\n",
        "\n",
        "nc_df = pd.DataFrame.from_dict(\n",
        "    ngrams, orient=\"index\", columns=[\"freq\"]\n",
        ").sort_values(\"freq\", ascending=False)\n",
        "\n",
        "nc_df.head(7)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca11b3ea-589e-4f1b-840f-67c386a0edb3",
      "metadata": {
        "id": "ca11b3ea-589e-4f1b-840f-67c386a0edb3"
      },
      "source": [
        "### Back to our corpora"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yyyqnj7XsLTy"
      },
      "source": [
        "#### ðŸš§ TODO: Build an ngram generator for the `\"woodworking\"` corpus\n",
        "\n",
        "*   Tokenize the corpus first with `nltk.word_tokenize`\n",
        "\n",
        "*   Normalize the tokens by simply simply lowercasing them\n",
        "\n",
        "*   Give the tokens frequency to the ngram generator"
      ],
      "id": "Yyyqnj7XsLTy"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b4d2af9-32ff-4e0b-abe1-ad70b29c5685",
      "metadata": {
        "id": "8b4d2af9-32ff-4e0b-abe1-ad70b29c5685"
      },
      "outputs": [],
      "source": [
        "wood_df = all_df[all_df.category == \"woodworking\"]\n",
        "wood_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74899092-8c6d-432a-bb04-eaf2c104b2e7",
      "metadata": {
        "id": "74899092-8c6d-432a-bb04-eaf2c104b2e7"
      },
      "outputs": [],
      "source": [
        "wood_txt = wood_df.iloc[0].text\n",
        "print(wood_txt[:333])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWwnwcp1sLTy"
      },
      "source": [
        "#### ðŸš§ TODO: Text generation with ngrams\n",
        "\n",
        "*   Generate a text of words until a stop word is generated (`.`)\n",
        "\n",
        "*   Assign a probability to each ngrams based on their frequency\n",
        "\n",
        "*   Sample from the ngrams based on their probability\n",
        "\n",
        "*   Try different starting words (use bigrams)\n",
        "\n",
        " *Note:* [`default_rng`](https://numpy.org/doc/stable/reference/random/generator.html#numpy.random.default_rng) is the recommended constructor for random number generation in NumPy."
      ],
      "id": "JWwnwcp1sLTy"
    },
    {
      "cell_type": "markdown",
      "id": "ca86525f-c78b-465b-bcc8-7f281bfd6176",
      "metadata": {
        "id": "ca86525f-c78b-465b-bcc8-7f281bfd6176"
      },
      "source": [
        "### TODO (optional): Improve this algorithm\n",
        "\n",
        "- Try to fix some possible issues, by handeling exceptions\n",
        "\n",
        "- Improve the output spacing (no space before comma, etc.) and maybe handle the upper case\n",
        "\n",
        "- Try on a different domain of our corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d850e54-f593-4be9-967b-fffd88423ed9",
      "metadata": {
        "id": "1d850e54-f593-4be9-967b-fffd88423ed9"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "09de49be0f1a3fc16ad182c7831a4ec7541eb2056b05e1cb8bc46f2102d16c60"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}