Q:

What was Gauss's method for solving the "trinomial equation"?

In the second part of his fourth proof of the fundamental theorem of the algebra (published in 1849 [1]), entitled "A contribution to the theory of algebraic equations", Gauss gives a new method for numerical solution of trinomial equations, i.e equations of the form $x^n + fx^m +g = 0$, which also includes the case when $f,g$ are complex. For example, the Bring radical $x^5+px+q = 0$ belongs to this class of equations.
Numerical methods for solving trinomial equations date back to the time of J.H. Lambert and L. Euler, who gave a solution in the form of a series expansion. However, Gauss remarks in his 1849 publication that these series converge very slowly in the case when two roots are very close to each other, and that in the extreme case when there are multiple roots (i.e when two roots are equal) the convergence is weaker than that of any geometric progression. He says that his method is better suited for these cases, and mentions that he had already applied this method in 1843 for the solution of a certain cubic equation occurring in astronomy [2] (the last fact is interesting, since cubic equations need not be numericaly solved - they can be solved by radicals).
Unfortunately, as is usually the case with Gauss, he goes directly to describing his algorithm without revealing his actual thought process. So, I just wanted to check if anyone can tell something about Gauss's method. Maybe someone who is familiar with numerical methods for solving equations in the complex plane can help here.
EDIT
[1]
"Beitraege zur Theorie der algebraischen Gleichungen." Abhandlungen der Koeniglichen Gesellschaft der Wissenschaften zu Goettingen 4. 1849. Link.
[2]
"Astronomische Nachrichten No. 474." Astronomische Nachrichten 20. Akademie-Verlag, 1843. Link. Gauss's letter appears on pp. 299-300.

A:

I think I can shed some sort of light on the other methods in use for solving trinomials (and general polynomials) from Lambert (1758) to Langrange (1770) and Euler (1776), as well as the (more specialized) method of Gauss.
BACKGROUND
In 1758, Johann Heinrich Lambert published Observationes Variae in Mathesin Puram (Various observation on pure mathematics) in Acta Helvetica Physico-Mathematico-Botanico-Medica (specifically, Acta Helvetica Vol III). In $\S$29 he considers the general polynomial,
$$0 = x^m - Ax^{m-1}+Bx^{m-2}-...+Hx^2-Ix+K$$
Where $x=\alpha, \beta, \gamma, \delta, etc$ are the $m$ roots of the equation.
Now take the sums of the roots, their squares, their cubes, and so on, and denote these sums by $\Sigma r$ for the sum of roots, $\Sigma r^2$ for the sum of the squares of the roots, etc. So,
$$\begin{align*}
\alpha+\beta+\gamma+\delta+... &= \Sigma r \\
\alpha^2+\beta^2+\gamma^2+\delta^2+... &= \Sigma r^2 \\
\alpha^3+\beta^3+\gamma^3+\delta^3+... &= \Sigma r^3 \\
&...
\end{align*}$$
Next, substitute the roots into the original expression as:
$$
\begin{align*}
0 &= \alpha^m - A \alpha^{m-1} + B\alpha^{m-2}-...+H\alpha^2-I\alpha+K. \\
0 &= \beta^m - A \beta^{m-1} + B\beta^{m-2}-...+H\beta^2-I\beta+K. \\
0 &= \gamma^m - A \gamma^{m-1} + B\gamma^{m-2}-...+H\gamma^2-I\gamma+K. \\
0 &= \delta^m - A \delta^{m-1} + B\delta^{m-2}-...+H\delta^2-I\delta+K. \\
&...
\end{align*}
$$
Add all of these equations together, to obtain
$$
0 = \Sigma r^m - A \Sigma r^{m-1} + B\Sigma r^{m-2}-...+H\Sigma r^2-I\Sigma r+mK.
$$
And thus, we can solve for $\Sigma r^m$,
$$\Sigma r^m = A \Sigma r^{m-1} - B\Sigma r^{m-2}+...-H\Sigma r^2+I\Sigma r-mK.$$
Take $m$ to be $1, 2, 3, ...$, and we have expressions for the different sums,
$$\begin{align*}
\Sigma r &= A \\
\Sigma r^2 &=  A \Sigma r - 2B\\
\Sigma r^3 &=  A \Sigma r^2 - B \Sigma r + 3C\\
\Sigma r^4 &=  A \Sigma r^3 - B \Sigma r^2 + C \Sigma r - 4D\\
\Sigma r^5 &=  A \Sigma r^4 - B \Sigma r^3 + C \Sigma r^2 - D \Sigma r + 5E 
\end{align*}
$$
We can substitute recursively, as
$$\begin{align*}
\Sigma r &= A \\
\Sigma r^2 &= A^2-2B \\
\Sigma r^3 &= A^3-3AB+3C \\
\Sigma r^4 &= A^4-4A^2 B + 2B^2 + 4AC - 4D \\
&...
\end{align*}$$
But these expressions still do not give us the roots directly.
Instead, Lambert makes a critical assumption: assume there exists a root which is purely real, and which has a larger real part than all other roots' real parts. (Lambert's assumption becomes the condition for convergence of his series solution. ) Then, taking the ratio of two sums, in the limit as $m\rightarrow \infty$, all terms go to zero but the leading powers.
$$\lim_{m\rightarrow \infty} \frac{\alpha^m + \beta^m + \gamma^m + \delta^m + ...}{\alpha^{m-1}+\beta^{m-1}+\gamma^{m-1}+\delta^{m-1}+...}  = \frac{\alpha^m}{\alpha^{m-1}} = \alpha$$
The series representation of this root can be found as:
$$S = \frac{A \Sigma r^{m-1} - B\Sigma r^{m-2}+...-H\Sigma r^2+I\Sigma r-mK}{A \Sigma r^{m-2} - B\Sigma r^{m-3}+...+H\Sigma r^2-I\Sigma r+(m-1)K}$$
In using this to solve for the trinomial,
$$x^m + px = q$$
Lambert obtains the series solution,
$$x = \frac{q}{p} - \frac{q^m}{p^{m+1}} + m \frac{q^{2m-1}}{p^{2m+1}} - m \frac{3m-1}{2!} \frac{q^{3m-2}}{p^{3m+1}} + m \frac{(4m-1)(4m-2)}{3!} \frac{q^{4m-3}}{p^{4m+1}} - ...$$
Which I've put into summation-notation form:
$$S = \sum_{k=0}^\infty (-1)^k \frac{q^{k(m-1)+1}} {p^{km+1}} \frac{1}{k!} \prod_{j=0}^{k-2}(km-j)$$
Details of the actual derivation can be found in Observationes $\S$38.
Lambert's solution was used by Euler in 1776 to obtain his series solution for a symmetric trinomial, that is, a trinomial symmetric in parameters $\alpha$ and $\beta$ of the form:
$$x^\alpha - x^\beta = (\alpha-\beta)vx^{\alpha+\beta}$$
But his solution technique was the same as Lambert's, so he doesn't go into it, and neither will I. (I did try to derive my own series solution, but obtained a slightly different answer than Euler; I wrote to him regarding the discrepancy, but have yet to hear back).

In 1770, six years prior to Euler's De Serie Lambertina, J-L. Lagrange published Nouvelle méthode pour résourdre les équation littérales par le moyen des séries. (Mémoires de l'Académie royale des Sciences et Belles-Letters de Berline, t. XXIV, 1770). In it, he describes a new method of series expansion for the roots of a general polynomial, of any degree, and suitable for calculating any roots (and their powers).
The first method employed appears to have been known for some time, and it is distinct still from Lambert's series. It uses logarithms, and though it gets more involved (I have a translation of the first section on my History of Math and Science blog here) it begins in a fairly straightforward way.
Take as our general equation,
$$ a - bx + cx^2 - dx^3 +... $$
Whose roots are $p, q, r, ...$, so that, from elementary algebra, we have
$$ a-bx+cx^2-dx^3+...=a\left(1-\frac{1}{p}\right)\left(1-\frac{x}{q}\right)\left(1-\frac{x}{r}\right)...$$
Divide by $a$, and take the log of both sides,
$$
\log \left(1-\frac{bx-cx^2+dx^3-...}{a}\right) = \log \left(1-\frac{x}{p}\right) + \log \left(1-\frac{x}{q}\right) + \log \left(1-\frac{x}{r}\right) + ...
$$
Now Lagrange uses the fact that $\log(1-u) = -u - \frac{u^2}{2} - \frac{u^3}{3}-...$ to obtain
$$
\begin{align*}
    \log &\left(1-\frac{x}{p}\right) + \log \left(1-\frac{x}{q}\right) + \log \left(1-\frac{x}{r}\right) + ... \\
    = &- x\left(\frac{1}{p} + \frac{1}{q} + \frac{1}{r}+...\right) \\
    &-\frac{x^2}{2}\left(\frac{1}{p^2} + \frac{1}{q^2} + \frac{1}{r^2}+...\right)\\
    &-\frac{x^3}{3}\left(\frac{1}{p^3} + \frac{1}{q^3} + \frac{1}{r^3}+...\right)\\ 
    &.\ .\ .\ .\ .\ .\ .\ .\ .\ .\ .\ .\ .\ .\ .\ .\ .\ .\ .\ .\ .\
    \end{align*}
$$
Expand: $-\log \left(1-\frac{bx-cx^2+dx^3-...}{a}\right)$ into a power series $Ax + Bx^2+Cx^3$ to obtain:
$$
\begin{align*}
    A &= \frac{1}{p} + \frac{1}{q} + \frac{1}{r} + ...,\\
    2B &= \frac{1}{p^2} + \frac{1}{q^2} + \frac{1}{r^2} + ...,\\
    3C &= \frac{1}{p^3} + \frac{1}{q^3} + \frac{1}{r^3} + ...,\\
    &.\ .\ .\ .\ .\ .\ .\ .\ .\ .\ .\ .\ .\ .\ .\ .\ .\ .\ .\ .\ .\
    \end{align*}
    $$
Now, since we assumed a power series exists in the form
$$-\log \left(1-\frac{bx-cx^2+dx^3-...}{a}\right) = Ax + Bx^2 + Cx^3 + ...,$$
Taking the differential of both sides, and divide by $dx$,
$$\frac{b-2cx+3dx^2-...}{a-bx+cx^2-dx^3+...} = A + 2Bx + 3Cx^2 + ...,$$
And finally cross multiplying, we have formulas which, Lagrange says, were known even to Newton.
$$\begin{align*}
    A &=\frac{b}{a}, \\
    2B &= \frac{Ab-2c}{a},\\
    3C &= \frac{2Bb-Ac+3d}{a}
    \end{align*}$$
Lagrange goes on, obtaining (after much manipulation) a general expression, and as a particular case, an expression for the general trinomial.

GAUSS'S METHOD
We can compare these two methods with what Gauss is doing in his work. However, it will soon become clear that Gauss's work is rather unique in its approach.
He begins with the trinomial,
$$x^{m+n} \pm e x^m \pm f = 0$$
Under the conditions that the desired root is real, and $m,n,e,f > 0$, and (because it is sufficient) $m$ and $n$ have no common divisor. Doing away with the case where $x^{m+n} + ex^m + f = 0$ (which cannot have positive real roots) we take the first case,
$$x^{m+n} + e x^m - f = 0$$
I'll fill in some of the gaps with the reasoning here. With a bit of rearranging, we get
$$\frac{x^{m+n}}{f} + \frac{ex^m}{f} = 1$$
Noting the similarities between this expression and the Pythogorean identity,
$$ \sin^2 \theta + \cos^2 \theta = 1$$
We introduce the quantity $\theta$, under the constraint that $0 \le \theta \le 90^o$, such that
$$\sin^2 \theta = \frac{x^{m+n}}{f},\ \ \ \cos^2 \theta = \frac{ex^m}{f}$$
Note that the existence of $\theta$ is guaranteed by the Pythagorean identity. We can use $\theta$ to solve for $x^{m+n}$, $x^m$, and $x^n$. The first two are given immediately:
$$x^{m+n} = f \sin^2 \theta,\ \ \ x^m = \frac{f \cos^2 \theta}{e}$$
To find $x^n$, start with the equation for $x^{m+n}$ and multiply throughout by $x^{-m}$,
$$ x^{-m}\frac{x^{m+n}}{f} = x^{-m}\sin^2 \theta$$
From the equation for $x^m$ we know
$$x^{-m} = \frac{e}{f \cos^2 \theta}$$
Finally giving
$$\frac{x^n}{f} = \frac{e}{f} \frac{\sin^2 \theta}{\cos^2 \theta}$$
Or equivalently,
$$ x^n = e \tan^2 \theta$$
Now we wish to solve for $\theta$ in terms of the coefficients $e,f$. To eliminate $x$, we must find a way to solve for $x$ in two equations so that the expressions can be set equal. In fact, if we have $x^{m+n}$ and $x^n$, then we can make $x^{m(m+n)}$ from both; the first by raising it to the power $m$, the other by raising to $m+n$. Note, this is just one way, and it's convenient because it only involves positive powers (useful for Gauss's hand-calculations with tables). We get:
$$
\begin{align*}
&(x^{m})^{m+n} = \left(\frac{f}{e}\cos^2 \theta\right)^{m+n}\\
&x^{m(m+n)} = \frac{f^{m+n}}{e^{m+n}}\cos^{2m+2n} \theta\\
&(x^{m+n})^m = \left( f \sin^2 \theta \right)^m\\
&x^{m(m+n)} = f^m \sin^{2m} \theta
\end{align*}
$$
Now set these equal and we get our solution,
$$ \frac{f^n}{e^{m+n}} = \frac{sin^{2m}\theta}{\cos^{2m+2n}\theta} $$
Because the left hand side appears in other derivations, Gauss denotes it by $\lambda$.
In order to solve the trinomial, then, solve for $\theta$ in the above equation. Gauss assures that the right hand side is monotonically increasing for $0 \le \theta \le 90^o$, and so the root is uniquely determined.
This case, and the case of $x^{m+n}-ex^m-f=0$, are comparatively simple, while the third case, $x^{m+n}-ex^m+f=0$, has some more considerations. But they all follow this general procedure. In the remainder of the paper, these solutions are put into a form using logarithms, which is where the utility becomes more apparent, as extensive log tables were available.
CONCLUSION
Gauss saw the issue with the existing methods (slow to converge, difficult to compute by hand) and provided a new method which, while more specialized, is very efficient for hand calculation, approximating a purely real root of the given trinomial. There's more to be learned from his solution, e.g. which root does it actually give? And then, can we use polynomial long division to find the remaining roots? But these I'll leave for a post e.g. on Math.StackExchange.

