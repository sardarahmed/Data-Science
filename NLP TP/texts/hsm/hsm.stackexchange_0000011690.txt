Q:

Significance level $\alpha$ values - who devised to use $\alpha = 5 \%$?

In a statistical hypotheses testing a significance level $\alpha$ has to be set. The most often, $\alpha$ is set to be 5 %, sometimes 1 % and 10 % values are used. 
Value of $\alpha$ tells us what is a maximal tolerated probability of error of first kind (rejecting true hypothesis). There is also error of second kind (not rejecting false hypothesis). The convention is to anchor maximal probability of the error of first kind. But there is no reason to prefer error of first kind to error of second kind.
My questions are these:

Who first devised to use values 1 %, 5 % and 10 % for statistical signifance and why?
Why error of first kind is preferred?

A:

Based on comments, the reasons for using $\alpha = 5\%$ are these:

The value was used by prominent statisticians (such as R. A. Fisher). In order to have statistical analysis comparable with others, statisticans started to follow their example.
Probability 5 % and 95 % is connected with $2\sigma$ interval under normal distribution

The reason for using $\alpha$ equal to $1 \%, 5\%$ and $\,10\%$ is given by the fact that these values have more or less equal distribution on logaritmic scale.

