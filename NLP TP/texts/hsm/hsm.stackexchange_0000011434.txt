Q:

When was the first recorded occurence of irrational and imaginary number usage in number theory?

I saw a letter of Euler to Lagrange congratulating him on his usage of imaginary numbers in the "analysis devoted to rational numbers alone", was that the first known such usage? What was the likely inspiration of Lagrange(and Euler)?

A:

Irrational numbers were used by the ancient Greeks when they were discovered. The earliest texts did not survive but there are plenty of them in Euclid. Though they are not called numbers. The theory of proportions of Eudoxus-Euclid is equivalent to the theory of real numbers. Euclid's book X contains some very complicated theory of some irrational numbers.
Ancient theories only dealt with algebraic numbers (like square and cubic root of integers), they did not know that there are transcendental numbers.
They did not know that $\pi$ is transcendental.
The first general result on all irrational numbers is due to Nicole Oresme (14th century):
he proved that the fractional parts of $na$ are dense on $[0,1]$ for any irrational $a$.
Imaginary numbers occur for the first time in the theory of cubic and quartic equations (16th century). When the formulas for them were discovered, it was noticed that even in the case when the final answer is real, the intermediate calculation involves square roots of negative numbers. For example try to solve $x^3-x=0$ using the Cardano formula. So they started to develop arithmetic of complex numbers.
This was a surprisingly long development achieving a clear and complete theory only in the end of 18th century. Before this was achieved, many looked at their use with a suspicion. This suspicion did not dissolve completely until the later part of the 19th century. (They say that Chebyshev was against the use of complex numbers. If this is true, he was probably the last great mathematician who did not recognize them. His students used them routinely). 

A:

Regarding the headline question, Newton, in his text Universal Arithmetick, gave what Leo Corry states may be the first definition of number that included both positive and negative integers, fractions, and irrationals.
Here is Newton's definition:

By number we understand, not so much a Multitude of Unities as an abstracted ratio of any Quantity, to another Quantity of the same kind, which we take for unity. And this is threefold; integer, fracted, and surd: An Integer, is what is measured by Unity;  a fraction, that which a submultiple Part of Unity measures; and a Surd, to which unity is incommencurable.

On this definition, Corry writes:

The unit, the integers, the fractions, and the irrational numbers appear here - perhaps for the first time and certainly in an influential text in such clear-cut terms - all as mathematical entities of one and the same kind, the differences between them being circumscribed to a single feature clearly discernible in terms of a property of ratios. ... Moreover, and very importantly, numbers are abstract entities: themselves they are not quantities, but they may represent either quantities or a ratio between quantities. 

Regarding imaginary number, Newton is more hesitant.  He does not refer to them as "imaginary" or "fictions", as did Descartes and Wallis.  Newton uses the term "impossible". Quoting Corry:

And what he meant by "impossible" he explained by reference to the solution to the equation $$x^2 + 2ax + b^2 = 0.$$  Here, we obtain two roots, namely $$a + \sqrt{a^2 - b^2} \text{  and   } a - \sqrt{a^2 - b^2}.$$  Now, when $a^2$ is greater than $b^2$ - Newton wrote - the roots are "real".  In the opposite case, when $b^2$ is greater than $a^2$, of course, the root is "impossible".  But, interestingly, Newton nevertheless went on to stress that both expressions are roots of the polynomial, for the simple reason that when they are introduced in the equation in place of the unknowns, the equation is satisfied because "their factors eliminate each other". In other words, a square root of a negative number is an impossibility and hence does not represent a number in the proper sense of the word, but expressions containing such impossible entities are legitimate roots of an equation and allow for an appealing formulation of the fundamental theorem of algebra, as Newton conceived of it.

[Source: Leo Corry, A Brief History of Numbers.]

