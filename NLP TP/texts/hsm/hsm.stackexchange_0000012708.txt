Q:

How was charge to mass ratio measured via electrolysis in 19th century?

I think this question is useful to know how sir J.J.Thomson inferred from his experiment that the electron had to be 1700 lighter than hydrogen, as he declared in his Nobel Lecture of 1906:

Before the cathode rays were investigated, the charged atom of hydrogen met with in the electrolysis of liquids was the system which had the greatest known value of e/m, and in this case the value is only 104, hence for the corpuscle in the cathode rays the value of e/m is 1700 times the value for the corresponding quantity for the charged hydrogen atom.

In some nuclear physics notes I've found that Thomson compared his ratio to another calculated by H.A.Lorentz starting from electrolysis products. He was attempting to give an explanation for the
Zeeman Effect and measured the charge to mass ratio of the hydrogen.
Is this true? Are there any reliable sources?

A:

The determination of mass to charge ratio of the hydrogen ion came from electrolysis experiments and Faraday's law of electrolysis, which were well established in J. J. Thomson's time. I wish modern textbooks also included this experiment, but a 1913 The Chemical Engineer (monthly publication of applied/analytical chemistry) gives an account (Vol XX, No. 2, pg. 28). Google Books has it.
It was established that if 96,540 Coulombs were passed through HCl solution, one gram equivalent or 1.008 g of hydrogen ions ${H^+}$ will be reduced to hydrogen gas. The number of Coloumbs could be determined by silver coulometry or current/time relationship. I do not exactly how Faraday determined 96,540 in the beginning. His law is exact and no exception has been found so far! However, measuring the mass of electrolyzed gas was easy.
Here is an excerpt from The Chemical Engineer.

