Q:

History of "independent and dependent variables"

I have a lot of questions that can be summed up by "whats the history of independent and dependent variables?" Here is a list of those questions:
Where does our conception of independent and dependent variables come from? When did science get a notion of variable? How was this notion distinct from one quantity changing as another changes? What did they call measured quantities in science? When were these measured quantities first called "variables". Was there ever a time when mathematical variables were regarded as a distinct notion from measured quantities? Where does this particular terminology come from? When did this terminology become so established as it is today? Was it always central to science and hence a part of all science curricula or is its prevalence due only to some educational reform?
However I look into these questions I always get an introduction to independent and dependent variables as if I have never seen them before, and those sources always present the material as if the ideas have been around forever with no historical dimension. The most insight I've gotten into these questions so far has been on this sight here: Is it true that Leibniz introduced "constant," "variable," and "function"? . So variables, at least in mathematical contexts, have been around for a while, but nothing else is mentioned about variables in experimental contexts. I've also searched for the 'independent and dependent variables' on the Stanford encyclopedia of philosophy but those articles will at best discuss "causally independent variables" or "probabilistically independent random variables". There is no discussion even acknowledging the basic ideas learned in school. All of this suggests to me that the notion of independent and dependent variables is a modern invention for teaching science, but I don't have any proof of this. Also, even if this were the case the notion would have taken from some established ideas in science and I'm curious what that history would be.

A:

It is helpful to separate concepts from words. The sharp separation between mathematical variables and physical quantities they represent is a modern phenomenon. Ancient Greeks had magnitudes, which included numbers and lines, areas, etc., without taking them as something abstract and separated from reality. They had the idea of "independent and dependent variables" as well, both "mathematical" and "experimental". Ptolemy (c.150 AD) tabulates values of chords vs angles in the Almagest, and incident vs refracted angles in Optics. He even explains a version of linear interpolation. Diophantus (c. 250 AD) already had an algebraic symbol for a variable, an equivalent of modern $x$.
Let's recall that many of the creators of new mathematics in the 17-18th centuries, including Newton, Leibniz, Euler, Laplace, etc., were also doing physics, and the latter often inspired the former. Leibniz used the Latin word for "variable" in 1692, but not quite in the modern sense, since his notion of the function was not modern either. Bernoulli's 1698 notion of function as analytic dependence was closer. Newton talked of quantities and fluents in his papers even before Principia (1687), and the applied and "pure" contexts were intermixed, see What was the notion of limit that Newton used? Physical forces and masses are quantities, fluents "depend" on them, and velocities are fluxions of position. By 1710 "variable quantities" made their way into Harris's Lexicon Technicum, according to Miller's Earliest Known Uses of Some of the Words of Mathematics, and they bear resemblance to Greek magnitudes:

"Variable Quantities, in Fluxions, are such as are supposed to be continually increasing or decreasing; and so do by the motion of their said Increase or Decrease Generate Lines, Areas or Solidities".

As for the words, in English "variable" on its own and  "independent/dependent variables" crop up only at the beginning of 19th century. Memoirs of the Analytical Society from 1813 contain:"The method of Laplace for reducing an equation of the first order, where the difference of the independent variable is any function of the variable itself, to one wherein that difference is constant, is well known." English translation of Lacroix's  Differential and Integral Calculus (1816) had:

""The limit of the ratio... will be obtained by dividing the differential of the function by that of the variable", "Treating the subordinate variables as implicit functions of the indepdndent [sic] ones".

Further from Miller:

"Dependent variable appears in 1831 in the second edition of Elements of the Differential Calculus (1836) by John Radford Young: "On account of this dependence of the value of the function upon that of the variable the former, that is $y$, is called the dependent variable, and the latter, $x$, the independent variable""
"In Statistics R. A  Fisher used the terms dependent and independent in his presentation of regression analysis: see Section 25 of Statistical Methods for Research Workers (1925) on "Regression coefficients". For Fisher these terms qualified variate but later writers have generally favoured variable."

