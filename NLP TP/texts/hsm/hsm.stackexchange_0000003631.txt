Q:

When was the measurement problem solved?

I have been looking into the measurement problem that arises when considering different interpretations of quantum mechanics. Nowadays it seems to be considered a solved problem (in fact some people on physics.SE seem to be quite annoyed at people asking about if it is solved). Please correct me if that is not true.
I tried to have a look through the papers relevant to the topic. Some milestones seem to be papers by Legget & Caldeira, Joos & Zeh, Zurek and others. My question is at which time exactly the measurement problem was solved? Which paper or similar publication was the final brick?
EDIT
People were asking about a precise statement of the problem. I consider the measurement problem to be the fact that there used to be two evolution laws of the wavefunction: a continuous one (Schrödinger equation) and a discontinuous one (measurement/collapse). The problem lies in how to reconcile these two.

A:

To a certain extend, the measurement problem (MP) is indeed philosophical, in the end, it still comes down to which interpretation you prefer. 
However, there are also of course also certain physical results, which come into play here. These are also highly relevant to the various interpretations, which I will try to put in appropiate context. The keyword in order to understand the modern solutions of the measurement problem is decoherence.
Formally speaking the measurement problem is the obvious contradiction between two of the textbook axioms

Time evolution of isolated systems is described by the Schrödinger equation $i\partial_t|\psi\rangle=H|\psi\rangle$. The important thing to notice is, that the time evolution operator $t_0\rightarrow t_1$ is unitary.
After measuring the system in its eigenstate corresponding to the measurement outcome.

Formally speaking, the operator which describes the transition "before measurement" to "after measurement" is a strict projection (in case the system is not already in an eigenstate, which is generic), and therefore obviously not unitary, although it should be a time evolution in some sense.
Some early versions of the Copenhagen Interpretation (CI), Bohr's views in particular, propose a different, classical set of physical rules as soon as macroscopic objects interact with the quantum objects, because the classical notions are inalienable in order to describe our world ("Unverzichtbarkeit klassischer Begriffe"), the irreversible interaction between the quantum system and the measurement device are an integral part of our description. This point of view can be found in many of Bohr's essays, for example "Einheit des Wissens (1954) or his answer to EPR 1935 "Kann man die quantenmechanische Beschreibung der Wirklichkeit als vollständig erachten?". I should emphasize at this point that the Copenhagen Interpretation is not a clear-cut set of rules how to treat certain situations and how to interpret them physically, its more like an approach which works well in practice and is underpinned with various philosophical concepts, which differ between the various personalities. What I briefly sketched is mainly Bohr's point of view, von Neumann for example have a rather formal approach, while Heisenbergs emphasis was always on his uncertainty relations. However, the common denominator of what we call "Copenhagen Interpretation(s)" today is a somewhat nonunitary behavior at some classic-quantum boundary. For more details, see Jammer's book "The Philosophy of Quantum Mechanics".
Philosophically, CI, and in particular Bohr's views mimic Kant's concepts in some ways, however, it is surely not pleasing from a physicists point of view (at least not for me), as in this sense, QM is not "more fundamental" than the classical theories as we need the latter in order to formulate stuff like observables, as there is for example no quantum-mechanical notion of "position".
One could say, that this anti-reductionist point of view is another different formulation of the measurement problem. 
Considering a "solution" of the MP, as I already mentioned, the major breakthrough of the  "second generation" of QM-"Philosophers" was the development of Quantum decoherence.
The first step in this "modern development" was Everett '57 - "The Theory of the Universal wave function" or the (after getting massive flak from Bohr) shortened, first published version ""Relative-State"-Formulation of Quantum Mechanics". The content of these famous publications is today known as "Many-Worlds-Interpretation" (MWI). While the ontological content of this work is most certainly debatable, the formal part of this work is what is mostly overlooked, although I personally think that Everett formulated the key idea for the modern approaches to the MP. In short, he re-formulated the known QM from a smaller set of axioms, in particular leaving out the "collapse" axiom and Born's rule, granting the Schrödinger equation universal validity, he then rederived Born's rule and something similar to the collapse from the reduced set of axioms. His description of the measurement process was basically "System Observer+Experiment, where the state of the observer changes with the state of the experiment, i.e. he can determine the state of the system from his own state.
The key concept of relative states he used (or rather his perspective on these) was probably crucial for the later development of decoherence. The mathematical observation is rather simple: Take a system consisting of two correlated systems $S=S_1\otimes S_2$ and put it in some state, then for a fixed state of the first system $S_1$, you can put a relative state by expanding in some basis of the second system. In this sense, you cannot independently fix states for $S_1$ or $S_2$ (especially as this works in any basis) - if you live in $S_1$, it only makes sense to talk about $S_2$ relative to your own state. Of course, this has been one of the well-known key features of quantum mechanics, entanglement - yet, Everetts key observation was, loosely speaking, that we can reproduce QM by entangling the observer to the system in our formal description. Notice, that this still does not solve the key problem "Where do the classical states come from?" - The basis we expand the relative system in is still arbitrary, so why should we expand in some "classical bases" from our point of view? This is in essence what is called "preferred basis problem" in the literature. In the end, it is basically again the measurement problem, stated in slightly different terms. However, these terms proved to be much more approachable than the philosophical discussion around Einstein&Bohr.
When Everett's work was first published, most people either either didn't  notice or didn't care. Everett's work was revitalized in the 70's by Graham and DeWitt, and it was no big surprise, that decoherence theory emerged shortly after and had a prime in the 80's and early 90's. Notable figures were Zeh, Joos and others. Decoherence is basically the process, where a quantum system interacts with a thermodynamic environment and loses its "quantumness" i.e. its ability to be in superposition states. Mathematically speaking, the offdiagonals of the corresponding density matrices are "traced out" by the environment. In this sense, the quantum irreversibility arises in a similar way as the irreversibility in thermodynamics from the classical theories - information gets lost in the heat of the environment. One key insight considering the "preferred basis problem" was made by Zurek , I think the first publication on this was "Environment-induced superselection rules" in 1982 (Phys. Rev. D 26): The coupling to the environment induces a certain selection of quantum states which will not appear in superposition, this process is called "environment-induced superselection" or short "Einselection", the basis "chosen" by this proces is also called "pointer basis". In particular, it gives you a preferred basis in which it interacts with the environment, depending on the structure of the Hamiltionian. Details to this can be found in Zurek's publications, in particular

Decoherence and the transition from quantum to classical (2003) 
Decoherence, einselection, and the quantum origins of the classical  (2001)

Now this sounds pretty good, does this solve our measurement problem, or at least the preferred basis problem? To some extend, yes. It explains, why the observer has a preferred basis, but it cannot exactly explain where this basis come from, or how an observer exactly has to look like to produce a classical basis. For example, einselection cannot explain why the stuff around here tends to be localized. An argument often brought up in this context is the so-called "Quantum Darwinism": Observable states are states which are decently stable - if some state or respectively some basis was highly unstable, it would immediately trace out when interacting with the environment and therefore not be observable in a macroscopic context. In this sense, the observables which we perceive as classical are just the "fittest" states. As far as I know, this is still subject of current research. However, I know that some people claim that MWI/Consistent Histories+Decoherence+Einselection+Darwinism solve the measurement problem. The main argument to this arises from the anthropic principle (AP): If it was different, we wouldn't be there to observe it. Not a very satisfying answer if you ask me.
So, to summarize my massive textwall:

Most people agree, that the measurement problem is partially(!) solved by invoking a relative-state formulation for Observer+System+Environment and combining it with a suitable interpretation, mostly many worlds or consistent histories. The observer measures the system in his einselected basis, and the irreversibility comes from the information-loss in the environment.
Most people also agree, that the quantum-to-classical transition ("Collapse of the wave function") is a quantum-mechanical process which can be interpreted in a statistical sense. In particular, it is not some instantaneous collapse happening at some quantum-to-classical boundary as for example Bohr imagined it.
Sometimes, people claim that the measurement problem is solved by decoherence without further interpretation. This is clearly wrong, as decoherence only works in a interpreted formalism. Also, decoherence is no magic addition to quantum theory, it arises naturally from "normal" quantum mechanics.
Few people claim that Quantum Darwinism together with the AP solves the measurement problem. This is very controversial.
Most people agree, that the measurement problem is not fully solved by the decoherence program. Some people think, that in principle, it can be solved, I imagine by deriving the preferred basis from some fundamental properties of our universe, such as the form of certain fundamental Hamiltonians and relations between the fundamental constants.
In conclusion, it still comes down more or less to which interpretation you prefer. However, my impression is that most people working in this area tend to Everett-ish interpretations (Many-Worlds, Consistent Histories, Existential Interpretation), in particular because the Schrödinger equation holds everywhere.

Most/Some/Few is of course my personal impression, I don't have stats to back this up. However, I hope I could give a decent overview over the historic development of this question and its current state. For a more detailed discussion about the current state, I can also recommend the overview article by Schlosshauer Decoherence, the measurement problem, and interpretations of quantum mechanics. There are also many, many other articles by different people with different opinions out there, if you're interested, you can actually find a lot of stuff out there. There are for example some very clearly written articles by one of the fathers of the decoherence program, H.D. Zeh, many of them are however in german, and he is very biased towards Many Worlds.

