Q:

Was 18th century algebra more symbolic/formal than the modern conception?

I've found Lagrange's Sur la résolution des équations algébriques to be a very confusing and difficult read, and I think I'm starting to see why: it seems that Lagrange thinks of algebra in a much more formal/symbolic way than I'm used to. Whereas I think of a symbol $x$ as referring to a specific number (which may be unknown), and I think of $2x + 3x = 5x$ as being justified because it would be true no matter which number $x$ is, Lagrange seems to just view $x$ as a sort of symbol on which certain rules of operation are defined, and $2x + 3x = 5x$ as being justified because it's one of those rules.
Here are some examples:

When explaining Cardan's method for solving the cubic, Lagrange blithely divides by a variable $y$ with no consideration for whether it is or isn't zero. At first I thought this was sloppy reasoning but now I wonder if he basically thinks of this as a calculation in the field of rational functions $\mathbb C(y)$.
Where $a, b, c$ are the roots of a particular cubic, Lagrange states in passing that $\frac {a+\alpha b + \beta c} 3$ will have $3!$ different values when we permute $a, b, c$ in every possible way. This is correct if we think of "different values" as meaning different forms, but possibly incorrect if $a, b, c$ are standing in for specific numbers (if they're all zero, for instance).
With $\alpha$ a primitive cube root of unity, Lagrange concludes from $\alpha Aa+\alpha Bb + \alpha Cc=Aa+Bc+Cb$ that $\alpha A=A$. This may be incorrect if $A, B, C$ and $a, b, c$ are specific numbers, but correct if we think of them as symbols and just want the expression on the left to be the same as the expression on the right.

Am I correct that 18th century algebra, or at least Lagrange, was done in this more "symbolic" way? If so, how can this be made rigorous in modern terminology? Where can I learn more about the way Lagrange and his contemporaries thought about their subject?

A:

There is indeed a difference between 18th century and modern concepts of algebra. For example, Lagrange and his contemporaries did not define algebraic structures into existence by specifying axioms for operations on abstract sets. Most of algebra was about integers and polynomials, which were assumed to pre-exist, and they were not thought as manifestations of "algebraic structures", although similarities between them were known. However, I do not believe that this is what is causing the conceptual block.
Lagrange's formal manipulation of polynomials is actually in the spirit of modern algebra, but a modern algebraist would distinguish between treating polynomials formally and as functions. Lagrange did not. He treated not only polynomals and rational functions formally, but transcendental functions and their Taylor series as well. This is because not only the concept of algebra was different in the 18th century, so was the concept of analysis.
Some historians refer to 18th century analysis as "algebraic analysis", see e.g. Fraser, The Calculus as Algebraic Analysis. Lagrange gave it its completed form in his 1797 Théorie des fonctions analytique. The modern notion of a function, as assignment of values to arguments, only appeared later in Fourier's work, and was made explicit by Dirichlet (1829). To Euler, Lagrange and others "function" was analytic expression formed by arithmetical rules and compositions, from a set of basic functions. Roughly, it corresponded to our "elementary function", although functions given implicitly were also admitted, only occasionally more general expressions (e.g. integrals of elementary functions) were considered.
Additionally, unlike modern algebraists, Lagrange did not distinguish between polynomials, formal or not, over real and complex fields. It was assumed that algebraic expressions can be operated upon according to the rules of algebra independently of such distinctions. If a formula is derived algebraically it is valid in general, for $x$ real or complex, infinite or infinitesimal. The idea can be traced back to Leibniz's law of continuity and was turned into a broader "generality of algebra" principle that Cauchy ascribed to Euler and Lagrange. In particular, the sum of a geometric series was considered equal to $\frac1{1-x}$ "in general", essentially by definition. Convergence was studied, but more as a practical matter of obtaining approximations and estimates.
Can it be made rigorous? Yes and no. As long as we do not read modern idea of a function into 18th century works much of what needs to be "made rigorous" goes away. However, sometimes rules established formally were used to compute values with implicit identification of the two function concepts. To some extent, "generality of algebra" is vindicated by analytic continuation theory, summation methods for divergent series, and the transfer principle in nonstandard analysis, but not to the extent assumed by Lagrange and Euler, and sometimes it led to outright self-contradictory results, as with the value of $\ln(i)$.

A:

The answer depends on what you consider the "modern concepts". In modern algebra, they usually do not consider polynomials or rational functions as functions of the variable $x$,
but exactly as you say in $1$, as elements of the field $C(y)$.
So Lagrange's writing is not more symbolic/abstract than modern algebra, but more abstract
than modern "high school algebra". 

