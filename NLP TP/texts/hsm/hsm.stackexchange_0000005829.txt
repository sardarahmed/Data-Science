Q:

Why $x_a$ (or $x_o$) and not $a_x$? (conventions for algebraic quantities)

It's my understanding that the convention of using letters from the end of the alphabet ($x$, $y$, $z$) to represent $variables$, and letters from the start of the alphabet ($a$, $b$, $c$) to represent $constants$ came to us from F. Vieta (who proposed vowels and consonants) by way of Descartes. 
However, I was curious to know if there was ever any debate around subscript notation for the same distinction, i.e. $x$ vs. $x_o$, where the first is considered a variable and the second, an unknown constant. 
I can imagine many reasons for $x_a$ being more popular than $a_x$; but, does anyone know where this notation got its start? &nd was there any contention? 

A:

Like Ben Crowell in the comments, I'm not sure I fully understand the question. But interpreting it broadly about the origin of the index notation, I quote Spalt who in Die Analysis im Wandel und im Widerstreit (2015), p. 106 while referring to a figure in Leibniz De quadratura arithmetica circuli ellipseos et hyperbolae (p. 528) claims (my translation):

Leibniz is the inventor of the index notation. Today we would write $D_1, D_2$ etc. for the indexed points $D$. Leibniz does this differently: he writes the numbers in front of the letter and he does not set them lower. (In the case of this figure he does not write the numbers smaller [so he wrote $1D, 2D$ etc.]. But he does in other cases. Sometimes he also sets the numbers lower, but he never writes the numbers smaller and sets them lower at the same time. ...)

I don't know when the passage to lower and smaller and after the letter happened and if one can find explicit reasons for this change.

