Q:

tf exception map to base_link

Hi all
We have a robot with 2 machines (1 quad core, 1 dual, both SSD) that makes heavy use of tf.
With everything running, we have about 80 nodes/nodelets.
The machines are certainly being taxed, but we have found that recently tf lookups are simply not working. Here's a typical exception, though it's not the only type we've seen:
Caught Transform Exception: Unable to lookup transform, cache is empty, when looking up transform from frame [/base_link] to frame [/map]
I am particularly confused about how the cache could be empty, unless the machine is hung so badly that absolutely no /tf data is being received.
Also, if we are logged in remotely to a machine and call rosrun tf tf_echo /map /base_link, the echo works as expected.
In this particular example, I am calling the tf lookup as follows:
  try{
    ros::spinOnce();
    tf::StampedTransform fixed_to_base;
    listener_.waitForTransform(fixed_frame_,base_frame_,msg->header.stamp,ros::Duration(30.0));
    listener_.lookupTransform(fixed_frame_,base_frame_,msg->header.stamp,fixed_to_base);

This is happening with calls in multiple processes, both C++ and python, so is there anything we should be doing differently?

Originally posted by phil0stine on ROS Answers with karma: 682 on 2012-04-19
Post score: 2

A:

It's possible you're saturating your network connection, which could cause messages not to be received. I've found bwmon (http://bwmon.sourceforge.net/) helpful for diagnosing this sort of issue.

Originally posted by Dan Lazewatsky with karma: 9115 on 2012-04-19
This answer was ACCEPTED on the original site
Post score: 1

Original comments
Comment by phil0stine on 2012-04-19:
Thanks I will check it out. It looks like we've at least realized that the throughput spikes when certain messages are broadcast, although we are using a Gigabit Ethernet switch which should handle it. I wonder if the ethernet controllers on the machines are whats choking
Comment by Dan Lazewatsky on 2012-04-19:
gigabit seems like a lot, but pushing point clouds around, I've seen upwards of 1Gbps on a single interface.
Comment by phil0stine on 2012-04-19:
Tentatively (!) looks like that was the issue, we removed some point cloud publishers and the same nodes work with tf. Im still hesitant to blame the switch, and the CPUs arent being pegged, so it narrows to something in between :-)

