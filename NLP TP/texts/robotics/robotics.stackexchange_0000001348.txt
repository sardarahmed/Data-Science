Q:

Single camera vision and mapping system

Some time ago I saw a demo of a small 'toy tank' with a single camera mounted on it. This tank was able to drive around the floor and detect objects and then move/steer to avoid them.
The interesting part was that it used a single camera vision system and as far as I remember was taking advantage of the floor being flat. and then using the rate a feature was moving in the scene relative to the motors and directions of travel to evaluate and hence map the scene.
Can anyone send me pointers what to search for to get some more information on this, or some pointers to codebases that can do this.
The reason I ask is that this was a single camera system from a number of years ago (5+) and therefore (from what I remember) was a relatively low compute load.
I was intending to try this out on a Raspberry PI to build a car/tank that maps a room or set of rooms.

A:

It's hard to say exactly what they were doing, but the terms you may want here are "optical flow" and "egomotion". Sounds like there may have been some feature detection and matching (something like SURF or SIFT) or foreground/background segmentation thrown in as well.
OpenCV is probably the most widely used codebase for computer vision, they have a lot of functionality for motion analysis. OpenCV should run on the Raspberry Pi, although your algorithms may be limited by computing power.

A:

Building on WildCrustcean's response another possiblity would be stereo vision. While we often think of stereo vision as using two cameras the techniques really only need images displaced in space and a model of the displacement. In other words I can take an image, move, then take another image. So long as I know the transformation between these two images I can then use stereo vision techniques to calculate the distance to a point in the image.

