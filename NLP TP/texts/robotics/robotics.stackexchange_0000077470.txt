Q:

does callbacks get drop from queue when it is exceeded it expected execution time by too much

Current I have a application with a timer which triggers at a fixed interval to grab a frame from a camera stream. This works fine when only two camera streaming at 720p are running. When all 3 of my camera are running, the timer's does not seems to execute all of the callbacks triggered.
For example if I am triggering the timer at 15fps, I should have the callback triggered 150 times if i ran for 10s. However when three cameras are running I will record a significant drop in the amount of callback being triggered. This increases along with the run time.
I suspect that due to my system's processor being overloaded with the need to handle multiple camera stream,  the time when the callback of the timer is being executed is significantly being delayed. Thus I wonder if a callback in queue get dropped if it is being delayed for too long? This will explain the missing callbacks.
Thanks

Originally posted by zenifed on ROS Answers with karma: 93 on 2016-11-23
Post score: 0

A:

As far as I know, a Timer will not queue a callback until the previous one is finished.
If you set a Timer that triggers at 15fps, means that the callback will have ~0.06 seconds to run.
Let's say that your callback takes 0.02 seconds to run (when the processor is not overloaded), then the timer will schedule the next callback in 0.04 seconds and the net effect is that you see your callback being executed at 15fps.
But if your callback takes more than 0.06 seconds, let's say it takes 0.1 seconds because your processor is overloaded with work, then the timer will execute the next callback immediately after the last one (because it has been longer than 0.06 seconds since the last callback) and the net effect is that you will see your callback being executed at 10fps.
So, to sum up, it is not that the callback gets dropped if its being delayed for too long, it is that the callback doesn't get scheduled until the last one is finished.

Originally posted by Martin Peris with karma: 5625 on 2016-11-23
This answer was ACCEPTED on the original site
Post score: 2

Original comments
Comment by zenifed on 2016-11-24:
I see that make sense!  So just to reiterate. Due the way the timer works, if my processor is only switching between 2 processes, a and b, and a is being queued up by the timer.
Comment by zenifed on 2016-11-24:
Effectively the interval between the timer triggering is :
{remanding duration of b when the timer first trigger} + {duration of a} + {timer duration}
Is that correct?

