Q:

How to use robot_localization with ar_track_alvar?

I'm looking for a way to use visually detected markers from the ar_track_alvar module as measurements for the ekf_localization_node from the robot_localization module.
It seems like robot_localization has no message type for observed markers. Sure, I can invert an observed marker pose and use the result as pseudo-observed robot pose. But how to come up with realistic covariance information, which is needed for the message of type PoseWithCovarianceStamped? A marker gives me distance and direction (or x, y, z in a local coordinate frame). So my derived pose and orientation will be highly correlated and its covariance matrix singular.
Is that a reasonable approach?
If not, how to combine these nodes the right way?
Are there any resources and tutorials you can suggest? (I googled a lot, but couldn't find one.)
Edit: I'm thinking of implementing an Unscented Transform (UT) for deriving the pose uncertainty from pixel uncertainty. In other contexts I had positive experiences with the UT. What do you think?

Originally posted by Falko on ROS Answers with karma: 268 on 2015-08-10
Post score: 2

Original comments
Comment by Mehdi. on 2015-08-11:
There is an old package where they say they can integrate visual landmarks into the EKF. http://wiki.ros.org/pose_ekf_slam . maybe it could help

A:

Both ekf_localization_node and ukf_localization_node assume that all measurements provide either the robot's body frame (base_link) velocity, its world frame (map or odom) pose, or its body frame linear acceleration. Of course, if your data is provided in another coordinate frame, that's fine, so long as a transform exists between the source and target frames. Your approach of using an intermediate node or modifying the existing node to provide the data you need is definitely the way to go.
Your second question is probably a better question for the Robotics Stack Exchange site. What you're asking is how to compute a covariance matrix of the robot's pose from a relative range and bearing measurement to a marker location, and I think the answer is sufficiently theoretical to warrant a question there. Off the top of my head, it feels like the error in the range and bearing from the robot to the marker is the same as the error in the range and bearing from the marker to the robot. If you can transform that range and bearing error to (X, Y, Z) error, then would it not be directly applicable to whatever pose measurement you compute? In any case, you'll have to be careful to compute the covariance in the world (map or odom) frame and not the robot's body (base_link) frame.
Just doing a brief look on Google, it seems like other people have applied the unscented transform to similar problems successfully.
In any case, the approach suggested by @Jim Rothrock will at least get you some result. If you went that route, you'd probably want to inflate the diagonal values a bit so as to over-estimate the error, though I realize you'll lose the correlation between the variables.

Originally posted by Tom Moore with karma: 13689 on 2015-08-11
This answer was ACCEPTED on the original site
Post score: 2

Original comments
Comment by Falko on 2015-10-26:
I finally found some time to try the unscented transform: I noticed that it's quite annoying to compute the 17 sigma poses (calibrated camera with 4 observed points -> no direct solution), so I decided to use approximate formulas for the expected uncertainty found in a photogrammetry lecture.

