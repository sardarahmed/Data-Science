Q:

ROS navigation with combined odometry

Hello,
I have been trying to find the best solution to create a robot with move_base + AMCL + combined sensory odometry and I have a few questions.
I will be using a robot with Kinect depth sensor and wheel encoders.
Now I know that AMCL uses /odom and Laser/PointCloud as input to determine the robot localization.
What I am worried about is would the robot is going to be navigating terrain which is slippery such as peagravel etc.
My questions are:

would the robot "realise" if for example the wheels have turned on a slippery floor without actually moving the robot by usint the Laser/PointCloud and encoder odometry alone? Because I fear that this could mark that the robot has moved forward on the navigation map but actually it stayed behind in reality.

I was thinking that this problem could be tackled by introducing an IMU and Visual Odometry which can be combined using robot_pose_ekf in order to create a more accurate odometry (if I am assuming right).

However I have read that this is a bad idea somehow because move_base and robot_pose_ekf don't work well together. Is this the case?

Wouldn't something like this work to override the move_base odometry intake (including IMU or VO)?

I would appreaciate any clarifications you can provide on this matter.
Thanks.

Edit: Thanks very much to both for your input. I have ordered an IMU sensor and I will have a try in the coming days.
Would you happen to know of a an online example which combines odomery from IMU and Encoders and uses move_base?
I have found a bit of difficulty to find ready examples of the desired solution and hence is why I was thinking that such thing couldn't work.
In any case I will post feedback when my sensor arrives and I try this out.

Originally posted by dgrixti on ROS Answers with karma: 95 on 2017-01-07
Post score: 1

Original comments
Comment by Procópio on 2017-01-17:
I reorganized your question because you actually have 3 of them.
Comment by Procópio on 2017-01-17:
check the robot_localization package for examples on sensor fusion.

A:

not exactly, what will likely happen is that AMCL will project the particles forward due to the odometry info and them you will likely see the AMCL particles spreading apart, as the filter will detect that the measurement from projected particles do not match what was expected. Eventually, it could converge again and relocalize the robot.

this is not the case. move_base will use whatever odometry you provide it. If it is filtered or not, it does not matter.

yes, it would, but you would also need to update your tf tree with your filtered odometry, which robot_localization already does, for example.

you got the right idea though, the way to go is to improve your odometry, eventually using visual odom, GPS or other source to filter out slippages. Then you feed your enhanced odometry to AMCL and move_base and you will get a much better localization and navigation.

Originally posted by Procópio with karma: 4402 on 2017-01-17
This answer was ACCEPTED on the original site
Post score: 0

