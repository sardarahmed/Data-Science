Q:

background subtraction

Hey,
given an indoor fixed mounted lidar scanner, is there a simple way to apply a background subtraction to a pointcloud?
More specifically, is there way to subscribe to the incoming Point Cloud topic of the lidar, compare it with a previously captured PointCloud (or PCD file) of the fixed surrounding and only output those objects, which are not part of the fixed surrounding.

Originally posted by th6262 on ROS Answers with karma: 50 on 2018-08-14
Post score: 0

Original comments
Comment by PeteBlackerThe3rd on 2018-08-14:
You could simply filter out points that are not closer than the background scan. The main challenge would be aligning the scans if they're not fixed structured clouds.
Comment by th6262 on 2018-08-14:
hello again! Basically the Scanner is fixed and mounted in the top corner of a room. I could capture a pcd file of the "empty" room ( only fixed objects).
I would like to compare the live point cloud with the fixed cloud and filter out every point in the live cloud which is also in the fixed cloud.
Comment by th6262 on 2018-08-14:
By "filter out" I mean, I want to end up with a pointcloud of only the objects which aren't part of the fixed cloud.
Note: The room has multiple "fixed" obstacles, so a simple ground plane removal would't be enough in this case, hence the background removal approach.

A:

This depends on they type of LIDAR scanner you're using. Does it produce structured point clouds, a 2D grid of points of the same shape and size each time? If this is the case then there should be a one-to-one mapping between the points of any two point clouds. In this case you can simply compare the distance of the new point from the sensor to the distance of the background point, and only include it if it's closer (plus a noise threshold).
If there isn't a one-to-one mapping between scans then your job is slightly harder, because you'll have to work out this mapping point by point and decide what to do if the current scan has points which don't correspond to the background scan. You may even need to interpolate the background scan to generate a suitable matching point.
Processing it as a depth map is simpler and far more efficient than processing it as an unstructured point cloud.
Hope this makes sense.

Originally posted by PeteBlackerThe3rd with karma: 9529 on 2018-08-14
This answer was ACCEPTED on the original site
Post score: 1

Original comments
Comment by th6262 on 2018-08-14:
I'm using this scanner: SICK MRS6000
So II suppose it's unstructured 3D Point Cloud Data. What you've describred in the first part of your reply was my initial approach, however your answer explains..
Comment by th6262 on 2018-08-14:
..why I couldn't work it out / find any documentation on it.
I suppose it would be computationally infeasible to compare every point in one scan to every point in the static scan and then decide based on distance+treshold
Comment by th6262 on 2018-08-14:
my thought was that this approach could be way more efficient than down-sampling, clustering, etc. (the classical approach)

