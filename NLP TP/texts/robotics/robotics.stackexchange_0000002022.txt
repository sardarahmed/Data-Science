Q:

How will the currently evaluated computer technology influence robotics and embedded systems in the foreseeable future?

This is my first question on this site, might be a little subjective :)
There is an ongoing process of many cool cyclonic changes of technology in the electronics and software industry.
Concurrency and Parallelism
What will be the implications of GPGPU, Multi-core  and "programming in the large" model in the specific case of embedded software, and how will it influence the styles and conventions in the community?
Single board multicore hardware like soon to be released Parallela can be an example?
Programming language research
The results have been excellent. Functional Programming has supposedly started reaching the masses. It was late night previous weekend when I briefly skimmed through an example of functional reactive programming to solve real time problems. AI people are also suggesting that we should be programming our robots in Declarative Domain Specific languages soon. It would be nice to know the implications on the robotics community. 
There has been a tremendous growth of frameworks like ROS and Urbi too!! That should be the region to look upon.. 
Most of the robotics, embedded and high performance AI codebase directly depends on C/C++ , and though languages like Rust and D are bubbling up, wouldn't it take massive amount of time to adopt the new languages, if ever adaptation begins? 
AI
Correct me, but it seems like a lot of time has passed and there are not many major production results from the AI community. I've heard about cognitive architectures of old like ACT-R and 4CAPS. They seem to be in hibernation mode! 
There seems to be a lot of work lately on otherwise intelligent systems (solved problems) like Computer vision and Data mining, but these problems cater to supercomputing and industrial crowd more. Could there be any possible shift towards low powered systems soon?
Thanks

A:

There's a lot of advancement on the software side, but there are many reasons why we don't currently see them on robots.
First, notice that there are a huge variety of robots already built or possibly built in the future. So let's look at a couple of them:

Manipulators: A manipulator is just an arm. You define a task for it, it does it. It's dumb, it's big and it's plugged in the wall at some factory. Do you really need AI or parallel processing for a manipulator? Does the programming language even matter? Probably not.
Small mobile robots: a variety of robots are small, or could be small. The biggest issue on those robots is power. To process heavily, you need more power. To provide more power, you need bigger batteries. To carry bigger batteries you need stronger motors. Stronger motors are big and need a bigger support. In short, small robots have to be really energy efficient, that's why they also can't be processing too much. GPGPU's and complicated AI is usually off limits for those robots.
Humanoids: This is probably what you have been thinking when you asked the question. So let's continue this answer assuming you are talking about humanoids, or more generally big mobile robots.

What will be the implications of GPGPU, Multi-core and "programming in the large" model in the specific case of embedded software,

It would be great! Besides the fact that you need proper hardware (actual computers vs. microcontrollers), it would be great. I believe this would specifically be helpful on areas where there is need for processing of huge input data, such as vision or tactile input.

and how will it influence the styles and conventions in the community?

Probably not much. Software is done in modules. Someone writes an odometry module and you use it. If they change the odometry implementation underneath, you won't notice it (much). Using GPGPUs is low-level enough to be part of lower level libraries, and often the actual users (programmers) of the robot would be unaware of it. Multi-core processing is all too easy and I think it's already widely used (you just make threads!)

Functional Programming has supposedly started reaching the masses. It was late night previous weekend when I briefly skimmed through an example of functional reactive programming to solve real time problems. AI people are also suggesting that we should be programming our robots in Declarative Domain Specific languages soon. It would be nice to know the implications on the robotics community.

Basically all non-joke (and even some joke) programming languages we have are turing-complete. This means that they can all get the job done. So choosing a language is really just a matter of convenience. Currently, C is the ubiqutous languange for every architecture. Everything can link to C and C is compiled on the remotest microcontroller on this planet. I don't think C could be put aside.
For higher level programming, I don't see any reason why any language couldn't be used. If the robot is controlled by a computer (vs. a microcontroller), it can probably run anything compiled or interpreted in any language.
Regarding real-time, probably you are misusing the word. If you are actually referring to an actual real-time system, then probably the biggest obstacles are the tools. Believe it or not, there are not many good ones. I program in real-time and trust me, there's something wrong with each of them. Still, you'd probably be forced to write in C or C++ if you want to use any of those tools.

There has been a tremendous growth of frameworks like ROS and Urbi too!! That should be the region to look upon.

I don't know Urbi, but I know ROS. ROS is quite famous, and everything you make, people immediately request a ROS module. It's nice, it works, but it's not perfect. Biggest flaw is that it can't be real-time, but they can't really be blamed. What I mean is, it's not a region to look upon, it's a region where people are already at.

Most of the robotics, embedded and high performance AI codebase directly depends on C/C++, and though languages like Rust and D are bubbling up, wouldn't it take massive amount of time to adopt the new languages, if ever adaptation begins?

It sure will. As for embedded systems, I doubt it would ever change away from C, definitely not to a functional language. C is quite low-level yet very high-level. It's at the exact sweet spot for embedded programming. You can be portable yet have access to all hardware.
Embedded programming is in direct violation of most of what's important in a functional language. The fact that you are working with hardware means that the functions can't be pure (in Haskell, they would be IO). So what would be the benefit of functional programming if most functions are not-pure and their results can't be cached? Besides, it's not even practical to load a whole interpreter of a functional language in a tiny microcontroller and hope to be able to use all sorts of dynamic caching during runtime to compensate for lack of non-functional language structures.
In short for microcontrollers, functional languages simply aren't fit. For "high performance AI codebases", there is again no reason why they can't be used. It's just that people have to pick up one of those languages and start using it. Biggest problem is though that People are lazy and don't like change.

Correct me, but it seems like a lot of time has passed and there are not many major production results from the AI community. I've heard about cognitive architectures of old like ACT-R and 4CAPS. They seem to be in hibernation mode!

They are not in hibernation mode. The nature of the research has changed. Take a look at a very brief history of AI on Wikipedia. Whereas in the 80s you would see for example a lot of algorithms developed on solving NP-complete problems, now you see artifical intelligence for example in Kinect that you usually don't think of as "intelligent".
There is also a lot of way more complicated research going on such as cognitive science and others. My personal feeling is that they are still maturing and not yet at a stage that could be widely used. Either that, or I have just been ignorant about it myself.

There seems to be a lot of work lately on otherwise intelligent systems (solved problems) like Computer vision and Data mining, but these problems cater to supercomputing and industrial crowd more. Could there be any possible shift towards low powered systems soon?

Could there? Yes. Would there? Probably not. Like I said before, heavy processing requires power and power is very precious on a (mobile) robot. There would however definitely be a middle-ground used in robotics. Perhaps not a super complete AI, but a dumbed down, simpler one (but still more intelligent than current) would be placed on robots.

