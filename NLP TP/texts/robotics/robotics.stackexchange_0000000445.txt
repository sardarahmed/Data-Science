Q:

How to obtain dense point clouds from stereo cameras?

I am trying to use a stereo camera for scene reconstruction, but I can usually only obtain sparse point clouds (i.e. over half the image does not have any proper depth information). 
I realize that stereo processing algorithms rely on the presence of texture in the images and have a few parameters that can be tweaked to obtain better results, such as the disparity range or correlation window size. As much as I tune these parameters, though, I am never able to get results that are even remotely close to what can be obtained using an active sensor such as the Kinect.
The reason why I want that is because very often point clouds corresponding to adjacent regions don't have enough overlap for me to obtain a match, so reconstruction is severely impaired.
My question to the Computer Vision experts out there is the following: what can I do to obtain denser point clouds in general (without arbitrarily modifying my office environment)?

A:

You can try to skip the salient point detection, and just densely sample over the image (as grid or so) and compute a feature descriptor at every sample point. You can probably even go as far as computing a descriptor for every pixel.
You might lose scale-invariance, but I think this won't hurt too much for stereo vision as objects will be at approximately the same scale in both images.
Another approach might be to combine multiple salient point detection algorithms: corners, edges, blobs and so on. Then you have to use the same feature descriptor algorithm for all detected points, however this latter part could be somewhat tricky to implement.

A:

So the stereo image processing algorithms I have used in the past were implemented pixel by pixel. We just used the pinhole camera model and did some old fashioned measurements with measuring tape until our depth estimations matched the real thing.
The equations for a pair of parallel cameras are:

$d = $half the distance between the cameras
$f = $the focal length of the cameras (assumed to be the same)
Coordinate Frames: 

$x, y, z = $ coordinate frame between the cameras (i.e. the camera base frame)
$u_R, v_R$ camera coordinates in the right camera from the perspective of the robot (u is horizontal, v is vertical)
$u_L, v_L$ camera coordinates in the left camera 
Note: the camera coordinates have their origins at the coordinate frame between the cameras (i.e. the u axes face opposite directions)

$u_L = \frac{f(x-d)}{z}$, $u_R = \frac{f(x+d)}{z}$
$zu_R = f(x+d)$, $zu_L = f(x-d)$
$z(u_R - u_L) = 2df$
$z = \frac{2df}{u_R - u_L}$
$y = \frac{v_L*z + df}{f}$
$x = \frac{u_L*z + df}{f}$
Using these equations you can compute a dense stereo cloud.  One for each pixel on your cameras.

