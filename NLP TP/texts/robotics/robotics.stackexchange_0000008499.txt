Q:

Mapping between camera pose and image features in visual servoing

I have a robotic arm and a camera in eye-in-hand configuration. I know that there is a relationship between the body velocity $V$ of the camera and the velocities $\dot s$ in the image feature space that is $\dot s=L(z,s) V$ where $L$ is the interaction matrix. I was wondering if one can find a mapping (a so called diffeomorphism) that connects the image features' vector $s$ with the camera pose $X$. All I was able to find is that it is possible to do that in a structured environment which I don't fully understand what it is.

A:

Image features are connected to the camera pose through two steps: (1) the relationship between the feature pixel coordinates and its homogeneous coordinates in the camera reference frame, and (2) the relationship between the camera reference frame and the world frame. Take a look at this figure, where the world frame is denoted with subscript 0 and the camera frame is denoted with subscript C. A feature is shown as a blue dot, with position $p$ in the world frame.

The camera has a particular field of view (shown with a pyramid in the image), which relates the pixels coordinates to the relative position of the feature in the camera reference frame, $\tilde{p}$, through the camera projection matrix:
$\begin{bmatrix} I \\ J \\ 1 \end{bmatrix} = \begin{bmatrix} k_x & 0 & C_x \\ 0 & k_y & C_y \\ 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} X \\ Y \\ 1 \end{bmatrix}$
Where $I$ and $J$ are the pixel coordinates of the feature in the image, and the camera is defined with parameters $k_x$, $k_y$, $C_x$ and $C_y$ (based on the field of view and output image size). The homogeneous coordinates, $X$ and $Y$ are defined based on the relative position of the feature in the camera frame:
$\tilde{p} = \begin{bmatrix} \tilde{x} \\ \tilde{y} \\ \tilde{z} \end{bmatrix}$
$X = \frac{\tilde{x}}{\tilde{z}}$
$Y = \frac{\tilde{y}}{\tilde{z}}$
That relative position of the feature is then related to the actual position of the feature (in the world frame) as well as the camera pose, according to:
$p = R_C \tilde{p} + p_C$
Where $R_C$ is the rotation matrix defining the camera orientation and $p_C$ is the position of the camera.
One thing I have not discussed is lens distortion, which must be accounted for before using this approach but can be treated separately. Note that the OpenCV Camera Calibration and 3D Reconstruction page provides a more detailed explanation.

