Q:

DRC Laser / Stereo Disparity between Collision and Visual Elements

It is fantastic to have DRCSIM 2.0 and Gazebo 1.4, thanks! Obviously a huge part of the DRC competition is rectifying data between stereo and laser, but these produce differing results. Robots can only interact with collision boundaries based on objects; if it can't collide it can't affect.
The disparity is that the /multisense_sl/camera/points2 data produces point clouds based on visual elements, while /multisense_sl/laser/scan produces point clouds based on collision boundaries.
Visual data is not useful if it doesn't lead to a means of interaction.

Originally posted by klowrey on Gazebo Answers with karma: 41 on 2013-02-05
Post score: 0

A:

I believe this pull-request will resolve this issue:
https://bitbucket.org/osrf/gazebo/pull-request/188/missing-gpu-laser-ray-sensor-in-13-issue/diff
It will enable the use of render-based ray sensor.

Originally posted by nkoenig with karma: 7676 on 2013-02-05
This answer was ACCEPTED on the original site
Post score: 0

Original comments
Comment by ThomasK on 2013-02-05:
Partially ... GPU laser sensor is capped at 90 degrees as it's intended to be used for sensors with a 2d image sensor I think (e.g. TOF sensors or Kinect or other depth cameras), but you could use the GPU ray sensor to extend it to a lidar sensor, see my suggestion in my comment here: https://bitbucket.org/osrf/gazebo/issue/309/use-of-visual-model-instead-of-collision
Comment by nkoenig on 2013-02-05:
The GPU laser sensor can render a full 360 degrees. At least the test I've seen has shown that it can.
Comment by ThomasK on 2013-02-05:
Ah it's the vertical fov that is restricted to 90 degrees, it was late already yesterday :) ... all good then I guess, great

