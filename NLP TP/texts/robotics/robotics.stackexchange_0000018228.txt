Q:

How to perform active search in point feature based monoSLAM?

I am modifying an implementation of SLAM with single camera, MonoSLAM [1].  Instead of image patches, I want to use features points (ORB) to track landmarks. MonoSLAM uses a EKF framework. So for every feature tracked, I have a covariance matrix which defines the uncertainty of the measurement in current camera frame. 
Had this been an image patch, as in Davidson et al [1], one has to center a sliding window with every pixel inside the ellipse defined by the covariance.  Then check for correlation of pixel intensities between the tracked patch and the patch from the new frame. Some thing like this figure below.

For features, should I search in a similar fashion by calculating the descriptor for each pixel in the ellipse? 
or
Since features come with their own detectors, should I leave it to the detector to find the list of features within the ellipse and then match them with my previous frame feature?
[1]: Davison, Andrew J., et al. "MonoSLAM: Real-time single camera SLAM." IEEE Transactions on Pattern Analysis & Machine Intelligence 6 (2007): 1052-1067.

A:

This is not the answer of your question but ORB is a feature detection algorithm. Feature detection and tracking are different. Once you found a feature it is better to track them by a feature tracking algorithm such as KLT. You can also track the features by feature matching algorithms but it is not very efficient.
Since features come with their own detectors, should I leave it to the detector to find the list of features within the ellipse and then match them with my previous frame feature?
-> Yes
The ellipse is usually to find a Mahalanobis distance to fairly count the uncertainty. If Mahalanobis distance of the two matched features is close enough, it can be used. This is how you find a matched features in EKF slam. Just remember that this is a quite old way.

