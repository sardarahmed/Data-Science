Q:

simulating kinect sensor in Gazebo

I want to use the input from a kinect in simulation. I use ROS and therefor I think the best way is to use gazebo for simulation. Is there a way to simulate the kinect input data in gazebo?
Thanks for help

Originally posted by Stefan on ROS Answers with karma: 90 on 2011-04-21
Post score: 6

A:

There is a Gazebo BlockLaser plugin which is able to simulate point cloud producing sensors (but there's no RGB value per point available easily AFAIK). See http://www.ros.org/doc/api/gazebo_plugins/html/group__GazeboRosBlockLaser.html. I added such a block laser to a erratic_robot for testing, the working xacro macro I used is here:
http://pastebin.com/F5Jamf78
Note that I had to make quite a few changes to how it was used in the API doc to make it work. Also note that it's performance isn't too good, so simulating a 640x480 point cloud or such will probably result in a severe slow down.
Luckily, there's work in progress regarding this topic, for some info see:
http://answers.ros.org/question/715/how-can-i-speed-up-simulation-in-gazebo
In the long answer there, Kevin Watts talks about changes to Gazebo for efficiently making depth camera data available.
http://ros-users.122217.n3.nabble.com/Simulating-RGB-D-sensors-in-Gazebo-td2518418.html
Basically same topic as this thread, only on the mailing list. There's also a link to the related ticket on trac.

Originally posted by Stefan Kohlbrecher with karma: 24361 on 2011-04-21
This answer was ACCEPTED on the original site
Post score: 4

Original comments
Comment by malharjajoo on 2018-02-01:
Hi, nice answer. you say the performance isn't too good, but what about the point cloud ?
Is it accurate ?
Comment by malharjajoo on 2018-02-01:
Also, the first link is broken.

