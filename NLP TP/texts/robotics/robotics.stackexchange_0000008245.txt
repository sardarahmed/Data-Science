Q:

Aligning datasets with drift

I have a dataset that contains position information from tracking a robot in the environment. The position data comes both from a very accurate optical tracking system (Vicon or similar) and an IMU. I need to compare both position data (either integrating the IMU or differentiating the optical tracking data).
The main problem is that both systems have different reference frames, so in order to compare I first need to align both reference frames. I have found several solutions; the general problem of aligning two datasets seems to be called "the absolute orientation problem".
My concern is that if I use any of these methods I will get the rotation and translation that aligns both datasets minimizing the error over the whole dataset, which means that it will also compensate up to some extent for the IMU's drift. But I am especially interested in getting a feeling of how much the IMU drifts, so that solution does not seem to be applicable.
Anyone has any pointer on how to solve the absolute orientation problem when you do not want to correct for the drift?
Thanks

A:

You gave two methods of comparison - "either integrating the IMU or differentiating the optical tracking data". I'm talking about one basic transform to convert from the vision frame to the global frame and vice-versa.
One transform, that you calculate in advance, that relates one origin to another origin. You seem to be talking about methods of best-fit to get good correspondence, but I'm saying to use one fixed, somewhat generic transform. 
All you need to do is shift from one frame to another. This will preserve drift.
:EDIT: 
I may have misunderstood the question, or your particular scenario. I thought that you could just go out and measure the location and orientation of the camera, measure the location and orientation of the robot, and make your own transform. 
In the event that you can't or don't want to (too time consuming, lots of trials, etc.), then you could "cheat" by exploiting the fact that drift is a slow event, where a very small error is added over a very large number of samples. 
What you might be able to try is to use the method you already found and apply that to only the first few samples of your data set. 
If your robot is in motion from the start of the set, then the first sample should be enough to fix a position and the second sample should fix a heading/rotation. Use the resulting transform you get from only these two samples and apply that transform to all the others. Noise may be an issue, but that seems to be the point of what you're trying to do. 
If you think it's too noisy or there's too much error, then you could try the algorithm with the first 3 samples, or first 4, or 5, etc. As you've noted, the more samples you include the more this will account for drift, but if you can't physically measure the transform you don't really have any other option. 

