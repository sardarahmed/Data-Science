Q:

Computer stereo vision simulator

In my research project I deal with a mobile robot that perceives through stereo vision. As the stereo input data I currently use several datasets taken from a passenger vehicle that contain real world photos. The datasets are good to get started but have a limited content so I would need to model my own traffic situations to further work on the stereo vision system.
I am thinking about using some kind of synthetic graphics simulation as the input for the stereo system. What are my options? I can imagine a 3D graphics rendering engine whose output would be fed as the input for the stereo vision could probably be used.
I found there are general robotic simulators available like Gazebo but since I am all new to robotic simulation I do not really know where to begin.
EDIT:
I forgot to write that all my code is a pure C++. I use OpenCV and LIBELAS for stereo vision and Point Cloud Library (PCL) for visualization. All glued together into a single C++ project and compiles into single binary.

A:

I highly advise against using synthetic image data for testing your stereo vision algorithms. What will happen is that you end up with a system that works excellent on your synthetic data, but poorly in the real-world. Synthetic images are much easier to process than real-world images, as they lack all the shortcomings of real cameras that will make stereo matching hard.
You can try it out yourself. There are some synthetic image sequences out there. Have a look at this one:
https://cerv.aut.ac.nz/set-2/

