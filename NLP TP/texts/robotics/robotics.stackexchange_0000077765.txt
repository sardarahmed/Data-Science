Q:

Is nav stack useable for "eagle eye" camera scenario?

We build a simple robot, without any sensors, just two motors. To detect the position and orientation of the robot, we use a camera mounted over the table, where the robot is running on.
I want to use the navigation stack for that setup, hopefully it makes the live easier, to get the best route for the robot. So far I've only read, that I need a LaserScan or PointCloud sensor on the robot, to fulfill the nav stack setup requirements.
And here I ask myself currently, if it is a good idea, to use the nav stack for my navigation scenario (changing/moving targets on the table and changing barriers).
Is it enough to setup tf / map / odom / acml / move_base / base_controller? I read always, that the LaserScan or PointCloud is important for the nav stack, but I don't have any data like that, but I know exactly, where my robot is :-)

Originally posted by Cyberdoc on ROS Answers with karma: 3 on 2016-12-09
Post score: 0

A:

move_base is basically just obstacle avoidance, path planning and path tracking.
If you have an accurate, external position reference that provides position and velocity data (similar to odometry), you can omit the other sensor inputs to move_base and it will plan as if it is in an empty world.
Most of the local planners rely on velocity feedback from the Odometry message, so you'll do need to spoof that if you want it to work well.
Note that amcl is a localization package, so if you're providing external localization, you don't need to run amcl at all. Instead, your localization system should publish the TF transform from map to base_link.

Originally posted by ahendrix with karma: 47576 on 2016-12-11
This answer was ACCEPTED on the original site
Post score: 0

Original comments
Comment by Cyberdoc on 2016-12-21:
Thank you for your answer.
I've created my own TF from map to base_link, and now I am able to use move_base to trigger move actions.

