Q:

Is it to possible to measure a room exact with rplidar?

I want to measure a room using rp3+, rplidar, ROS and SLAM.
It should be easy possible to create mapping of the room. But how exact it would be? Is it possible to get less than 1 cm precision over 10m?
Regarding the rplidar specification its possible, but it is also applies for real environment?

A:

You don't say which RPLIDAR you are talking about, but looking at the spec of the RPLIDAR A2M8 (the first hit on google), it has a range of 0.15 m to 12 m, and an angular resolution of between 0.45 and 1.35 degrees.
LIDAR in a fixed position in the centre of the room
We can use trig to calculate the distance to the corners of the room and convert angular resolution to linear resolution:
$$a^2 +b^2 = c^2$$
$$\tan A = a/b$$
based on

If your room 10m x 10m then Pythagoras' Theorem gives us 707 cm from the centre of the room to the corners.
With $A$ as the angular resolution and $b$ as the range, then $a$ is the linear resolution:
$$ a = b \tan A$$
If we assume that this LIDAR can achieve 0.45 degrees at 707 cm then plugging these values into the equation we get an 5.55 cm linear resolution.
I suspect that angular resolution and range are linked however, so angular resolution at 707 cm will probably be closer to 1.35 than 0.45 degrees. 
If we assume a more realistic 1 degree angular resolution (by linear interpolation), we get a 12.34 cm linear resolution.
Both of these are much larger than your desired 1 cm resolution, so no, this LADAR can't do what you want with a single 2D point cloud.
In order to get 1cm resolution over 10m with a LIDAR, you would need one with an angular resolution of just 0.08 degrees at 707 cm, which seems like quite a jump.
LIDAR on a movable platform
The maths change if you are able to move your LIDAR to any position within the room. Then you are limited by the number of point clouds you are prepared to measure, and some combination of the positional accuracy of your LIDAR and/or distribution of features.
If you take a point cloud every 100 cm (a 9x9 grid) then the maximum diagonal distance of each point cloud would be 141 cm which at 0.45 degrees is 1.11 cm, very close to your desired target.
The trouble is you now have 81 point clouds to fuse together.
The more point clouds you measure, the higher the resolution, but given the minimum distance of the LIDAR is 15cm, there is little point in taking a measurement more than every 30cm.
A 31x31 grid would have a separation of 31.25 cm and a maximum diagonal distance of 44.19 cm, which at 0.45 degrees would be 0.35 cm. 
Data fusion uncertainty and accumulated errors
Once you have all of your point clouds, you then have to fuse them together, and this is where it starts to get tricky.
To fuse the data you start to become limited by the resolution of the positional data you have for your point clouds.
If your LIDAR is on a mobile robot, you might be limited by the encoders and the accumulated error in position tracking as your robot moves between all of your positions.
You could use the point clouds to correct any positional errors, but that means identifying common features, and the further apart these features are, the lower resolution they become. If your room is a featureless void, the best data you have might be from the one point cloud from the centre of the room. All of the others point clouds have better data for some edges, but worse data for others.

