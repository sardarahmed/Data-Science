Q:

Localization by comparing current lidar scan with previous lidar scan

I have managed to use an ICP algorithm to produce a relative pose difference between a new lidar range scan and the previous lidar scan. When I tested it on individual scan pairs, the results look good and exactly as expected.
I have noisy odometry data [x, y, heading angle] in addition to the lidar range data (2D).
How do I actually use my comparison result to localise the robot? Is this possible to do without a typical SLAM technique like a particle filter ? I tried correcting the current odometry reading based on the comparison result, and setting this as my new position estimate of the robot, but the results do not look good. I know I cannot just use the raw odometry data on its own, but I thought correcting the current odometry readings with my relative pose estimate would be somewhat accurate and make the need for a SLAM filter unnecessary.

A:

So you want to perform sensor fusion. If you are using ROS, you have a bunch of options, two of which are fuse (which uses factor graphs) and robot_localization (which uses an EKF or UKF).
But you should note that both of the data sources you are fusing (wheel odometry and scan-to-scan matching) will have errors associated with each measurement. Over time, those errors will accumulate. So you're not really doing localization per se, but more incremental pose estimation. It may take some time for the pose to drift from reality, but it will.
If you want to keep the robot's error constrained, then you need to localize it by, e.g., matching the laser scans to a static map, like the ROS amcl package does. So you would build the map using a SLAM package, save that map, and then configure amcl to localize from it.

