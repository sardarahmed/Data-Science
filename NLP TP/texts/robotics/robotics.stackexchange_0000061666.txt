Q:

Mapping with Kinect sensor

I am working on a robot, which finally should move to the specified location in the map autonomously. (For which I am planning to use Navigation stack).
For which I want to map the environment.
I have a kinect sensor available, so which mapping I should use so that I can generate a map (2D or 3D), which can be later fed to the navigation stack....??
I can fetch the odo ticks from the micro controller and give for the mapping stack.
As the kinect is a fixed connection to the robot, I can publish a static transform from kinect to the base link.
Are these sufficient to use the mapping stack, but the question is which mapping stack to go with..? and any examples suggesting the use of the mapping stack will be great full to start with.
Many thanks in advance.

Originally posted by sumanth on ROS Answers with karma: 86 on 2014-08-11
Post score: 0

A:

For the task of reaching goal  2d map is sufficient.
First generate the map by moving robot manually . Use gmapping with 2d laser and odometry.
(use depthimage_to_laserscan to generate laser scan from kinect data)
When you have map use amcl for localisation. This tutorial of Autonomous Navigation of a Known Map with TurtleBot is helpful.

Originally posted by bvbdort with karma: 3034 on 2014-08-11
This answer was ACCEPTED on the original site
Post score: 2

Original comments
Comment by sumanth on 2014-08-11:
Thanks,
but then where the 3D map is used..?
Comment by bvbdort on 2014-08-11:
check the videos here http://wiki.ros.org/3d_navigation. Of course you can use 3d maps for navigation. try rgdb slam but i never get chance to use 3d navigation.
Comment by sumanth on 2014-08-12:
but does navigation stack takes the map from the gmapping.?
Comment by sumanth on 2014-08-12:
Are there any exampels/tutorial to generate the map by moving robot manually . Use gmapping with 2d laser and odometry..?
Comment by bvbdort on 2014-08-12:
first you need to build the map using gmapping, then use map for navigation. Which robot are you using ? and how you are driving it ÃŸ
Comment by sumanth on 2014-08-12:
I am using my custom built robot, its a differential drive mobile robot, plan to drive the robot for mapping is using commands from laptop (similar to teleop on turtlebot).
Comment by bvbdort on 2014-08-12:
start with building map by driving robot
Comment by sumanth on 2014-08-13:
@bvbdort, I tried mappin with the gmapping but was not sucessful.
Steps followed:

open the kinect with openini using "roslaunch openni_launch openni.launch".
pointcloud to the laser scan using "rosrun depthimage_to_laserscan depthimage_to_laserscan image:=/camera/depth/image_raw"

Comment by sumanth on 2014-08-13:
3. run the gmapping with "rosrun gmapping slam_gmapping scan:=/scan tf:=/odom"
I am running the node which publishes the odom (the odometry data from the real robot).
4. Then open rviz with "rosrun rviz rviz".
But nothing I can see in rviz.
What am I missing here..?
Comment by bvbdort on 2014-08-13:
can you share  rqt_graph picture for more info. i think you should start gmapping like "rosrun gmapping slam_gmapping scan:=scan _odom_frame:=/odom"
Comment by sumanth on 2014-08-13:
I have created a separate question to keep the forum organised.
link for the question: http://answers.ros.org/question/189963/gmapping-problem/

