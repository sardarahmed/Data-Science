Q:

it's worth to make a line follower using a raspberry pi and a web cam?

I wonder if this would be a competitive robot compared with one made with a traditional approach using a microcontroller and infrared sensors. I suppose that raspberry can perform an edge detection to tell the dynamic of the line far away, much more that the infrared sensor, but how fast can the raspberry do this process? should be a relative simple process in terms of computational requirements , an edge detection in a high contrast arena. Probably the bigger issue would be get the relative position of the robot respect to the line, may be a combination of the camera with some infrared sensors would work better, and what about the size? the robot will be significantly bigger when is used a camera and a raspberry for the control.

A:

This may be overkill, but some of the past work I was involved in was trying to detect a vertical line (a pipe) in the camera's field of vision, and navigate relative to it.

The process was as follows:

Pick a threshold value, and split the image into black and white
Perform Canny edge detection on the image
Use a Hough transform to find the strongest lines in the image, and consider those to be the boundaries of the vertical strip you're looking for
Adjust for the perceived width (indicating distance -- you won't need this) and angle of the vertial strip in the image

We used OpenCV to do this.  
We didn't get very good results at full camera resolution; The very pixellated lines in the image above were from downsampling the source video frame (by a factor of 3 in both directions, I think... so 1/9 the number of pixels to worry about).  This allowed us to process frames fast enough for it to be useful.  Also, the point of the initial threshold step was to speed up the Canny process -- fewer lines to find.

