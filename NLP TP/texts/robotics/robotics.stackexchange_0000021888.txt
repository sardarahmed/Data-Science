Q:

Localization and Mapping in a Small Crop Field

I have a small crop field inside my house and I want to create a map of it. The goal is to create a map so that I can monitor the growth of the plants and do some post processing to analyze the details of individual plant.
Hardware: Since it is a small crop field, I have two poles on either ends of the field with a wire connecting the two poles. the robot/assembly will be hooked to the wire. Therefore, the path of the robot is always fixed. I have a 3D lidar, a camera, IMU and a GPS.
I have interfaced all the sensors with my Nvidia TX2 controller using ROS. In short, I have raw data coming into the system from all sensors. I want some advice regarding how to proceed with this project?
Should I start with localization? As a first step, should I fuse data from the GPS and IMU to get accurate positioning of the system? With the idea that, if I accurately localize my system in the field and then acquire images for every position, is it possible to join the images somehow?
Do I need to implement SLAM here?

A:

If you intend to stitch images together, you don't need to strictly implement slam. As long as there is good overlap between images, you can use SIFT/SURF features to extract matching keypoints and stitch the images together. Take a look at this tutorial on panorama creation.
However, given your setup, I believe SLAM would be the way to go! since you already have ros setup, I'd suggest going forward with the robot_localization package. You can use this package to fuse the GPS and IMU data to get an accurate location of your robot and then use that to transform your images and lidar pointclouds to create a map. To create a map with lidar pointcloud you can use iterative closest points or use a package like rtabmap/octomap.
As a stretch, you can even fuse your lidar pointcloud with images to colorize your map.

