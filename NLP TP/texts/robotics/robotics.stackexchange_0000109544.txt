Q:

Further clarification on marginalization

I was re-reading a prior answer about marginalization along with some documents,
Marginalization vs Dropping states for sliding window VO
My understanding is as follows:

After convergence of the objective function(say which includes marginalization prior), you do a first order expansion of the objective function.

You select the nodes you want to remove from the factor graph.

You can write the optimal value of those nodes you want to remove parameterized by the remaining nodes you keep in the expansion.

The confusion I have is that the schur-complement examples I have seen only shows the dependency when solving for the normal equations for all nodes currently in the factor graph. When writing the dependencies don't you want to show the optimal values for the removed nodes depending upon the remaining nodes which are the ones to be optimized again in the future?

A:

So I did some further investigation.

The marginalization arguments hinge on linearizing the control and and observation functions, once linearized you can factor the joint PDF into a joint gaussian. Once in gaussian form you can do marginalization as you would any distribution.

I believe basically doing a ridge maximum likelihood by optimizing the "nuisance variables" in terms of the variables of interest as I talked about in the OP will produce the same result.

Anyway the key point behind any discussion of marginalization comes down to linearization and converting to joint gaussian. If you can't do that you won't be marginalizing anything, not in closed form at least.

