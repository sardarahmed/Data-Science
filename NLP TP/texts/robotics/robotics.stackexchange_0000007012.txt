Q:

algorithm Simple Stereo Vision

I'm working in a project implementing a vision system. I'm a student and this is the first time I'm doing something like this, it has been a challenge.
I'm using a controller (Netduino+2, .Net MicroFramework) and a camera (CmuCam5 - Pixy) and for now it's working well. I'm communicating with the robot(Fanuc M430iA) using Modbus, and aquiring the data from the camera using I2C. 
But, the next challenge is using 2 cameras to implement stereo vision and I'm not shure how to achieve that. I'm reading a lot about that and I understand the process and generally how it works, but I think my case is very specific.
My cameras detect the center of an object and give me the coordinates, so, I have that, and that's good. 
What do you think it's the more reasonable approach?
(sorry for my english, let me know if I'm not being explicit, I'll edit the question if I see there's not enough information)

A:

The first thing is to make sure that the cameras will get the coordinates of the object at the same time (I don't know if Pixy has a FREX or STROBE signal for synchronization), or that the object is not moving.
Then, have a look at OpenCV, it has a section on 3D calibration and reconstruction (i.e. find the depth of an object based on the coordinates of two pictures or frames, taken at the same time, from two different (known) angles).
As you start with the 2D coordinates and not the whole image, I guess you will have to dig in and adapt the examples (especially for the calibration part), but it might be a good place to start with
The official doc:
http://docs.opencv.org/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html
Other links worth checking:
http://wiki.ros.org/camera_calibration/Tutorials/StereoCalibration

A:

To further explain my comment, I've decided to turn it into an answer.
Using the law of sines to calculate distance.
First mount the cameras on the robot parallel like this ---¦---   ---¦---
¦ is the lens. Mount them 100mm apart - measuring from the lenses.
Second find the viewing angle of your camera, in this case 75*.
Third get data from the camera: Cam1 : (50, 50); Cam2 : (10, 50);
Fourth (optional depending on the co-ordinate system used) Normalize the data (transform the co-ordinate system) to center the origin and re-map your co-ordinates.
Fifth Calculate your degrees per division - take your viewing angle and the range of the supplied co-ordinate system and calculate how many degrees are represented per division.
Sixth multiply this Degrees per division by each of your co-ordinate values. To get the degrees from center line to the object for each camera.
Seventh using the Law of Sines you can calculate the distance from each camera to the object using both of the angles you calculated plus the distance between the camera lenses. Optionally average them to get the distance from the camera mounting mid-point to the object.
There are many optimizations that can be applied to this process, especially calibrating the camera lens but that's another question. I would say that errors do creep in and taking multiple readings and averaging the result can improve reliability.

