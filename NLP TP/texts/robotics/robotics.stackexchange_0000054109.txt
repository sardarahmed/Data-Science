Q:

Getting Raw Data from Turtlebot Simulator

Currently running the newest version of turtlebot_simultor and everything is running very well on hydro.
I'm working on a project where I'm trying to integrate POMDP with the turtlebot. I'm trying to get the simmulation working with POMDP first before moving to the actual turtlebot while I'm still testing the Policy and Model.
How can I get raw data from the kinect sensor in the simulation. I've seen the point cloud map it can create in rviz which is useful for one part of the project but I currently need a way to extract a depth value for following a target at a set distance. I'd like to write a small script that just listens to the data being output from the sensor node and passes the important data into the policy and then send the move command somehow to the turtlebot.
Eventually this will be used for a tracking and facial recognition project. The aim is to see how the turtlebot will act while following a target with uncertainty. Depth will be important for following from a certain distance (Which is the first goal) and then after that will be the bot moving to see a face that will be run through a facial recognition program. This last goal will require an image output to be scanned.
Does anyone know of a way to achieve something like this? Some form of automated control?
Thanks.

Originally posted by Alkaros on ROS Answers with karma: 103 on 2013-09-15
Post score: 0

A:

Welcome to ROS Answers, Alkaros!
After reading through your question I believe the point cloud is actually what you are looking for. There is already an application in the turtlebot_follower package, which does something similar to what you are trying to achieve. It uses the point cloud generated by the 3D sensor - RViz is not creating the pointcloud, but only visualising it - to track peoples legs in order to follow them. For this the depth information of the point cloud is used.
However, if for other reasons you like to use depth information only, then you should look at the /camera/depth/image_raw ROS topic. Through it you can receive the depth data of the 3D sensor as an image stream.
Hope this helps!

Originally posted by bit-pirate with karma: 2062 on 2013-09-15
This answer was ACCEPTED on the original site
Post score: 1

Original comments
Comment by Alkaros on 2013-09-16:
Should the depth image topic show upin gztopic list?
Comment by tfoote on 2013-09-16:
It you want it to work on the real robot you should use rostopic list not gztopic list.
Comment by bit-pirate on 2013-09-16:
The mentioned depth image topic is available through ROS.
Comment by Alkaros on 2013-09-16:
Excellent, what is the best way to send movement commands. Is it just simply to stick to teleop and have the script that is processing the policy send teleop commands? Sorry if this is a simple question.
Comment by bit-pirate on 2013-09-16:
Just let your script send the velocity commands directly. You can get inspiration on how to do that from the kobuki_keyop and turtlebot_teleop packages. And please remember to mark you question as solved, if you're happy.
Comment by Alkaros on 2013-09-16:
Should "rosrun image_view image_view image:=/camera/depth/image_raw" Work? It works with the RGB image, the depth image doesn't show anything using that command. Thanks.
Comment by bit-pirate on 2013-09-16:
It should. I can see the depth image.
Comment by Alkaros on 2013-09-16:
Any clues to why it wouldn't? It just opens up as a pain gray Window. Nothing that I can interpret as actual data in it.
Comment by bit-pirate on 2013-09-16:
That's hard to tell. I suggest to open a new question for this specific problem. Please describe how to reproduce your problem in detail. Don't forget to add information about the software versions you are using (ROS distro, deb or source install, etc.).
Comment by Alkaros on 2013-09-18:
http://answers.ros.org/question/80946/turtlebot-depth-image-not-displaying-with-image_view-with-gazebo/

