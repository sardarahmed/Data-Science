Q:

Gazebo not working in ros kinetic singularity image

I am trying to create an emika_franka_panda robot simulator using the osrf/ros:kinetic-desktop-full docker image as explained in this topic. To do this i created the following definition file:
Bootstrap: docker
From: osrf/ros:kinetic-desktop-full

%post  
    # Initiation commands
    apt-get update

    # Setup ros dependency tool
    rosdep update

I then tried to create the image by using the following command:
sudo singularity build --sandbox ros_kinetic_panda ros_kinetic_panda.def

Following when running the container using:
sudo singularity shell --nv --writable ros_kinetic_panda

and trying to launch gazebo using:
gazebo

I run into troubles. The gazebo interface tries to launch but crashes with the following errors after 1 second.
Singularity panda_kinetic:~/singularity/containers> gazebo
libGL error: libGL error: No matching fbConfigs or visuals found
No matching fbConfigs or visuals found
libGL error: failed to load driver: swrast
libGL error: failed to load driver: swrast
Gtk-Message: Failed to load module "appmenu-gtk-module"
Gtk-Message: Failed to load module "gail"
Gtk-Message: Failed to load module "atk-bridge"
Gtk-Message: Failed to load module "canberra-gtk-module"

What I already tried to fix this issue
Use the melodic docker image
Bootstrap: docker
From: osrf/ros:melodic-desktop-full

%post
    echo "Setting up ros melodic emika franka container."

    # Initiation commands
    apt-get update

    # Setup ros dependency tool
    rosdep update

Now the gazebo simulator launches without any problems.
Trying to build from the apt-get repository
In this case I tried to first bootstrap from the ubuntu 16.04 repository and following trying to install ros kinetic by using the installation guide. This gave me the following errors when trying to run the gazebo simulator.
libGL error: No matching fbConfigs or visuals found
libGL error: No matching fbConfigs or visuals found
libGL error: failed to load driver: swrast
libGL error: failed to load driver: swrast
terminate called after throwing an instance of 'std::runtime_error'
  what():  locale::facet::_S_create_c_locale name not valid
Singularity ros_test:~> gazebo
libGL error: No matching fbConfigs or visuals found
libGL error: failed to load driver: swrast
libGL error: No matching fbConfigs or visuals found
libGL error: failed to load driver: swrast
terminate called after throwing an instance of 'std::runtime_error'
  what():  locale::facet::_S_create_c_locale name not valid

System information

PC: Hp Zbook G3
Root linux system: Ubuntu 18.04
Graphics driver: Quadro M1000M
Cuda versions: 10.0 (cudnn 7.4.2) and 9.0 (cudnn 7.5.0)
Singularity version 3.0.3

nvidia-smi output

Shell output log
VERBOSE: Set messagelevel to: 4
VERBOSE: Container runtime
VERBOSE: Check if we are running as setuid
VERBOSE: Spawn scontainer stage 1
VERBOSE: Get root privileges
VERBOSE: Execute scontainer stage 1
VERBOSE: Get root privileges
VERBOSE: Create mount namespace
VERBOSE: Spawn smaster process
VERBOSE: Spawn scontainer stage 2
VERBOSE: Create mount namespace
VERBOSE: Spawn RPC server
VERBOSE: Execute smaster process
VERBOSE: Serve RPC requests
VERBOSE: Found 'bind path' = /etc/localtime, /etc/localtime
VERBOSE: Found 'bind path' = /etc/hosts, /etc/hosts
VERBOSE: Checking for template passwd file: /usr/local/var/singularity/mnt/session/rootfs/etc/passwd
VERBOSE: Creating passwd content
VERBOSE: Creating template passwd file and appending user data: /usr/local/var/singularity/mnt/session/rootfs/etc/passwd
VERBOSE: Checking for template group file: /usr/local/var/singularity/mnt/session/rootfs/etc/group
VERBOSE: Creating group content
VERBOSE: Execute scontainer stage 2

Shell output after fix log
    VERBOSE: nvidiaContainercli returned: no nvidia-container-cli present: exec: "nvidia-container-cli": executable file not found in $PATH
VERBOSE: Falling back to nvliblist.conf
VERBOSE: Set messagelevel to: 4
VERBOSE: Container runtime
VERBOSE: Check if we are running as setuid
VERBOSE: Spawn scontainer stage 1
VERBOSE: Get root privileges
VERBOSE: Execute scontainer stage 1
VERBOSE: Get root privileges
VERBOSE: Create mount namespace
VERBOSE: Spawn smaster process
VERBOSE: Spawn scontainer stage 2
VERBOSE: Create mount namespace
VERBOSE: Spawn RPC server
VERBOSE: Serve RPC requests
VERBOSE: Execute smaster process
VERBOSE: Found 'bind path' = /etc/localtime, /etc/localtime
VERBOSE: Found 'bind path' = /etc/hosts, /etc/hosts
VERBOSE: Checking for template passwd file: /usr/local/var/singularity/mnt/session/rootfs/etc

Originally posted by rickstaa on ROS Answers with karma: 111 on 2019-04-10
Post score: 0

Original comments
Comment by gvdhoorn on 2019-04-10:
Your image doesn't contain any graphics drivers (or at least: not the level of 3D support that Gazebo requires).
--nv is the right approach, but with versions of Singularity newer than 2.5.x/2.6 you'll need to take additional steps, as not all required files (notably: OpenGL related) are injected into your container.
It's not trivial, but I believe the Singularity user documentation should document this.
Comment by gvdhoorn on 2019-04-10:
Also:

sudo singularity build --sandbox ros_kinetic_panda ros_kinetic_panda.def

If you're using --sandbox here (and --writable with shell) because you have included your workspace in your image, then don't. Build your image to include the necessary dependencies, but keep your workspace out of the image. You can use rosdep to install all required dependencies (after having copied the workspace temporarily into your image build dir, then remove after rosdep has finished).
Comment by rickstaa on 2019-04-10:
Thanks a lot for your answer @gvdhoorn! And the tip about the --sandbox and --writable options. I review the documentation again and I now better understand their usage. Based on the documentation I indeed thought the --nv option solved the driver problems I encountered previously. After your suggestion, I tried to install 2 OpenGL packages that were missing to my images but this did unfortunately not work either.
Do you maybe know what the best way is to solve this problem? I have a hard time finding a good guide on how to do this. Do I need to use the old nvida-HPC-docker solution, use the nvidia-GPU-docker image instead of the ros-kinetic-image, use the gpu4singularity package or copy the needed nvidia .run files from my host system to my image container.
Thanks a lot in advance
Comment by gvdhoorn on 2019-04-10:
Question: do you actually have NVIDIA hardware in your system?
If so: can you show the output of singularity -v shell --nv /path/to/your/image?
Comment by rickstaa on 2019-04-10:
Thanks a lot for looking into it my problem. For clarity, I added my system specifications and the singularity shell output to the question above.
Comment by gvdhoorn on 2019-04-10:
Which version of Singularity is that? And can you show the command line you used to start it (unless it was exactly the command I suggested)?
I would've at least expected to see some Found NV library: .. lines in there.
Comment by rickstaa on 2019-04-10:
Yea that is strange it is singularity 3.0.3 with the singularity -v shell --nv ros_kinetic_panda command in the /bin/bash/ command line.
Comment by rickstaa on 2019-04-10:
I solved my issue with the tips of @gvdhoorn. The problem was caused by my Nvidia-driver after having purged the driver according to this guide link text and reinstalling a newer driver my issue disappeared.
Comment by gvdhoorn on 2019-04-11:
This should work regardless of how you've installed the driver (and additionally: installing NVIDIA drivers using the pkgs from nvidia.com is not necessarily the best way to go about things on Ubuntu, which is also what the guide you link to states I believe).
If it works for you though, that's ok.
If you could show the output of singularity -v shell --nv /path/to/your/image.simg now that it works that would be nice, as we should be able to see the difference in the log.
Comment by rickstaa on 2019-04-11:
Ah thanks for the comment about the pkgs.
Comment by rickstaa on 2019-04-11:
So although gazebo works now there is still some problem with the nvidia-container-cli. Does this mean that GPU is not enabled in the container?
Comment by gvdhoorn on 2019-04-11:
Could you please not use comments for such large copy-pastes of console text? Just edit it into the question next time.

So although gazebo works now there is still some problem with the nvidia-container-cli. Does this mean that GPU is not enabled in the container?

No, things are working. Singularity will first try nvidia-container-cli, if that doesn't work (or can't be found), it'll fall back to nvliblist.conf. You also see this in the log you included:

Falling back to nvliblist.conf

(and I believe this is actually beneficial in this case, as nvidia-container-cli does not include the OpenGL libraries (last time I checked), so accelerated OpenGL would still not work in that case. As nvliblist.conf is used, that's not a problem and things work.
Comment by rickstaa on 2019-04-11:
Ah, thanks a lot :). I added the output log to the question as a reference. As you stated that the script should work regardless of how the drivers are installed, what do you think did fixed my problem? So people with the same problem know how to tackle the errors I first encountered.
Comment by gvdhoorn on 2019-04-11:
I would have to check how Singularity checks for the presence of the NVIDIA drivers.
Did you have a driver installed previously? And how did you install that one?
Comment by rickstaa on 2019-04-11:
Before using NVIDIA driver metapackage from nvidia-driver-418 (opensource) i was using NVIDIA driver metapackage from nvidia-driver-418 (opensource). The earlier driver was installed via  sudo ubuntu-drivers autoinstall the new driver was installed using sudo apt-get install nvidia-driver-418.
Comment by gvdhoorn on 2019-04-11:
The only OSS driver for NV hw is nouveau. nvidia-driver-418 is not OSS.

the new driver was installed using sudo apt-get install nvidia-driver-418.

Ok, so in the end you did not install anything from nvidia.com?
In any case: this should indeed all work.
You might want to post what you did as an answer and then accept your own answer.
Comment by rickstaa on 2019-04-11:
Ah, your right I had the nouveau driver but since I could not get my pc to work I installed the nvidia-415 driver through the GRUB menu terminal. I did not install anything from nvidia.com. Thanks for all the information. I will post my answer below. I, unfortunately, cannot accept my answer due to being new on the forum. I think a  moderator has to do this for me.
Comment by rickstaa on 2019-04-29:
Dear @gvdhoorn altough gazebo seems to work now I still get the errors when I run the image without sudo singularity run --nv ros_kinetic_panda.
Gtk-Message: Failed to load module "appmenu-gtk-module"
Gtk-Message: Failed to load module "gail"
Gtk-Message: Failed to load module "atk-bridge"
Gtk-Message: Failed to load module "canberra-gtk-module"

Do you maybe know what causes this? I included chmod 755 /root in my recipe file and execute xhost local:root > /dev/null before running the image.
Comment by gvdhoorn on 2019-04-29:
This has to do with theming, You can ignore those.
I'm not sure why you are running chmod or xhost for that. There is no need. And the whole point of using Singularity here is partly to make integration with your X server easier, so xhost is not needed at all.
Comment by rickstaa on 2019-04-29:
@vgdhoorn Thanks for the clarification!

A:

With the comments made by @gvdhoorn I managed to solve the issue by reinstalling my driver. In my case I needed the NVIDIA driver metapackage from nvidia-driver-418 (opensource) driver instead of the NVIDIA driver metapackage from nvidia-driver-415 (opensource).
Although I'm not totally sure what the exact issue was this might solve the issue for other people as well. To find the right driver for your video card run the nvdia-smi command to get the name of your video card. Following you can find which driver you need on the nvidia driver site. The new driver can then be installed according to this guide.

Originally posted by rickstaa with karma: 111 on 2019-04-11
This answer was ACCEPTED on the original site
Post score: 2

