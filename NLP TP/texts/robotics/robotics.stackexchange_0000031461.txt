Q:

Solutions for visually aided 2D/3D SLAM for UAV with ROS

I've been wondering what possibilities are there to do visually aided 2D/3D SLAM for UAV in indoor settings with ROS. I've got an IMU talking to ROS, a Kinect sensor and two UVC cameras. What I originally thought would work is feeding robot_pose_ekf with vslam_system's visual odometry (/vo) and IMU data, but I'm having trouble (described here) obtaining /vo. So I'm wondering what other possibilities do I have.
My general goal is very similar to Rainer Hessmer's, but with a cheap DIY quadrotor platform. On the beginning I'd use sonar based altitude hold and assume no pitch/roll for this to hopefully work. Similar projects I'm so far aware of are:

As mentioned above, assuming I can make vslam_system work
A great solution I can only dream of copying is MIT's SLAM with Kinect on a Quadrotor, their code is not yet and will not shortly be released
Patrick Bouffard's Quadrotor Altitude Control and Obstacle Avoidance, similar, could maybe be integrated with gmapping using pointcloud_to_laser to provide SLAM
As suggested here I could maybe use rgbdslam. I'd need to extract 3D odometry estimates from rgbdslam and fuse them with IMU's readings to eliminate drift. Would this be possible?

I'm open for all suggestions on feasibility of the above and beyond. The main problem I'm having is obtaining visual odometry with Kinect or stereo cameras. Any help appreciated. I'd be more than happy to open and describe my solution once I get it working.

Originally posted by tom on ROS Answers with karma: 1079 on 2011-05-04
Post score: 3

A:

When using rgbdslam I'd assume that the computational limitations on the UAV and the bandwidth limitation could be a problem.
This could be addressed, by sending only (downsampled?) monochrome (do you need color?) and depth image over wireless, do registration and backprojection offboard (and change the pcl-point type used by rgbdslam to XYZ only plus some changes in the UI do adapt to monochrome).
Also, rgbdslam has no motion model yet, so you would need to code the integration of the imu data. You could just integrate another subscriber-callback, that integrates all imu readings and, on insertion of a kinect frame to the pose graph, adds an edge that represents the summed up transformation with an appropriate information matrix.

Originally posted by Felix Endres with karma: 6468 on 2011-05-05
This answer was ACCEPTED on the original site
Post score: 4

Original comments
Comment by Felix Endres on 2011-05-08:
You could do so, by comparing only to the last frame and removing the graph optimization. However, building that from scratch with opencv2 and pcl might be less pain, than removing 90% of rgbdslam to dig out the desired code.
Comment by tom on 2011-05-08:
Thanks Felix. I'm assuming computational power won't be a problem, I'll focus on this when the prototype works. I need a source of visual odometry for robot_pose_ekf to work with IMU data. The question is, if I can extract this info using RGBDSLAM, and cut on doing anything unnecessary (from my POV)

