Q:

Visual Odometry options?

What are the pros/cons of the different visual odometry options? 

Stereo Camera
Optical Flow
SLAM
other?

Criteria:

how well it performs vs other odometry options/sensors (lidar, radar)
sensor fidelity
computation
accuracy
precision
drift
native resilience and repeadability in sensor noise or vehicle speed
ease of integrating with IMU/GPS
etc

In general, of course, because there are a lot of different ways the trade-offs go when we get into specifics about applications and hardware.  I'm asking out of curiosity, not for designing anything in particular.

A:

In general, visual odometry is a method that performs odometric measurements using visual means. This rules out the SLAM component, since visual odometry is only a relative means of navigation (see my answer on navigation). There are a number of methods to extract the relative motion of the camera. Since the transform from camera to robot is known this will also give you the relative transform of your robot.

If you only work with a single camera, you will always have a scale ambiguity, unless you have something to resolve that ambiguity in your image (e.g. object of known size) or combine it with another form of odometry like wheel odometry.
From a vision point of you, there are two ways you can follow: dense and sparse processing. Dense processing means, you use a regular sampling of your image points for your processing (could also be the full image). Sparse processing would mean, that you first extract feature points and only perform your operations on these features. 
Using optical flow for visual odometry, I would consider a dense approach. Once you got the flow field, you need to extract the motion components. This is not so trivial, as it largely depends on how your environment looks. There was an interesting way of resolving this in the presence of outliers presented in "Monocular Heading Estimation in Non-Stationary Urban Environment"
Sparse approaches will use features, which are extracted using feature detectors. Then there are two different methods on handling the features. First approach will perform tracking of the feature and in this way estimate where the scene point was moving to. Second approach performs association of features through feature descriptors. First one is usually used with fast algorithms on high frequency, while the second one is more robust to larger scene changes and hence can be run at lower frequency. The visual odometry on the MER Rovers works this way e.g. Some form of geometric constraint is used to remove outliers, usually with a form of RANSAC. There is quite a nice library called libviso2 for performing visual odometry this way.

