Q:

ROS force vector transformation

In my work, I am using force sensors that provide measurements based on their frame pose in Gazebo. Due to the fact that frames are not constructed based on DH parameters, I need to track the tip's pose in order to be able to apply force measurements to a haptic device {$H$} (i.e. not shown there). I'm struggling now with the right transformation matrix between the tip's frame and the frame of the haptic device. To create a minimal working example, take a look at the following picture.

The tip's frame {$T$} is shown at the top of the second arm. In the left side menu, there are the pose and the relative pose. The former seems to me the pose of {$T$} with respect to the base's frame. When the tip interacts with the enviroment, the force sensor provides measurements with respect to {$T$}, my question is what is the proper way to transform ${}^Tf$ to ${}^Hf$ using ROS capabilities?

A:

If you have robot state publisher that publishes transformations between links/joints. You can use tf package to fetch those transformations.
If you want to transform tip measurements into base frame, you should multiply T^-1 * tip_measurements.
To find out T you can use tf package and lookUpTransform to find out what's the transform between tip frame and the base frame.
You can check Python API for tf here and C++ API for tf here.
C++ API has helper classes TransformBroadcaster and TransformListener which can help you.

