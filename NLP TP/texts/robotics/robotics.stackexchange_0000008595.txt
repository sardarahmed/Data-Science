Q:

In order to integrate MCL and Occupancy Grid to implement Grid-based FastSLAM, do you have to record all data?

It's unclear as to how one goes about integrating Occupancy Grid mapping and Monte Carlo localization to implement SLAM.
Assuming Mapping is one process, Localization is another process, and some motion generating process called Exploration exist. Is it necessary to record all data as sequenced or with time stamps for coherence? 
There's Motion: $U_t$, Map: $M_t$, Estimated State: $X_t$, Measurement: $Z_t$
so..

each Estimated state, $X_t$, is a function of the current motion, $U_t$, current measurement, $Z_t$, and previous map, $M_{t-1}$;
each confidence weight, $w_t$, of estimated state is a function of current measurement, $Z_t$,  current estimate state, $X_t$, and previous map, $M_{t-1}$;
then each current map, $M_t$ is a function of current measurement, $Z_t$, current estimated state, $X_t$,  and previous map, $M_{t-1}$.

So the question is, is there a proper way of integrating mapping and localization processes? Is it something you record with timestamp or sequences? Are you suppose to record all data, like FullSLAM, and maintain full history. 
How can we verify they are sequenced at the same time to be referred to as current (i.e. measurement) and previous (measurement).

A:

The question is a bit old but an answer might help. I think you are getting confused by thinking of mapping, localization and exploration as separate processes in the context of grid-based FastSLAM. In the most basic form of the algorithm you have the three steps you described:
At every timestep :

Update the poses of your particles using your motion model
Assign weights using your measurement model
Use the calculated weights to resample the particle set

For a given particle, once you have estimated $x_{t}$ based on $x_{t-1}$ through sampling (step 1), you simply update the previous map by integrating the new measurements you have made at time $t$ "trusting" that they were made from pose $x_{t}$.
It's because the algorithm gives importance to assigning precise weights (step 2) and regularly resampling your particle set (step 3) that it can confidently use the $x_{t}$ estimates made through the sampling step to update the map.
Because with every timestep you are marginalizing over the previous state - like all Bayesian filtering approaches to online SLAM - you do not need to keep track of the full history of your data.
If you are still fuzzy on how these steps work together, you can take a look at the pseudocode for gmapping from this presentation:

