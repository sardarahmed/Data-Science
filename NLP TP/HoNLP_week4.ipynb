{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f5cb8f6c-94df-4e39-b208-e54edc3a9df6",
      "metadata": {
        "id": "f5cb8f6c-94df-4e39-b208-e54edc3a9df6"
      },
      "source": [
        "# Hands-On NLP\n",
        "## Class 4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aaf60f2c",
      "metadata": {
        "id": "aaf60f2c"
      },
      "source": [
        "<span style=\"color:magenta\">Group Names:</span>\n",
        "\n",
        "* Name 1\n",
        "* Name 2\n",
        "* Name 3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6a78cc4-3723-43df-84a1-7c2c3d9de7a0",
      "metadata": {
        "id": "e6a78cc4-3723-43df-84a1-7c2c3d9de7a0"
      },
      "source": [
        "### Outline:\n",
        "\n",
        "- Simple tokenization\n",
        "\n",
        "- Words and indices\n",
        "\n",
        "- Initialization\n",
        "\n",
        "- Visualization\n",
        "\n",
        "- Forward, cost, backward, training\n",
        "\n",
        "- First on a sentence, then on a larger text\n",
        "\n",
        "Main goal: understand the transformations needed to obtain vector representations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ac9f2e3-1064-4f09-b809-081e20d4cf49",
      "metadata": {
        "id": "4ac9f2e3-1064-4f09-b809-081e20d4cf49"
      },
      "outputs": [],
      "source": [
        "import doctest\n",
        "import re\n",
        "from collections import Counter\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from pprint import pprint\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.decomposition import PCA\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9690cac3",
      "metadata": {
        "id": "9690cac3"
      },
      "outputs": [],
      "source": [
        "tqdm.pandas()\n",
        "\n",
        "sns.set_style(\"darkgrid\")\n",
        "sns.set_context(\"notebook\")\n",
        "\n",
        "pd.set_option(\"display.precision\", 4)\n",
        "\n",
        "print(\"sklearn's version:\", sklearn.__version__)    # 1.4.0\n",
        "print(\"pandas's version:\", pd.__version__)          # 2.2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbb9aa5b-462a-45ba-9055-8b2eb3dd996c",
      "metadata": {
        "id": "bbb9aa5b-462a-45ba-9055-8b2eb3dd996c"
      },
      "outputs": [],
      "source": [
        "%config InlineBackend.figure_format=\"retina\"  # For high DPI display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba1229e8",
      "metadata": {
        "id": "ba1229e8"
      },
      "outputs": [],
      "source": [
        "doctest.testmod(optionflags=doctest.ELLIPSIS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "431d8b2a",
      "metadata": {
        "id": "431d8b2a"
      },
      "outputs": [],
      "source": [
        "EPS = np.finfo(float).eps\n",
        "\n",
        "TOY_CORPUS = \"\"\"What is a word embedding?\n",
        "A word vector (or embeddings) is a dense representation,\n",
        "which captures semantic relationships between words.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a81f756-5626-4332-8606-bc90e85cd3c1",
      "metadata": {
        "id": "7a81f756-5626-4332-8606-bc90e85cd3c1"
      },
      "outputs": [],
      "source": [
        "def tokenize(text, pattern=r\"\\w+\"):\n",
        "    \"\"\"Tokenize a text into a list of tokens.\n",
        "    >>> tokenize(\"Let's do hands-on Natural Language Processing!\")\n",
        "    ['let', 's', 'do', 'hands', 'on', 'natural', 'language', 'processing']\n",
        "    \"\"\"\n",
        "    return re.findall(pattern, text.lower())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44fcb450",
      "metadata": {
        "id": "44fcb450"
      },
      "outputs": [],
      "source": [
        "pprint(tokenize(TOY_CORPUS), compact=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "557265dd-0fc4-4d3d-98b3-6d14dabb5ffe",
      "metadata": {
        "id": "557265dd-0fc4-4d3d-98b3-6d14dabb5ffe"
      },
      "outputs": [],
      "source": [
        "def build_vocab(tokens, show_df=False):\n",
        "    \"\"\"\n",
        "    ðŸš§ TODO:\n",
        "    describe types of input and output of this function\n",
        "    tokens:\n",
        "    vocab:\n",
        "    voc2id:\n",
        "    end TODO\n",
        "    \"\"\"\n",
        "    df = pd.DataFrame(Counter(tokens).most_common(), columns=[\"token\", \"count\"])\n",
        "    if show_df:\n",
        "        display(df)\n",
        "\n",
        "    vocab = df.token.to_list()\n",
        "    voc2idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "\n",
        "    return vocab, voc2idx"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa1f97a8-d549-4267-b741-1ccca268ec6a",
      "metadata": {
        "id": "aa1f97a8-d549-4267-b741-1ccca268ec6a"
      },
      "source": [
        "### CBOW\n",
        "\n",
        "For the calculation of the word2vec we also have the CBOW (continuous bag of words) method as an alternative to Skipgram.\n",
        "In CBOW, we try to predict the central word from the whole context of the central word.\n",
        "Our xs, ys training data have fewer inputs, only as many as the text has words, but are richer.\n",
        "For prediction, we sum the context vectors to predict the center."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "048de2d3-734e-428a-a9de-cc29d16ea211",
      "metadata": {
        "id": "048de2d3-734e-428a-a9de-cc29d16ea211"
      },
      "outputs": [],
      "source": [
        "def generate_cbow_data(tokens, window_size):\n",
        "    \"\"\"\n",
        "    Create the pairs xs, ys for the training:\n",
        "    xs and ys have the same length (index of the central word,\n",
        "    index of an observed word of the context of the central word)\n",
        "\n",
        "    generate_cbow_data(\n",
        "       ['let', 's', 'do', 'hands', 'on', 'natural', 'language', 'processing'],\n",
        "       window_size=2,\n",
        "    )\n",
        "    ([[1, 2],\n",
        "      [0, 2, 3],\n",
        "      [0, 1, 3, 4],\n",
        "      [1, 2, 4, 5],\n",
        "      [2, 3, 5, 6],\n",
        "      [3, 4, 6, 7],\n",
        "      [4, 5, 7],\n",
        "      [5, 6]],\n",
        "     [0, 1, 2, 3, 4, 5, 6, 7])\n",
        "    \"\"\"\n",
        "    tokens_n = len(tokens)\n",
        "    _, voc2idx = build_vocab(tokens)\n",
        "    xs, ys = [], []\n",
        "\n",
        "    for i in range(tokens_n):\n",
        "        # ðŸš§ TODO : complete the code to fill xs and ys. xs is a list of lists.\n",
        "        ...\n",
        "        # end TODO\n",
        "\n",
        "    return xs, ys"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "636f13d0",
      "metadata": {
        "id": "636f13d0"
      },
      "source": [
        "### Skipgram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c77784a1-2d9a-415e-9e3d-ade3ac33e142",
      "metadata": {
        "id": "c77784a1-2d9a-415e-9e3d-ade3ac33e142"
      },
      "outputs": [],
      "source": [
        "def generate_skip_data(tokens, window_size):\n",
        "    \"\"\"\n",
        "    create the pairs xs, ys for the training:\n",
        "    xs and ys have the same length (index of the central word,\n",
        "    index of an observed word of the context of the central word)\n",
        "\n",
        "    >>> tokens = ['let', 's', 'do', 'hands', 'on', 'natural', 'language', 'processing']\n",
        "    >>> xs, ys = generate_skip_data(tokens, window_size=2)\n",
        "    >>> print(len(xs[0]), len(ys[0]))\n",
        "    26 26\n",
        "    >>> print(xs)\n",
        "    [[0, 0, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 7, 7]]\n",
        "    >>> print(ys)\n",
        "    [[1, 2, 0, 2, 3, 0, 1, 3, 4, 1, 2, 4, 5, 2, 3, 5, 6, 3, 4, 6, 7, 4, 5, 7, 5, 6]]\n",
        "    \"\"\"\n",
        "    tokens_n = len(tokens)\n",
        "    _, voc2idx = build_vocab(tokens)\n",
        "    xs, ys = [], []\n",
        "\n",
        "    for i in range(tokens_n):\n",
        "        # ðŸš§ TODO: Use nested loops that fills xs and ys with the right ids\n",
        "        ...\n",
        "        # end TODO\n",
        "\n",
        "    return [xs], [ys]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13394528-77e1-46cc-b513-b89597d3c1e4",
      "metadata": {
        "id": "13394528-77e1-46cc-b513-b89597d3c1e4"
      },
      "outputs": [],
      "source": [
        "# let's test these functions:\n",
        "tokens = tokenize(TOY_CORPUS)\n",
        "pprint(tokens, compact=True)\n",
        "\n",
        "vocab, voc2idx = build_vocab(tokens, show_df=True)\n",
        "\n",
        "xs, ys = generate_cbow_data(tokens, 3)\n",
        "print(len(xs), len(ys))\n",
        "xs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddf08707",
      "metadata": {
        "id": "ddf08707"
      },
      "outputs": [],
      "source": [
        "ys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc354235",
      "metadata": {
        "id": "bc354235"
      },
      "outputs": [],
      "source": [
        "xs, ys = generate_skip_data(tokens, 3)\n",
        "xs, ys = np.array(xs), np.array(ys)\n",
        "print(xs.shape, ys.shape)\n",
        "print(xs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b3a2179",
      "metadata": {
        "id": "0b3a2179"
      },
      "outputs": [],
      "source": [
        "print(ys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0aa3e14c",
      "metadata": {
        "id": "0aa3e14c"
      },
      "outputs": [],
      "source": [
        "vocab, voc2idx = build_vocab(tokens)\n",
        "vocab_size = len(vocab)\n",
        "ys = np.array(ys)\n",
        "m = np.array(ys).shape[1]\n",
        "\n",
        "# ys in one-hot encoding:\n",
        "one_hot_ys = np.zeros((vocab_size, m))\n",
        "one_hot_ys[ys.flatten(), np.arange(m)] = 1  # important to understand for later!\n",
        "one_hot_ys.shape  # can you explain the shape?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afe1a508-d203-4ce0-9aea-fdc012815d52",
      "metadata": {
        "id": "afe1a508-d203-4ce0-9aea-fdc012815d52"
      },
      "source": [
        "### ðŸš§ TODO:\n",
        "\n",
        "*   Explain why there are three 3's, four 1's... in `xs`\n",
        "\n",
        "*   Explain why there are more 0's than, e.g., 3's in `ys`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ae2a47a-5ba4-41f8-a638-580f0a81d2d9",
      "metadata": {
        "id": "0ae2a47a-5ba4-41f8-a638-580f0a81d2d9"
      },
      "outputs": [],
      "source": [
        "def initialize_embeddings(vocab_size, emb_size, seed=None):\n",
        "    \"\"\"\n",
        "    vocab_size (int): vocabulary size of your corpus or training data\n",
        "    emb_size (int): word embedding size (dimensions to represent each word)\n",
        "    returns a matrix of size (vocab_size, emb_size)\n",
        "    ðŸš§ TODO: complete the \"_____\"\n",
        "    returns a matrix of shape: _____\n",
        "    each row corresponds to _____\n",
        "    end TODO\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    return rng.normal(loc=0, scale=0.01, size=(vocab_size, emb_size))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fd808d9",
      "metadata": {
        "id": "1fd808d9"
      },
      "source": [
        "Check out the parameters in the doc: [np.random.Generator.normal](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.normal.html#numpy.random.Generator.normal)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "264e2c4d-d1c2-4362-8f07-e88064f70ab5",
      "metadata": {
        "id": "264e2c4d-d1c2-4362-8f07-e88064f70ab5"
      },
      "outputs": [],
      "source": [
        "def initialize_dense(input_size, output_size, seed=None):\n",
        "    \"\"\"\n",
        "    input_size (int): size of the input to the dense layer\n",
        "    output_size (int): size of the output of the dense layer\n",
        "    ðŸš§ TODO: complete the \"______\"\n",
        "    returns a matrix of shape : _____\n",
        "    each row corresponds to _____\n",
        "    end TODO\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    return rng.normal(loc=0, scale=0.01, size=(output_size, input_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3dcc8b72",
      "metadata": {
        "id": "3dcc8b72"
      },
      "outputs": [],
      "source": [
        "def initialize_parameters(vocab_size, emb_size, seed=None):\n",
        "    \"\"\"\n",
        "    Initialize all the training parameters\n",
        "    \"\"\"\n",
        "    embs = initialize_embeddings(vocab_size, emb_size, seed)\n",
        "    ws = initialize_dense(emb_size, vocab_size, seed)\n",
        "\n",
        "    parameters = {}\n",
        "    parameters[\"EMBS\"] = embs\n",
        "    parameters[\"W\"] = ws\n",
        "\n",
        "    return parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e192032-39a5-4c4b-aeda-238363017876",
      "metadata": {
        "id": "2e192032-39a5-4c4b-aeda-238363017876"
      },
      "outputs": [],
      "source": [
        "# for example an embedding of dimension 5\n",
        "parameters = initialize_parameters(len(vocab), emb_size=5, seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9f9b419-7b36-4f99-a81f-f8db9bfb332b",
      "metadata": {
        "id": "f9f9b419-7b36-4f99-a81f-f8db9bfb332b"
      },
      "outputs": [],
      "source": [
        "print(parameters[\"EMBS\"].shape, parameters[\"W\"].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aabc759f-cbc3-41ca-8eca-1051bfb04bb4",
      "metadata": {
        "id": "aabc759f-cbc3-41ca-8eca-1051bfb04bb4"
      },
      "outputs": [],
      "source": [
        "# We can multiply them:\n",
        "(parameters[\"EMBS\"].T @ parameters[\"W\"]).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f030b86b-7385-4a11-ba13-830af20ef48d",
      "metadata": {
        "id": "f030b86b-7385-4a11-ba13-830af20ef48d"
      },
      "outputs": [],
      "source": [
        "def plot_embedding(vs, indices, vocab):\n",
        "    pca = PCA(n_components=2)\n",
        "    data = pca.fit_transform(vs).transpose()\n",
        "    xs, ys = data[0], data[1]\n",
        "    fig, ax = plt.subplots(figsize=(15, 8))\n",
        "    ax.scatter(xs, ys, c=\"green\")\n",
        "    for i, idx in enumerate(indices):\n",
        "        ax.annotate(\n",
        "            vocab[idx], (xs[i], ys[i]), xytext=(5, 5), textcoords=\"offset points\"\n",
        "        )\n",
        "    plt.xlabel(\"x\")\n",
        "    plt.ylabel(\"y\")\n",
        "    plt.title(\"2 dimensions of the word embeddings\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bf916df",
      "metadata": {
        "id": "8bf916df"
      },
      "outputs": [],
      "source": [
        "# plotting all the words that have just been initialized:\n",
        "# print(parameters[\"emb\"].shape)\n",
        "plot_embedding(parameters[\"EMBS\"], voc2idx.values(), vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca0f6236-735c-4426-86b5-e30e69e99acc",
      "metadata": {
        "id": "ca0f6236-735c-4426-86b5-e30e69e99acc"
      },
      "outputs": [],
      "source": [
        "def ind_to_word_embs(inds, parameters):\n",
        "    \"\"\"\n",
        "    inds: numpy array. shape: (1, m)\n",
        "    parameters: dict. weights to be trained\n",
        "\n",
        "    ðŸš§ TODO: complete:\n",
        "    returns a matrix of word vectors as _____ columns with shape _____\n",
        "    end TODO\n",
        "\n",
        "    \"\"\"\n",
        "    m = inds.shape[1]\n",
        "    embs = parameters[\"EMBS\"]\n",
        "\n",
        "    # Select the rows of the embedding matrix corresponding to the indices\n",
        "    #     flatten() -> 1D array (as many elements as indices)\n",
        "    w_embs = embs[inds.flatten(), :].T\n",
        "\n",
        "    assert w_embs.shape == (embs.shape[1], m)\n",
        "\n",
        "    return w_embs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a60908d",
      "metadata": {
        "id": "9a60908d"
      },
      "outputs": [],
      "source": [
        "def linear_dense(w_embs, parameters):\n",
        "    \"\"\"\n",
        "    w_embs: numpy array. shape: (emb_size, m)\n",
        "    parameters: dict. weights to be trained\n",
        "    returns: ws et zs\n",
        "    ws: matrix of weights of the dense layer\n",
        "    zs: output matrix of the dense layer\n",
        "        ðŸš§ TODO: complete :\n",
        "        zs is of shape: _____\n",
        "\n",
        "    \"\"\"\n",
        "    m = w_embs.shape[1]\n",
        "    ws = parameters[\"W\"]\n",
        "    zs = np.dot(ws, w_embs)\n",
        "\n",
        "    assert zs.shape == (ws.shape[0], m)\n",
        "\n",
        "    return ws, zs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e1e1361",
      "metadata": {
        "id": "2e1e1361"
      },
      "outputs": [],
      "source": [
        "def softmax(zs):\n",
        "    \"\"\"\n",
        "    zs: output out of the dense layer.\n",
        "        shape: (vocab_size, m = number of input vectors (tokens to update))\n",
        "    \"\"\"\n",
        "    softmax_out = np.divide(\n",
        "        np.exp(zs), np.sum(np.exp(zs), axis=0, keepdims=True) + EPS\n",
        "    )\n",
        "    assert softmax_out.shape == zs.shape\n",
        "\n",
        "    return softmax_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d203e8c8",
      "metadata": {
        "id": "d203e8c8"
      },
      "outputs": [],
      "source": [
        "def forward_propagation(inds, parameters):\n",
        "    w_embs = ind_to_word_embs(inds, parameters)\n",
        "    ws, zs = linear_dense(w_embs, parameters)\n",
        "    softmax_out = softmax(zs)\n",
        "\n",
        "    caches = {}\n",
        "    caches[\"inds\"] = inds\n",
        "    caches[\"w_embs\"] = w_embs\n",
        "    caches[\"W\"] = ws\n",
        "    caches[\"Z\"] = zs\n",
        "    return softmax_out, caches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4336e80d-65b6-48fc-881e-b46cb191e979",
      "metadata": {
        "id": "4336e80d-65b6-48fc-881e-b46cb191e979"
      },
      "outputs": [],
      "source": [
        "def cross_entropy(softmax_out, ys):\n",
        "    \"\"\"\n",
        "    softmax_out: output out of softmax. shape: (vocab_size, m=batch_size)\n",
        "    ys: ground truth: indices Ã  prÃ©dire. shape: (1, m)\n",
        "    \"\"\"\n",
        "    m = softmax_out.shape[1]\n",
        "\n",
        "    cost = -(1 / m) * np.sum(\n",
        "        np.log(softmax_out[ys.flatten(), np.arange(ys.shape[1])] + EPS)\n",
        "    )\n",
        "    return cost"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89bd6da0-3c7a-4daf-8c12-448f27618db1",
      "metadata": {
        "id": "89bd6da0-3c7a-4daf-8c12-448f27618db1"
      },
      "source": [
        "ðŸš§ TODO, read this carrefully:\n",
        "\n",
        "* From the softmax out, I select the lines corresponding to the predicted tokens\n",
        "\n",
        "* It gives me the probabilities, I take the log, and I average it\n",
        "\n",
        "* If the prediction was perfect, the probability would be 1, the log is 0 --> zero cost\n",
        "\n",
        "* If not, I have a proba smaller than 1, I take the average of these probas logs (which are all negative)\n",
        "\n",
        "* With the \"$-$\" it becomes positive, so it is a measure of the error of the prediction\n",
        "\n",
        "* Note that `ys` can contain several times the same index to predict. It is counted as many times as it should be predicted."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aed7f9c1-1380-444a-a714-9dfd01bcd5ec",
      "metadata": {
        "id": "aed7f9c1-1380-444a-a714-9dfd01bcd5ec"
      },
      "source": [
        "$$\\frac{dL}{dZ} = softmax\\_out - ys$$\n",
        "\n",
        "$$\\frac{dL}{dW} = \\frac{dL}{dZ}\\times\\frac{dZ}{dW}= \\frac{dL}{dZ}\\times w\\_embs.T$$\n",
        "\n",
        "$$\n",
        "\\frac{dL}{d w\\_embs} = \\frac{dL}{dZ}\\times\\frac{dZ}{d w\\_embs}\n",
        "    = \\frac{dL}{dZ} \\times W.T\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35f02b96-ceae-40d4-b897-c734687841fc",
      "metadata": {
        "id": "35f02b96-ceae-40d4-b897-c734687841fc"
      },
      "outputs": [],
      "source": [
        "def softmax_backward(ys, softmax_out):\n",
        "    \"\"\"\n",
        "    ys: labels of training data. shape: (1, m=batch_size)\n",
        "    softmax_out: output out of softmax. shape: (vocab_size, m=batch_size)\n",
        "    \"\"\"\n",
        "    m = ys.shape[1]\n",
        "\n",
        "    # we compute the difference between the prediction and the ground truth (ys)\n",
        "    # if the prediction was perfect, the dl_dz becomes 0\n",
        "    # we only touch the lines to predict\n",
        "    # if the token is not to be predicted, we do not touch the line\n",
        "    softmax_out[ys.flatten(), np.arange(m)] -= 1.0\n",
        "    dl_dz = softmax_out\n",
        "\n",
        "    assert dl_dz.shape == softmax_out.shape\n",
        "    return dl_dz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a342bebb",
      "metadata": {
        "id": "a342bebb"
      },
      "outputs": [],
      "source": [
        "def dense_backward(dl_dz, caches):\n",
        "    \"\"\"\n",
        "    dl_dz: shape: (vocab_size, m)\n",
        "    caches: dict. results from each steps of forward propagation\n",
        "    dl_dz has negative values where something needs to be changed.\n",
        "    the more negative they are, the worse it is.\n",
        "    \"\"\"\n",
        "    ws = caches[\"W\"]\n",
        "    w_embs = caches[\"w_embs\"]\n",
        "    m = w_embs.shape[1]\n",
        "\n",
        "    # we multiply where we need to change something:\n",
        "    # dl_dz.shape = (vocab_size, m)\n",
        "    dl_dw = (1 / m) * np.dot(dl_dz, w_embs.T)\n",
        "    dl_dw_embs = np.dot(ws.T, dl_dz)\n",
        "\n",
        "    assert ws.shape == dl_dw.shape\n",
        "    assert w_embs.shape == dl_dw_embs.shape\n",
        "\n",
        "    return dl_dw, dl_dw_embs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1237d075",
      "metadata": {
        "id": "1237d075"
      },
      "outputs": [],
      "source": [
        "def backward_propagation(ys, softmax_out, caches):\n",
        "    dl_dz = softmax_backward(ys, softmax_out)\n",
        "    dl_dw, dl_dw_embs = dense_backward(dl_dz, caches)\n",
        "\n",
        "    gradients = dict()\n",
        "    gradients[\"dL_dZ\"] = dl_dz\n",
        "    gradients[\"dL_dW\"] = dl_dw\n",
        "    gradients[\"dL_dw_embs\"] = dl_dw_embs\n",
        "\n",
        "    return gradients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30b552ce",
      "metadata": {
        "id": "30b552ce"
      },
      "outputs": [],
      "source": [
        "def update_parameters(parameters, caches, gradients, learning_rate):\n",
        "    \"\"\"\n",
        "    here we update the embedding\n",
        "    \"\"\"\n",
        "    vocab_size, emb_size = parameters[\"EMBS\"].shape\n",
        "    inds = caches[\"inds\"]\n",
        "    embs = parameters[\"EMBS\"]\n",
        "    dl_dw_embs = gradients[\"dL_dw_embs\"]\n",
        "    m = inds.shape[-1]\n",
        "\n",
        "    # note that only the lines corresponding to the central words are modified\n",
        "    embs[inds.flatten(), :] -= dl_dw_embs.T * learning_rate\n",
        "\n",
        "    parameters[\"W\"] -= learning_rate * gradients[\"dL_dW\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d89cbd45-3139-488a-ab9e-cce968c8fccb",
      "metadata": {
        "id": "d89cbd45-3139-488a-ab9e-cce968c8fccb"
      },
      "outputs": [],
      "source": [
        "def skipgram_model_training(\n",
        "    xs,\n",
        "    ys,\n",
        "    vocab_size,\n",
        "    emb_size,\n",
        "    learning_rate,\n",
        "    epochs,\n",
        "    batch_size=256,\n",
        "    parameters=None,\n",
        "    print_cost=False,\n",
        "    plot_cost=True,\n",
        "):\n",
        "    costs = []\n",
        "    m = xs.shape[1]\n",
        "\n",
        "    if parameters is None:\n",
        "        parameters = initialize_parameters(vocab_size, emb_size)\n",
        "\n",
        "    # for the moment these three variables are not used\n",
        "    # ðŸš§ TODO: use these variables to keep the best model at the end\n",
        "    best_epoch = 0\n",
        "    min_epoch_cost = float(\"inf\")\n",
        "    parameters[\"best_embeddings\"] = parameters[\"EMBS\"]\n",
        "\n",
        "    begin_time = datetime.now()\n",
        "    for epoch in range(epochs):\n",
        "        epoch_cost = 0\n",
        "        batch_inds = list(range(0, m, batch_size))\n",
        "        np.random.shuffle(batch_inds)\n",
        "        for i in batch_inds:\n",
        "            batch_xs = xs[:, i : i + batch_size]\n",
        "            batch_ys = ys[:, i : i + batch_size]\n",
        "\n",
        "            softmax_out, caches = forward_propagation(batch_xs, parameters)\n",
        "            cost = cross_entropy(softmax_out, batch_ys)\n",
        "            gradients = backward_propagation(batch_ys, softmax_out, caches)\n",
        "            update_parameters(parameters, caches, gradients, learning_rate)\n",
        "            epoch_cost += cost\n",
        "\n",
        "        costs.append(epoch_cost)\n",
        "        # ðŸš§ TODO: add here the code to keep the best_embeddings\n",
        "        if epoch_cost < min_epoch_cost:\n",
        "            ...\n",
        "        # end TODO\n",
        "\n",
        "        if print_cost and epoch % 200 == 0:\n",
        "            print(f\"Cost after epoch {epoch:4d}: {epoch_cost:.4f}\")\n",
        "        if epoch % (epochs // 100) == 0:\n",
        "            learning_rate *= 0.98\n",
        "    end_time = datetime.now()\n",
        "    print(f\"training time: {end_time - begin_time}\")\n",
        "    print(\n",
        "        f\"ðŸš§ TODO I've kept the embedding of epoch {best_epoch} \"\n",
        "        f\"with cost {min_epoch_cost:.4f}.\"\n",
        "    )\n",
        "    if plot_cost:\n",
        "        plt.plot(np.arange(epochs), costs)\n",
        "        plt.xlabel(\"# of epochs\")\n",
        "        plt.ylabel(\"cost\")\n",
        "    return parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7009ddd-30ad-4c52-a463-1d0a3cdb9d8e",
      "metadata": {
        "id": "b7009ddd-30ad-4c52-a463-1d0a3cdb9d8e"
      },
      "outputs": [],
      "source": [
        "print(ys.shape, xs.shape, one_hot_ys.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "169e27b0-7c86-4178-a6bc-755d32232df7",
      "metadata": {
        "id": "169e27b0-7c86-4178-a6bc-755d32232df7"
      },
      "outputs": [],
      "source": [
        "parameters = skipgram_model_training(\n",
        "    xs,\n",
        "    ys,\n",
        "    vocab_size,\n",
        "    emb_size=50,\n",
        "    learning_rate=0.05,\n",
        "    epochs=3000,\n",
        "    batch_size=128,\n",
        "    parameters=None,\n",
        "    print_cost=True,\n",
        ")\n",
        "# test with a small batch_size to see...."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "400b4d62-2e63-46f1-a225-b3c58a529563",
      "metadata": {
        "id": "400b4d62-2e63-46f1-a225-b3c58a529563"
      },
      "source": [
        "####  ðŸš§ TODO:\n",
        "\n",
        "- Keep the best model\n",
        "\n",
        "- Visualize also the best model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f44b4399-67be-429b-9a01-43e37de66d39",
      "metadata": {
        "id": "f44b4399-67be-429b-9a01-43e37de66d39"
      },
      "source": [
        "### First intrinsic evaluation\n",
        "\n",
        "- Let's take the indices of all the words\n",
        "\n",
        "- Send them into the grinder\n",
        "\n",
        "- See if the system can predict the words that were close as the most probable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0656d03-addb-4d26-ba09-70a917c3c31a",
      "metadata": {
        "id": "c0656d03-addb-4d26-ba09-70a917c3c31a"
      },
      "outputs": [],
      "source": [
        "test_xs = np.arange(vocab_size)\n",
        "print(test_xs, test_xs.shape)\n",
        "test_xs = np.expand_dims(test_xs, axis=0)\n",
        "print(test_xs, test_xs.shape)\n",
        "\n",
        "# ðŸš§ TODO: complete\n",
        "\n",
        "# send test_xs to the forward_propagation\n",
        "softmax_test, _ = ...\n",
        "\n",
        "# find the 5 most likely indices:\n",
        "top_sorted_inds = np.argsort(...\n",
        "\n",
        "# end TODO\n",
        "\n",
        "top_sorted_inds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80f836bf-4ae6-4865-98cc-32c1b8dd4273",
      "metadata": {
        "id": "80f836bf-4ae6-4865-98cc-32c1b8dd4273"
      },
      "outputs": [],
      "source": [
        "for input_ind in range(vocab_size):\n",
        "    input_word = vocab[input_ind]\n",
        "    output_words = [\n",
        "        vocab[output_ind] for output_ind in top_sorted_inds[::-1, input_ind]\n",
        "    ]  # explain ::-1 in few words\n",
        "    print(f\"{input_word}'s neighbor words: {output_words}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b28aff80-0776-45ce-98db-5a2668506f09",
      "metadata": {
        "id": "b28aff80-0776-45ce-98db-5a2668506f09"
      },
      "outputs": [],
      "source": [
        "plot_embedding(parameters[\"best_embeddings\"], test_xs[0], vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44403a99-64dd-4dfb-afe5-3d07fb79d334",
      "metadata": {
        "id": "44403a99-64dd-4dfb-afe5-3d07fb79d334"
      },
      "outputs": [],
      "source": [
        "parameters[\"best_embeddings\"].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b28952f0-921d-439d-9918-5b5523f5e1b2",
      "metadata": {
        "id": "b28952f0-921d-439d-9918-5b5523f5e1b2"
      },
      "outputs": [],
      "source": [
        "# Let's calculate the cosine distance between all words\n",
        "\n",
        "cos_dists = scipy.spatial.distance.cdist(\n",
        "    parameters[\"best_embeddings\"], parameters[\"best_embeddings\"], \"cosine\"\n",
        ")\n",
        "cos_dists.shape\n",
        "vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97eee6bc",
      "metadata": {
        "id": "97eee6bc"
      },
      "outputs": [],
      "source": [
        "def costs_heatmap(costs, vocab):\n",
        "    plt.figure(figsize=(9, 8))\n",
        "    sns.heatmap(\n",
        "        costs,\n",
        "        annot=costs.round(1),\n",
        "        xticklabels=vocab,\n",
        "        yticklabels=vocab,\n",
        "        cmap=\"RdPu\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "523efe65",
      "metadata": {
        "id": "523efe65"
      },
      "outputs": [],
      "source": [
        "costs_heatmap(cos_dists, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee775ab3-7336-4c23-93f0-4c80dc81b85e",
      "metadata": {
        "id": "ee775ab3-7336-4c23-93f0-4c80dc81b85e"
      },
      "outputs": [],
      "source": [
        "### ðŸš§ attempt a short hande-waving explanations of what you see in the heatmap"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8301ce1d-e610-45fc-806d-640ff3209755",
      "metadata": {
        "id": "8301ce1d-e610-45fc-806d-640ff3209755"
      },
      "source": [
        "## A longer text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "662980b9-f4a7-40ff-b8e5-3a53d3196d02",
      "metadata": {
        "id": "662980b9-f4a7-40ff-b8e5-3a53d3196d02"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"The celestial sphere is an imaginary projection of the Sun, Moon,\n",
        "planets, stars, and all astronomical bodies upon an imaginary sphere surrounding\n",
        "Earth. Although originally developed as part of the ancient Greek concept of an\n",
        "Earth-centered (geocentric) universe, the hypothetical celestial sphere gives\n",
        "astronomers an important tool for fixing the location and plotting movements of\n",
        "celestial objects. Ancient Greek astronomers envisioned concentric crystalline\n",
        "spheres centered around Earth, upon which the Sun, Moon, planets, and stars\n",
        "moved. Although heliocentric (Sun-centered) models of the universe were also\n",
        "proposed by the Greeks, they were disregarded as counterintuitive to the\n",
        "apparent motions of celestial bodies across the sky. Corresponding to Earthâ€™s\n",
        "rotation, the celestial sphere rotates through 1Â° in about four minutes. Because\n",
        "of this, sunrise, sunset, moonrise, and moon-set, all take approximately two\n",
        "minutes because both the Sun and Moon have the same apparent size on the\n",
        "celestial sphere (about 0.5Â°). The Sun is, of course, much larger, but the Moon\n",
        "is much closer. \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fa75470-be8e-4bf0-b56d-89583bd6ea02",
      "metadata": {
        "id": "6fa75470-be8e-4bf0-b56d-89583bd6ea02"
      },
      "source": [
        "### Do the same thing again\n",
        "\n",
        "for this larger text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8c1538a-ac3a-4b30-a136-27405d8f9046",
      "metadata": {
        "id": "c8c1538a-ac3a-4b30-a136-27405d8f9046"
      },
      "outputs": [],
      "source": [
        "tokens = tokenize(text)\n",
        "print(Counter(tokens))\n",
        "\n",
        "vocab, voc2idx = build_vocab(tokens, show_df=True)\n",
        "xs, ys = generate_skip_data(tokens, 3)\n",
        "\n",
        "xs, ys = np.array(xs), np.array(ys)\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "m = ys.shape[1]\n",
        "print(\"m:\", m)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddca05e3-6dab-4471-a976-d7b4e4e20a3e",
      "metadata": {
        "id": "ddca05e3-6dab-4471-a976-d7b4e4e20a3e"
      },
      "outputs": [],
      "source": [
        "# Initialize the parameters and visualize the distribution (normal, random)\n",
        "parameters = initialize_parameters(len(vocab), 5)\n",
        "plot_embedding(parameters[\"EMBS\"], voc2idx.values(), vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14989845-fa66-4deb-832d-0537c6856d79",
      "metadata": {
        "id": "14989845-fa66-4deb-832d-0537c6856d79"
      },
      "outputs": [],
      "source": [
        "# Be patient:\n",
        "parameters = skipgram_model_training(\n",
        "    xs,\n",
        "    ys,\n",
        "    vocab_size,\n",
        "    emb_size=50,\n",
        "    learning_rate=0.05,\n",
        "    epochs=5000,\n",
        "    batch_size=128,\n",
        "    parameters=None,\n",
        "    print_cost=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c564768e-019b-4bc1-9d02-64066d3ccb85",
      "metadata": {
        "id": "c564768e-019b-4bc1-9d02-64066d3ccb85"
      },
      "outputs": [],
      "source": [
        "# ðŸš§ TODO: as before find the predicted words from the context\n",
        "test_xs = np.arange(vocab_size)\n",
        "test_xs = np.expand_dims(test_xs, axis=0)\n",
        "softmax_test, _ = forward_propagation(...\n",
        "top_sorted_inds = np.argsort(...\n",
        "for input_ind in range(10):\n",
        "    ...\n",
        "    print(f\"{input_word}'s neighbor words: {output_words}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aeaf0bbd-5cd0-45fe-87fd-38357ac44414",
      "metadata": {
        "id": "aeaf0bbd-5cd0-45fe-87fd-38357ac44414"
      },
      "outputs": [],
      "source": [
        "plot_embedding(parameters[\"EMBS\"], voc2idx.values(), vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7b59b98-9925-43a7-a470-4529bdee016b",
      "metadata": {
        "id": "f7b59b98-9925-43a7-a470-4529bdee016b"
      },
      "outputs": [],
      "source": [
        "# very (!) slow and not very telling as it is.\n",
        "# optional TODO : who can reduce the heatmap to a few frequent words?\n",
        "\n",
        "cos_dists = scipy.spatial.distance.cdist(\n",
        "    parameters[\"best_embeddings\"], parameters[\"best_embeddings\"], \"cosine\"\n",
        ")\n",
        "costs_heatmap(cos_dists, vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "845552de-5d9f-43b2-88db-573286a818c3",
      "metadata": {
        "id": "845552de-5d9f-43b2-88db-573286a818c3"
      },
      "source": [
        "### Conclusions:\n",
        "\n",
        "- It works, but we would like to train on a much larger corpus\n",
        "\n",
        "  - For that, we should:\n",
        "  \n",
        "    - Not exceed the lines when computing the neighborhood\n",
        "  \n",
        "    - Use negative sampling\n",
        "  \n",
        "    - Maybe implement a part in C and optimized for graphic card --\n",
        "      or use a deep learning module (PyTorch, TensorFlow...)\n",
        "\n",
        "- We will rely on an already implemented module: **Gensim**\n",
        "\n",
        "â†’ Next section"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4db9a7f4-f8d9-4f71-aa14-1cd6570137f1",
      "metadata": {
        "id": "4db9a7f4-f8d9-4f71-aa14-1cd6570137f1"
      },
      "source": [
        "____________"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0b5d7a6-7a61-403a-818f-01f2f5936198",
      "metadata": {
        "id": "b0b5d7a6-7a61-403a-818f-01f2f5936198"
      },
      "source": [
        "### ðŸŒ¸ Gensim Word Embedding\n",
        "\n",
        "- Word embedding\n",
        "\n",
        "  - ðŸš§ TODO: complete a function to read a pre-computed embedding\n",
        "\n",
        "  - ðŸš§ TODO: normalize vectors\n",
        "\n",
        "  - ðŸš§ TODO: complete the function that finds the closest words by cosine distance\n",
        "\n",
        "  - ðŸš§ TODO: test the nearest words\n",
        "\n",
        "  - ðŸš§ mini-TODO: complete and test the function that computes analogies to see if our plotting is biased\n",
        "\n",
        "- Later: text classification\n",
        "\n",
        "  - ðŸš§ TODO: find and remove an outlier in our texts (to understand that the method works very well if the texts are very different)\n",
        "\n",
        "  - Optional TODO: other methods than k-nearest neighbors\n",
        "\n",
        "  - Optional TODO: redo based on keywords only\n",
        "\n",
        "All TODOs are marked with the symbol ðŸš§"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c855289e-30f0-4816-9d4c-b43746cf9b51",
      "metadata": {
        "id": "c855289e-30f0-4816-9d4c-b43746cf9b51"
      },
      "source": [
        "#### Load the pre-calculated word2vec\n",
        "  \n",
        "  - Get `enwiki-50k_100d.txt` from [here](https://gitlab.inria.fr/flandes/data-for-teaching/-/blob/master/enwiki-50k_100d.txt) and put it next to your notebook\n",
        "\n",
        "  - It consists in a reduction to the 50k most frequent types of the full word2vec [pretrainded](https://wikipedia2vec.github.io/wikipedia2vec/pretrained/) on the complete English Wikipedia.  \n",
        "  \n",
        "    In particular this [file](http://wikipedia2vec.s3.amazonaws.com/models/en/2018-04-20/enwiki_20180420_100d.txt.bz2)\n",
        "  \n",
        "  - Open the file and explore the format\n",
        "  \n",
        "  - ðŸš§ Write a `read_vectors` function that gives a numpy matrix `embeddings` with one row per type (~ one word), each row represents the word. `embeddings[i, :]` corresponds to one type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad0ecbf5-98ee-44f5-926b-e267a44679e2",
      "metadata": {
        "id": "ad0ecbf5-98ee-44f5-926b-e267a44679e2"
      },
      "outputs": [],
      "source": [
        "def read_vectors(vocab_size=50000, dimension=100):\n",
        "    \"\"\"\n",
        "    function that reads a backup of an embedding\n",
        "    returns\n",
        "    - a word dictionary type -> index\n",
        "    - the inverse dictionary\n",
        "    - a matrix containing a vector of words per word type\n",
        "    check the file to understand the format\n",
        "    \"\"\"\n",
        "    embeddings = np.zeros((vocab_size, dimension))\n",
        "    w2i, i2w = {}, {}  # as before\n",
        "    i = 0\n",
        "    with open(\"enwiki-50k_100d.txt\") as f:\n",
        "      for line in f.readlines():\n",
        "        # ðŸš§ TODO:\n",
        "        # ...\n",
        "        splitted = line.strip().split()\n",
        "        w2i[...\n",
        "        i2w[...\n",
        "        embeddings[i] = np.array(...\n",
        "        i += 1\n",
        "        # end TODO\n",
        "    assert len(w2i) == len(i2w) == len(embeddings)\n",
        "    return w2i, i2w, embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea0f5ece",
      "metadata": {
        "id": "ea0f5ece"
      },
      "outputs": [],
      "source": [
        "w2i, i2w, embeddings = read_vectors()\n",
        "\n",
        "print(len(w2i), len(i2w), embeddings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af15536a-1b63-4bd8-8010-9259954ce0cb",
      "metadata": {
        "id": "af15536a-1b63-4bd8-8010-9259954ce0cb"
      },
      "source": [
        "### Objectives\n",
        "\n",
        "We want to make a python function that for a given word returns the K nearest (semantic) neighbors\n",
        "\n",
        "To do this, we need to\n",
        "\n",
        "1. calculate the cosine distance between them\n",
        "\n",
        "2. order the distances by decreasing orders\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a16763d-7135-4d34-ad4a-92a153798f70",
      "metadata": {
        "id": "6a16763d-7135-4d34-ad4a-92a153798f70"
      },
      "source": [
        "#### 1. Calculating the cosine distance\n",
        "**Hint**, we can do this in two substeps:\n",
        "- ðŸš§ normalize the embeddings (with respect to the rows, i.e., each word vector to a unit norm)\n",
        "- ðŸš§ calculate scalar product between the source word embedding with the whole embedding space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eaada638-a507-4706-9517-de36bd0f4c8c",
      "metadata": {
        "id": "eaada638-a507-4706-9517-de36bd0f4c8c"
      },
      "outputs": [],
      "source": [
        "# ðŸš§ TODO: normalize the embeddings (with respect to the lines,\n",
        "#     i.e. each word vector to a unit norm)\n",
        "\n",
        "# normed_embeddings = # ðŸš§ TODO: ...??"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdbd4f9b-d677-4576-922e-c7e1af5adc00",
      "metadata": {
        "id": "cdbd4f9b-d677-4576-922e-c7e1af5adc00"
      },
      "source": [
        "**âš  Achtung!!!**:\n",
        "\n",
        "- To simplify the calculations later, we will base our calculations on the `normed_embeddings` matrix. Thus, we won't need to divide the dot product of the two vectors by their norms (the cosine similarity of two unit vectors is just the dot product of the vectors).\n",
        "\n",
        "- If you want to perform a similarity calculation based on another measure (Euclidean distance?), then you will have to base your calculations on the raw `embeddings` matrix (which is not normalized!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "116c7f97-2f20-483f-9da6-72699f7f3609",
      "metadata": {
        "id": "116c7f97-2f20-483f-9da6-72699f7f3609"
      },
      "outputs": [],
      "source": [
        "def find_closest_words_from_vector(vector, k_max=10):\n",
        "    \"\"\"For a given vector, find the K nearest semantic neighbors\"\"\"\n",
        "    closest_words = []\n",
        "\n",
        "    cosine_similarities = np.dot(...\n",
        "    # cosine_similarities = # ðŸš§ TODO: ...?? think of the scalar product\n",
        "\n",
        "    sorted_indexes = ...  # matrix sorted from smallest to largest\n",
        "    # sorted_indexes =    # ðŸš§ TODO: ...?? matrix sorted from smallest to largest\n",
        "\n",
        "    # Only K largest is of interest to us\n",
        "    for k in range(1, k_max + 1):  # to take the last elements, we start with -1\n",
        "        # ðŸš§ TODO: ...??\n",
        "        neighboor_index = ...\n",
        "        closest_words.append(...\n",
        "        # end TODO\n",
        "    return closest_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17cb6a62-9ad7-4178-8f5c-552353566901",
      "metadata": {
        "id": "17cb6a62-9ad7-4178-8f5c-552353566901"
      },
      "outputs": [],
      "source": [
        "def find_closest_words_from_word(word, k_max=10):\n",
        "    \"\"\"For a given word 'word', find the K nearest semantic neighbors\"\"\"\n",
        "    word_index = w2i[word]\n",
        "    word_vector = normed_embeddings[word_index]\n",
        "    closest_words = find_closest_words_from_vector(word_vector, k_max)\n",
        "    return closest_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c88987b-65be-4c9e-9f93-3a0923dced74",
      "metadata": {
        "id": "9c88987b-65be-4c9e-9f93-3a0923dced74"
      },
      "outputs": [],
      "source": [
        "def pretty_show(closest_words):\n",
        "    \"\"\"prettify the print for the closest words\"\"\"\n",
        "    for word, similarity in closest_words:\n",
        "        print(word.ljust(15, \" \"), similarity.round(3))\n",
        "    print(\"___\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "333cec83-2c19-447b-833a-9d761ea7d274",
      "metadata": {
        "id": "333cec83-2c19-447b-833a-9d761ea7d274"
      },
      "outputs": [],
      "source": [
        "# Creative time! Let's test our function on some terms\n",
        "pretty_show(find_closest_words_from_word(\"french\"))\n",
        "pretty_show(find_closest_words_from_word(\"what\"))\n",
        "# trained on WikipÃ©dia. --> few swear words... (e.g., holy ...)\n",
        "pretty_show(find_closest_words_from_word(\"holy\"))\n",
        "pretty_show(find_closest_words_from_word(\"cool\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77bfa1b5-b4dc-457f-8056-998eb33aea2c",
      "metadata": {
        "id": "77bfa1b5-b4dc-457f-8056-998eb33aea2c"
      },
      "source": [
        "### Analogies\n",
        "\n",
        "Finally the pretty analogies in the embeddings!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a0bfb0a-cbce-47b6-9a99-86d29df48e09",
      "metadata": {
        "id": "3a0bfb0a-cbce-47b6-9a99-86d29df48e09"
      },
      "outputs": [],
      "source": [
        "def find_analogies(wpos1, wpos2, wneg):\n",
        "    \"\"\"\n",
        "    wpos1: the first positive word\n",
        "    wpos2: the 2nd positive word\n",
        "    wneg: the negative word\n",
        "    Find the analog \"analog\" such that\n",
        "        <\"analog\" is to \"wpos2\" what \"wneg\" is to \"wpos1\">\n",
        "    For example: <\"queen\" is to \"woman\" what \"king\" is to \"man\">\n",
        "    \"\"\"\n",
        "    # add the positive vectors, substract the negative vector:\n",
        "    # vector_analog  = ... # ðŸš§ TODO\n",
        "    return find_closest_words_from_vector(vector_analog)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa3298ff-3b37-4b2d-bc91-45feb35390a4",
      "metadata": {
        "id": "aa3298ff-3b37-4b2d-bc91-45feb35390a4"
      },
      "outputs": [],
      "source": [
        "pretty_show(find_analogies(\"king\", \"woman\", \"man\"))\n",
        "pretty_show(find_analogies(\"nurse\", \"man\", \"woman\"))  # Watch out for bias!\n",
        "# ðŸš§ TODO: Find other \"questionable\" examples"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5c659c3-7f18-4584-b37e-fce2bdcd51ba",
      "metadata": {
        "id": "d5c659c3-7f18-4584-b37e-fce2bdcd51ba"
      },
      "source": [
        "# ðŸ’¥ Text classification by embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "995f9305-1685-4bd1-8036-46c7cd8b18e4",
      "metadata": {
        "id": "995f9305-1685-4bd1-8036-46c7cd8b18e4"
      },
      "outputs": [],
      "source": [
        "def doc2vec(text):\n",
        "    \"\"\"\n",
        "    The function takes a text, cuts it into tokens\n",
        "    For each token, if the token is in w2i, we take its vector\n",
        "    We add the vectors and renormalize\n",
        "    Returns: vector of the same format as the ones we have for each word\n",
        "    \"\"\"\n",
        "    tokens = re.findall(r\"\\w+\", text)\n",
        "    vectors = [normed_embeddings[w2i[t]] for t in tokens if t in w2i]\n",
        "    somme = sum(vectors)\n",
        "    normalised = somme / np.sqrt(np.sum(somme ** 2))\n",
        "    return normalised"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f023f1e",
      "metadata": {
        "id": "5f023f1e"
      },
      "outputs": [],
      "source": [
        "vex = doc2vec(\"let's see how this works, this word2vec thing!\")\n",
        "vex"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d850e54-f593-4be9-967b-fffd88423ed9",
      "metadata": {
        "id": "1d850e54-f593-4be9-967b-fffd88423ed9"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "honlp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "4092beadd17a8c123802d6bfeec2845d174d6d0bf5a666f517bdff4bcd2558e4"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}