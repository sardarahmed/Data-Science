Q:

Did Newton and Leibniz lack rigour?

I've read that the original approach to demonstrating the efficacy of calculus left something to be desired among mathematicians. What exactly was the problem? Does the later accepted definition illuminate anything about the original problem or avoid it?

A:

Problems were abundant. There was no rigorous definitions of limits, convergence, and even functions and real numbers. And without definitions there could be no real, rigorous proofs. All this was not achieved until 19 century. Newton's and Leibniz results were correct, but they were not proved to the standards of rigor which already existed in mathematics since the times of Euclid and Archimedes, not speaking of the later standards. 
And mathematicians of 17 and 18 century understood this. In many arguments they had to rely on intuition, as modern physicists frequently do, when they use mathematical tools which are not fully justified. 
As calculus developed further, this lack of rigor led to paradoxes and controversies. Only in 19s century, starting with Gauss, Abel, Cauchy, and later Weierstrass, Dedekind and Cantor, a satisfactory and rigorous foundation of
calculus was established.
EDIT. Here are two examples of 18 and early 19 century calculus:
$$x=2\sum_{n=1}^\infty\frac{(-1)^{n-1}}{n}\sin nx.$$
This can be verified numerically by plugging some values, and even experimentally. But this makes no sense because the RHS is periodic while the LHS is not.
The second example is from Euler. By multiplication term by term we obtain
$$(1-x)(1+x+x^2+x^3+\ldots)=1.$$
Putting $x=2$ we obtain
$$1+2+4+8+16+\ldots=-1.$$
This can be justified from the modern point of view, but at the time of Euler this led to controversies of course.
Today, the situation is very similar. Physicists discover new mathematical results using methods which make no sense to pure mathematicians. Mathematicians consider these results as conjectures and sometimes prove them with their rigorous methods. As it was in 17s and 18s centuries the process is very beneficial to both physics and mathematics. Actually Archimedes also used physical reasoning to discover new mathematical results, and he always made it very clear what is really proved and what is just ``discovered''.

A:

When speaking about rigor in a historical context, one must be careful not to apply modern habits of thought to historical developments where they are inappropriate.  For example, one distinction that must be made is between the set-theoretic foundations we take for granted today that were not available before the second half of the 19th century.  More specifically, modern punctiform continua (i.e., continua made of points) were certainly not the "foundational" background before, say, 1870.
This does not mean, however, that one can't meaningfully discuss the work of earlier authors without calling it unrigorous.  For example, the work of Gauss is generally considered rigorous.  The key distinction helpful in disengaging ourselves from the too-easy dismissal of historical mathematicians as unrigorous is the distinction between procedure and ontology.
This was dealt with by authors varying from Benacerraf to Quine to Wartofsky but it will suffice to say that the term "procedure" refers to the actual inferential moves as they appear in the work of those mathematicians, whereas "ontology" refers to the justification of the entities like number, point, function, etc. used by those authors.  The set-theoretic ontology commonly taken for granted today was not there in the work of Gauss and others, but this shouldn't prevent a scholar from analyzing their procedures, which often turn out to be rigorous to a satisfactory degree.
Thus, Cauchy used infinitesimals in much the way they would be used today, and his procedural definition of the continuity of a function is essentially indistinguishable from a modern one (an infinitesimal increment assigned to the variable always produces an infinitesimal change in the function), even though the ontological foundation that would be expected today was not there.
To answer your question about Leibniz more specifically, his infinitesimal procedures were more soundly founded than the generally touted critique thereof by George Berkeley.  This overlooks the procedure/ontology distinction.

