Q:

Historical origin of commas and periods in numbers

I have heard that in Canada and the United States of America, when we write a decimal number, then we use a period, like 1.111. When we write large numbers, then we use a comma, like 1,111. Sometimes we use spaces for large numbers, like  1 111. I have also heard that Europe, Southern Africa, Central America, the West Indies, and South America use commas and periods the other way around. So, for Americans, 1.111 is the decimal number, while for Europeans, 1.111 is one thousand one hundred eleven. For Americans, 1,111 is one thousand one hundred eleven, while for Europeans, 1,111 is the decimal number. 1 111 is one thousand one hundred eleven for the entire world. How did the commas and periods develop historically?
Update: Recently, USA and China too switched to using comma for the decimal. Now "Made In China" products too display the decimal comma.

A:

(I must admit that I'm writing this mostly from memory and I don't have sources and not enough time to look for any, so some details may be wrong.)
Simon Stevin, who developed the notation of decimal fractions in the late 16th century, originally used a circle with numbers inside, so 1.111 would be written as "1⓪1①1②1③" and 365.256 (the length of a year in days) as 365⓪2①5②6③, where ⓪ means the preceding digits are the integer part, ① means 1/10, ② means 1/100 and so on. Stevin published this in the pamphlet "De Thiende" ("The Tenth"), written in Flemish, in 1585 (thanks to @njuffa for providing this detail in the comments).
The positional system for integer numbers had, by that time, already a long history; it was originally developed in India, spread through the Islamic world and was introduced to Europe by Leonardo von Pisa (Fibonacci) in 1202. So Stevin did not need or want to modify the integer part of a number, as this was already well established; Stevin's notation just extends it to include fractional parts by marking it with the circled numbers.
Later people found that the ①, ② etc. are not necessary; you only need the marker ⓪ where the integer part ends and the fractional part begins, and later the ⓪ developed into a dot as it was easier to write, initially centred above the line, but later on the line.
In Germany, Leibniz however introduced a center dot for multiplication (as we still do) instead of the cross in the UK. The reason was that he was working with symbolic algebra and introduced the convention to use x, y, z for unknowns, but the x can easily be confused with the multiplication cross.
But when using a dot for multiplication, having a dot as decimal marker leads again to confusion, so people started using a comma instead.
As France and Germany always had very intense cultural exchange, this became the norm in France too and spread to most other European countries (possibly with Napoleon and the metric system?).
By the way, this was formalised by the 9th Conférence Générale des Poids et Mesures (CGPM) in 1948 and the 22nd CGPM (2003). Resolution 10 of the 22nd CGPM says:

The 22nd General Conference (...)

declares that the symbol for the decimal marker shall be either the point on the line or the comma on the line,

reaffirms that “Numbers may be divided in groups of three in order to facilitate reading; neither dots nor commas are ever inserted in the spaces between groups”, as stated in Resolution 7 of the 9th CGPM, 1948.

The second bit of the resolution is important and not well known; you should neither use a comma nor a point to separate the 1000s, just a space. This is exactly in order to avoid confusion, so 1.111 (like 1,111) can only ever mean 1+111/1000, but never 1111.

