Q:

Why were the matrix multiplication rules chosen this way?

The currently standard matrix multiplication is isomorphic to split-quaternions (for 2x2 matrices, and similar for higher ranks, maybe this is called Clifford algebras). As such, the sets of square matrices is not commutative, has zero divisors, idempotents and nilpotents.
I wonder, why other matrix multiplication rules were not chosen. For instance, the tessarine-like that would be commutative?
Yes, usual quaternions or tessarines or split-numbers of any dimension can be represented by the matrices, but since the multiplication rules differ, one would need the matrices of squared rank to represent them.

A:

As with your previous question about why tessarines' commutativity doesn't win them more interest, commutativity is not mathematicians' goal; problem-solving is. We're interested in vector spaces, and in linear operations from such spaces to the same or other spaces (i.e. matrices), and in the composition of such linear operations (per @Conifold's comment) viz.$$(AB)v:=A(Bv)\implies(AB)_{ij}v_j=A_{ik}(Bv)_k=A_{ik}B_{kj}v_j\implies(AB)_{ij}=A_{ik}B_{kj}.$$You're welcome to consider other operations of the form $(A\circ B)_{ij}:=A_{mn}B_{pq}C_{ijmnpq}$; the example above is $C_{ijmnpq}:=\delta_{im}\delta_{jq}\delta_{np}$.${}^\dagger$ You're welcome to determine which $C$ obtain $A\circ B=B\circ A$. But the irony is this:

These matrices comprise a vector space. We can halve the index count with suitable relabelling that squares the dimension.
Each $\circ$ comprises a two-vectors-to-one-vector bilinear operation, analogous to the cross product on $3$-dimensional space. (Does it bother you that that's anticommutative?)
Such operations also have a natural characterization in terms of matrices as they're defined above. To wit:$$(v\circ w)_i:=K_{ijk}v_jw_k\implies(v\circ)_{ik}=K_{ijk}v_j,\,(\circ w)_{ij}=K_{ijk}w_k.$$In other words, matrices are still the natural software you're running on.

${}^\dagger$ No others will obey $(AB)v=A(Bv)$, so you'll just be treating "matrices" as "arrays of numbers I can do anything I want with" rather than quantities that represent an underlying something in a suitable basis, but I suppose mathematics doesn't force you to do something useful if you don't want to. On the other hand, it doesn't force people to be interested in what you find elsewhere.

A:

Matrices were discovered in China in the 2nd century BCE as documented in the Nine Chapters of the Mathematical Art. Here, the method of Gaussian Elimination  is used to solve matrix equations - and which suggests that the term Gaussian elimination is misnamed. However, the Nine Chapters is an anonymous work, or rather, the author's name has not come down to us, so there is no easy way to refer to the method, other than Gaussian elimination.
This can be generalised to matrix multiplication. More modernly, matrix multiplication is simply the multiplication, or rather composition, of linear transformations in a given basis.

