Q:

When did it become understood that irrational numbers have non-repeating decimal representations?

I know that the notion of irrational number (in one form or another) goes back to the Pythagoreans, and therefore far predates the decimal system, and certainly the representation of non-integer quantities with decimal fractions.  I also know that al-Samaw'al ibn Yahya ibn Yahuda al-Maghribi (1125-1174), in Treatise on Arithmetic (1172), explained that quantities such as $\sqrt{2}$ can be approximated to as much precision as desired using decimal fractions.
Edit: My source the claim about al-Samaw'al is Katz, A History of Mathematics (3rd edition).  (Sorry I don't have access to any primary sources.)  On p. 270 we find the following explanation:

... Similarly, he [al-Samaw'al] calculated the square root of 10 verbally to be 3 plus 1 part of 10 plus 6 parts of 100 plus 2 parts of 1000 plus 2 parts of 10,000 plus 7 parts of 100,000 plus 7 parts of 1,000,000.  Unlike his predecessor [al-Uqlidisi], he still used words to describe the various places. Nevertheless, he understood the value of using decimal fractions for approximating rational numbers or irrational numbers.  In fact, when al-Samaw-al calculated higher roots by a method similar to that used in China, he explicitly noted the purpose of the successive steps of the algorithm:  "And thus we operate to determine the side of a cube, of a square-square, a square-cube and other [powers]. This method enables us... to obtain an infinite number of answers, each one being more precise and closer to the truth than the preceding one."  Al-Samaw'al evidently realized that, in theory at least, one can calculate an infinite decimal expansion of a number, and that the finite decimals of this expansion "converge" to the exact value, a value not expressible in any finite form.

But who was the first to explicitly state (and prove?) that rational numbers have decimal expansions that terminate or repeat, and irrational numbers therefore have non-terminating and non-repeating decimal representations?

A:

Before approximating roots Al-Samawal performs long division of 210 by 13 to five decimal places, not enough to notice that digits cycle after the sixth. And this is the problem with discovering it experimentally in general, rationals may have arbitrarily long periods (repetends), not to mention preperiods. According to Dickson, Al-Maridini in 15th century was first to note periodicity, in a particular example with sexagesimals. Leibniz observed in 1677 that expansion of $1/n$ is periodic for any base (he later added - relatively prime to $n$). Using this it is straightforward to show that all decimal expansions of common fractions terminate or are repeating decimals (a.k.a. periodic fractions), although Leibniz did not spell it out. From this point on patterns in decimal and other base expansions were studied in the context of number theory, Wallis, Robertson, Lambert and Euler were the early contributors, Jean Bernoulli summarized and built on their work. 
The earliest mention of the converse, that all repeating decimals are rationals, comes late, surprisingly late, by Lambert in 1758, he justifies it by using the now familiar trick with the geometric series. In Euler's Elements of Algebra (1765), "one of the earliest books to set out algebra in the modern form... and one of Euler's few writings that are accessible to the general public" we find the same trick $9.999...=9+9/10+9/10^2+...=\frac{9}{1-1/10}=10$. However, according to the formal attitude of the 18th century, e.g. Euler's, we also have $1+2+2^2+\dots=\frac{1}{1-2}=-1$, by definition. It took some time before Cauchy tied summing series to convergence. 
It seems surprising today but that rational numbers have decimal expansions that terminate or repeat, and that irrational numbers have non-terminating and non-repeating expansions are two distinct problems. This is because the connection between them relies on two things: that there are not just examples like $\sqrt{2}$ or $\pi$, but a species of things called "irrational numbers", and that completed, infinitely extending strings of decimal digits represent anything at all, and specifically their specimen. These realizations only crystallized in late 19th century. A symbolic year is 1872, when Cantor, Dedekind and Heine independently published their constructions of real numbers as a species. After 1872 acceptance was quick, a reply to a question in an 1889 issue of The Bizzarre shows that by then they were already commonly known as numbers having non-terminating and non-repeating decimal expansions.
In another book of Katz's the quoted passage is reproduced almost verbatim, except for one word:"one can potentially calculate an infinite decimal expansion of a number". This goes back to the view of Aristotle and Euclid that all infinity is only potential, not actual, not completed, which continued to exert heavy influence on mathematicians until late 19th century. Infinite strings of digits were mostly contemplated as processes, not finished objects. A notable exception was Stevin's Arithmetic (1585), largely responsible for acceptance of the decimal system in Europe (it helped that Stevin was more engineer than mathematician). But rejection of actual infinities infected even the identification of repeating decimals with rationals, let alone of "lawless" strings with irrationals. This attitude is deep seated, even today many students have Aristotelian intuitions of infinity in repeating decimals. According to Tall "interviews revealed that students continued to conceive of  $0.999...$ as a sequence of numbers getting closer and closer to 1 and not a fixed value, because 'you haven’t specified how many places there are'".
It is interesting to compare this to a similar issue for continued fractions. Fowler argues that already Theaetetus (c. 417–369 BC) had a heuristic understanding that simple terminating continued fractions correspond to commensurable ratios and periodic ones to ratios "commensurable in square" 
(rationals and quadratic irrationals in modern terms). But it was stated and proved explicitly only around the same time as for repeating decimals, by Euler in De Fractionlous Continious (1737) and Lagrange in Sur la Resolution des Equations Numeriques (1767), respectively.  

