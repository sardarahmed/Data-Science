Q:

Historically, what led to the question of the validity of interchange of limit operations?

It seems G. H. Hardy once wrote that "The problem of deciding whether two given limit operations are commutative is one of the most important in mathematics". I was wondering what led to the need to formulate the question of when the limit operations commute.

A:

G. H. Hardy wrote in A Course in Pure Mathematics  in Appendix II, section A note on double limit problems

Let us consider some special instances. In $\S\ 213$ we proved that
\begin{align*}
\log(1+x)=x-\frac{1}{2}x^2+\frac{1}{3}x^3-\cdots
\end{align*}
where $-1<x\leq 1$, by integrating the equation
\begin{align*}
1/(1+t)=1-t+t^2-\cdots
\end{align*}
between the limits $0$ and $x$. What we proved amounted to this, that
\begin{align*}
\int_{0}^{x}\frac{dt}{1+t}=\int_{0}^{x}\,dt-\int_{0}^{x}t\,dt+\int_{0}^{x}t^2\,dt-\ldots
\end{align*}
or in other words that the integral of the sum of the infinite series $1-t+t^2-\cdots$ taken between the limits $0$ and $x$, is equal to the sum of the integrals of its terms taken between the same limits. Another way of expressing this fact is to say that the operations of summation from $0$ to $\infty$, and of integration from $0$ to $x$, are commutative when applied to the function $(-1)^nt^n$, i.e. that it does not matter in what order they are performed on the function.

he continues with another important example:

... we proved that the differential coefficient of the exponential function
\begin{align*}
\exp x=1+x+\frac{x^2}{2!}+\cdots
\end{align*}
is itself equal to $\exp x$, or that
\begin{align*}
D_x\left(1+x+\frac{x^2}{2!}+\cdots\right)=D_x1+D_xx+D_x\frac{x^2}{2!}+\cdots;
\end{align*}
that is to say that the differential coefficient of the sum of the series is equal to the sum of the differential coefficients of its terms, or that the operations of summations from $0$ to $\infty$ and of differentiation with respect to $x$ are commutative when applied to $x^n/n!$.

The author accompanies these examples with some more and concludes then in convincing manner:

The really important case (as is suggested by the instances which we gave from Ch. IX) is that in which each operation is one which involves a passage to the limit, such as a differentiation or the summation of an infinite series: such operations are called limit operations. The general question as to the circumstances in which two given limit operations are commutative is one of the most important in all mathematics. ...

We can already defer from the examples above that exchanging limits is of fundamental importance and seemingly ubiquituous in analysis. So, the question for historical origin of the need of exchanging limits is rather hard to answer.
Maybe one could trace back some lines of history by looking at the history of named theorems which are based upon the exchange of limits, like Fubini's Theorem
,Theorem of Schwarz, etc.

A:

Once you're working with $\ge 2$ independent variables, it's necessary anytime you need to be able to commute differential/integral operators. With differentials, for example, we need it in order to change the order in which we take partial derivatives:
$$ \frac{\partial }{\partial x}\left( \frac{\partial f}{\partial y}\right) = \frac{\partial }{\partial y}\left( \frac{\partial f}{\partial x}\right) $$
With integrals, it's necessary anytime we want to take double (or larger) integrals: $$ \int_{X \times Y} f(x,y) \ d(x,y) = \int_X \left( \int_Y f(x,y) \ dy \right) dx = \int_Y \left( \int_X f(x,y) \ dx \right) dy $$
It also plays a role when working with limits of derivatives or limits of integrals, as for example in Hardy's own work.

