Q:

What is the distribution of corrected visual acuity?

While talking with friends about retina displays, resolution of TVs in relation to viewing distance, and so on, I pointed out that not everyone goes periodically to the eye doctor, unless the difficulty in seeing is evident. Not everyone therefore needs retina displays.
I wonder what is the actual distribution, if known, of the actually corrected visual acuity in the population.
I'm not referring to the achievable acuity after perfect correction, I'm referring to the distribution of what people tolerate before going to the doctor and have the glasses updated. For example, I know that many young or mature people with about 0.9975 (decimal scale, also called -0.25 dioptries) do not wear glasses at all because not worth the effort. Older people typically care even less about proper correction, because their "good enough" is broader.
Are there studies that measured this parameter?

A:

Most people will tolerate a visual acuity of up to 20/40 (being able to read something at 20 feet that a normal person with "perfect" vision can read at 40 feet) before seeing a physician. According to this article (https://www.ncbi.nlm.nih.gov/pubmed/16325714), patients over the age of 65 are more likely to tolerate this level of vision before seeking proper correction, similar to your statement of their vision being "good enough". 
Other articles that touch on this topic include the following:

https://www.ncbi.nlm.nih.gov/pubmed/18451738
https://www.ncbi.nlm.nih.gov/pubmed/18161606

