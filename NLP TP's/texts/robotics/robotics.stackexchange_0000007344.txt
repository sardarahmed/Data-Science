Q:

Image based 3d position estimation with one camera

There is too many 2d position estimation with one camera. Is there any 3d position estimation application or technique with one camera? If there is no application or technique why?

A:

One camera gives you no depth information, so you have to have some information about the scene before you start (a priori).
The most common way to handle this is with structured light, where you project a known pattern and evaluate deviations from the pattern. 

A:

It is possible to use one camera for 3d position estimation but it is significantly more difficult.
Depth/distance data must be obtained to generate the 3d representation and several techniques have been developed to this end such as rapidly moving the camera to a known, fixed secondary position to capture a second image for processing or mounting a fixed mirror in the camera view to provide the offset image.
However the reliability and complexity of these methods haven't led to their widespread uptake. Arguably the IR CMOS sensor in the Kinect is not a 'camera' and this system uses a 'point cloud' projection and interpretation method to obtain it's depth data.
Overall the simplified two camera approach has historically been the most cost effective and the maths behind the processing algorithms came from sound processing and radar signal interpretation, distance and range finding.

A:

take an image than take with little zoom out. crop two specific points from first and match (tepmlate)from second. than calculate difference this gives relation between two images than you can compute depth with camera angle.

