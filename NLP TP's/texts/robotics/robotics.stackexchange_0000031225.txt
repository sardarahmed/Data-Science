Q:

Limiting depth kinect sees in rgbdslam?

Is there any way I can limit the depth to which I record data in rgbdslam? What I mean by this is, I don't want to accumulate points that are far away from the camera.
My main goal is to use rgbdslam to scan objects, but it involves a lot of human post-processing to isolate the object and the file sizes are very big. It would be nice if there was a way I could reduce the human element and reduce the size of these point clouds altogether.
Is there any way this can be done while recording data online? Or is the only way to do this is to scan the pcd file after wards and remove points that are far away (have a high (x^2 + y^2 + z^2)^.5 value).

Originally posted by Shark on ROS Answers with karma: 241 on 2011-04-21
Post score: 0

A:

How about adding a pcl_ros/PassThrough nodelet? I haven't used the RGBDslam since it first came out, but I would think you should be able to remap the input topic of RGBDslam to the output of the PassThrough filter, which can be setup to limit the Z-height (depth) of the cloud.
There is a very brief tutorial on PassThrough filter on the wiki: http://www.ros.org/wiki/pcl_ros/Tutorials/PassThrough%20filtering

Originally posted by fergs with karma: 13902 on 2011-04-22
This answer was ACCEPTED on the original site
Post score: 1

