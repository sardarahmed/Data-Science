Q:

How to process pointclouds remotely, on workstation?

So I would like to run the PCL People Detection Code for the Kinect of my TurtleBot. However I want to do most of the 'work' on my (more capable) workstation, rather than the TurtleBot's netbook, to speed up things considerably.

So how do I run openni_launch/openni.launch (or rgdb_launch/kinect_frames.launch?) such that it does no processing, but just publishes raw data? I have looked into the launch files and noticed that a lot of processing args are set to true by default like here and here for example. So do I just set the ones that I don't need to false?

Having set the above args to false, and desiring to move the task of processing to my workstation, now I assume I have to, from my workstation, subscribe to this 'raw topic' (published to by the Kinect), retrieve the raw data, and do the processing (whatever the above args do) on the workstation. How would I go about doing that? I can't find the source code where these args are defined (in functions or whatever) and used!

Thank you.

EDIT:
Based on my research this is what I now run:

rosrun openni_launch openni.launch with all processing modules turned off. The following are the relevant (not all) camera topics active.
/camera/depth/camera_info
/camera/depth/image_raw
/camera/depth_registered/camera_info
/camera/depth_registered/image_raw
/camera/rgb/camera_info
/camera/rgb/image_raw

ROS_NAMESPACE=camera/rgb rosrun image_proc image_proc
ROS_NAMESPACE=camera/depth_registered rosrun image_proc image_proc

The above two, according to this, should give me the required topics - rgb/image_rect_color and depth_registered/image_rect - I need for converting to rgb pointclouds according to this.

Before converting to pointcloud though, I convert to metric by rosrun nodelet nodelet load depth_image_proc/convert_metric camera/camera_nodelet_manager --no-bond

Finally, I run rosrun nodelet nodelet load depth_image_proc/point_cloud_xyzrgb camera/camera_nodelet_manager --no-bond.

Now here's the problem: I do get the required /camera/depth_registered/points topic active, but I do NOT have any information on it, as evinced by a rostopic echo. Am I converting incorrectly?
EDIT 2 I have realized that running image_proc for camera/rgb 'works' because the topics it publishes to actually have infomation being published onto it; however, for camera/depth_registered, the topic being published to i.e. /camera/depth_registered/image_rect, has no information on it. As a result, the next step i.e. running depth_image_proc/point_cloud_xyzrgb would not work as it subscribes to an empty topic.
P.S. @bhaskara I know you are familiar with this (https://gist.github.com/bhaskara/2400165) so could you help me? @derekjchow you too please :)
Thanks guys!

Originally posted by oswinium on ROS Answers with karma: 105 on 2014-04-14
Post score: 1

A:

The preferred method for sending Kinect data over WiFi is to send the depth image, then convert to a point cloud on the processing PC.
In your situation set up distributed ROS (http://wiki.ros.org/ROS/Tutorials/MultipleMachines), then run "roslaunch openni_launch openni.launch" on the turtlebot.
On your host PC, you can run depth_image_proc/point_cloud_xyz to convert from a depth image to a point cloud message.

Originally posted by derekjchow with karma: 341 on 2014-04-15
This answer was ACCEPTED on the original site
Post score: 0

Original comments
Comment by oswinium on 2014-04-17:
So do I have to set the processing modules to "false" when I run openni.launch on the turtlebot?

