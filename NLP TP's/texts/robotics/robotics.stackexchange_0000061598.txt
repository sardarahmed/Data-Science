Q:

tf impact on perf (tf design)

Hi all,
I have been playing with tf a bit and I am wondering about its perfs mainly related to design reasons. Maybe some of you can put some light on my thoughts... Here they are:
tf is kind of a swiss knife for transforms, it ensures communication between nodes even distantly, buffers the data for history access, makes transformations between frames automatically... This is really great for ease of use, though:

tf buffers the transforms for historic access, it is all sorted for fast access, but even though the access is as fast as possible searching in a sorted list comes at a high cost
The frames are accessed by name (e.g. listener.lookupTransform("/ee_link", "/world", ros::Time(0), transform);), this access implies another search in a sorted list for the name: argh!
robot_state_publisher sends data we may not use, as it is over TCP/IP (and not via shared memory or so) it has an impact on perf

Especially as most use cases have nodes exchange transforms data on the same machine

Interpolation / extrapolation...

I am coming from the RT physics simulation / control world and so far I would have banned all the points mentioned above and traded all them off for a super-fast raw data structure with a uber-simple dereferencing for access. Is there something I missed or using tf has an enormous hit on perf?
Thanks,
Antoine.

Originally posted by arennuit on ROS Answers with karma: 955 on 2014-08-07
Post score: 2

A:

You're right that tf was designed focusing on ease of use. I'll comment on each of your points.

We have tested using a binary tree structure for lookup which is faster for arbitrary lookups. However we observed actually higher lookup times and more overhead on maintaining the structure than the linked list. The maintenance of the tree is obviously higher overhead as you need to insert into the tree and prune in the tree. And as most data comes in in sequence it never needs to walk the tree when inserting. And pruning a sorted linked list is also trivial. For the lookup times most of the time queries are very close to the front of the list and although the lookup complexity is O(n/2) where n is the length of the list. In most use cases it is much faster since it walks the list from the newest data. The other thing to keep in mind is that although the user/developer mostly sees the lookup times. There are many more insertions and background updates than lookups.
Names are stored in a hash map, which does not require a search. (yes it is not realitme safe to be processing strings, but names mean a lot more to developers and users than numeric ids. We tested originally using numeric frame ids and it was wholly undebuggable.
When sending data on the same machine the kernel is quite optimized and effectively passes the data through shared memory over the loopback interface. I've never seen tf data be a bottleneck performance wise on loopback. (It does become a problem over wifi)
I'm not sure what you mean by this bullet point. Interpolation is required estimate transforms across unsyncronized datasources. As well as allowing lower publish rates for data which is able to reflect the bandwidth of the signal of a link. (Fast moving transforms require high speed publishing. Low speed links can publish infrequently but remain accurate. Without this every link would be required at every timestep wanted to query, so it's actually a significant performance boost.) And extrapolation is discouraged and disabled by default.

Overall there are things which can be optimized however in typical testing on a desktop a tf lookup is measured in microseconds so we haven't worked on optimizing it further. There are many other things to optimize. The biggest bottleneck, mentioned earlier, is bandwidth usage over wifi. And there are several different techniques for downsampling and forwarding data over a limited bandwidth link.

Originally posted by tfoote with karma: 58457 on 2014-08-07
This answer was ACCEPTED on the original site
Post score: 6

Original comments
Comment by arennuit on 2014-08-08:
Haha! Thanks a lot for this very clear explanation. I better understand the solutions and trade offs now. It definitely looks less scary than it initially seemed.

