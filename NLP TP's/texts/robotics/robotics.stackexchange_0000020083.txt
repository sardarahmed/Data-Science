Q:

What does "resolution of a sensor" mean?

I am studying robotics and I am focusing my attention on the sensors used on robot manipulators. I have seen that there a proprioceptive sensors, such as encoders, and exteroceptive sensors, such as range finders and sonars.
During my studies on this topics I have encoutered the concept of resolution of a sensor and I have seen in paericular that the encoder, both absolute and incremental, have a way of calculating resolution, and also exteroceptive sensors have the resolution, such as depth resolution , angular resolution and width resolution.
I am tryin to understand how everything works, but I cannot understand the concept of resolution. I have seen that the definition is :

The resolution of a sensor is the smallest change it can detect in the quantity that it is measuring. The resolution of a sensor with a digital output is usually the resolution of the digital output. The resolution is related to the precision with which the measurement is made, but they are not the same thing. A sensor's accuracy may be considerably worse than its resolution.

which I have found here.
But I don't understand this concept very well since I don't know how to apply it in the various problems and cannot idetify the concept of resolution with the various type of resolution I mentioned above.
Can somebody please help me? 

A:

This ruler has a resolution of 1 inch:

It has a resolution of 1 inch because the only readings on the ruler are 0, 1, 2, or 3 inches. It can't measure anything smaller than an inch; it only measures inches. 
The ruler below has a resolution of a half-inch (0.5 inches). This ruler now has the capacity to measure 0, 0.5, 1.0, 1.5, ..., 3.5 inches. 

For these rulers, you or I could look at them and maybe guess a better reading, but digital circuits need to quantize the data at some point so it can be counted. The reading is more than an inch or it isn't. It's more than 2 inches or it isn't. Etc. 
Consider a LIDAR unit. It flashes a laser and starts a stopwatch. It counts time until it sees the reflection, and then estimates the distance to the reflecting object by half the speed of light times that elapsed time (because the laser shines out and then returns - it travels twice the distance). 
The stopwatch counts the elapsed time down to some degree of precision, and that precision then determines the positioning accuracy of the LIDAR.
Digital cameras have to measure the intensity of light that strikes the receptor; that intensity has to be counted, so again the ability to differentiate light intensities (and thus shades of color) is limited by how many values the sensor has available for counting. 
The odometer in your car might read to the nearest tenth of a mile/kilometer, but your car actually moves smoothly between those points. Again, your car's odometer is limited in the distance it represent because it has a limited number of digits from which to choose. You could have an odometer accurate to the nearest hundredth of a kilometer if you wanted, or meter, millimeter, etc., but then the display would be very large, the dials would turn very quickly and wear, etc., and what's more is that it wouldn't provide the driver with useful information.
It all comes down to counting. To be digital, it must necessarily be countable. To be countable, there must then be some approximation. How close of an approximation is required will depend on your application requirements and is really at the heart of a lot of engineering work.  

