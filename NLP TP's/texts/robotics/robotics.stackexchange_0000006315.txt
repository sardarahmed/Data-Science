Q:

Pose estimation, how to populate set of known edges and points?

I am building an estimator that solves for the camera pose relative to a reference frame which contains a known set of features and edges. Currently, the system works with an unscented kalman filter with four known points (red leds) in the reference frame. I am now hoping to improve robustness by adding edges to the model as well as robust features. I would like to add additional points that are uncovered by some opencv feature finding function (fast,cornerHarris,...).
So far I found the paper "Fusing Points and Lines for High Performance Tracking" and "Robust Extended Kalman Filtering For Camera Pose Tracking Using 2D to 3D Lines Correspondences" which seem to detail how to fuse edge and feature matching for pose estimation.
Is there a strategy to populate the known set of edges and features when it is impractical to measure them with a ruler/tape measure? My first thought is to start with a small known set of features, my red leds, then run some slam algorithm and keep all features/edges that have some minimum certainty.
Thanks a bunch!
I have misunderstood the RANSAC algorithm. This is not appropriate for my application. 
For those interested, I am hoping to use a similar approach to the one presented in the following paper.
Youngrock Yoon, Akio Kosaka, Jae Byung Park and Avinash C. Kak. "A New Approach to the Use of Edge Extremities for Model-based Object Tracking." International Conference on Robotics and Automation, 2005.

A:

First some terminology, finding camera pose is known in computer vision as external camera calibration. Since you mentioned Kalman filter, I assume you need to need to "update" the so-called external parameters of the camera continously (i.e. the camera is moving). 
If the camera moves in front of the reference frame you might use the usual chessboard pattern as a calibration rig. If however the camera moves about the reference frame, you might want to use other object and test robust feature detectors (like SIFT)  but you have to make sure your object doesn't have symmetrics that might lead to ambiguities..
In case you don't have control over the envionment, there are the so-called automatic calibraiton methods (see reference). The problem with this approach is that you have the assumption that the features you find in your envionment belongs to rigid bodies (on Mars you can make such assumption safely ;) 
References:

SIFT http://en.wikipedia.org/wiki/Scale-invariant_feature_transform
OpenCV Camera Calibration http://docs.opencv.org/trunk/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html
Carrera et al, "SLAM-Based Automatic Extrinsic Calibration of a Multi-Camera Rig", 2011
Miksch et al, "Automatic Extrinsic Camera Self-Calibraiton Based on Homography and Epipolar Geometry", 2010

