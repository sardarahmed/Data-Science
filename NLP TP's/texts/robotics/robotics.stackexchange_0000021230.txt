Q:

Computing attractive and repulsive forces from gradient of Artificial Potential Functions for path-planning

I'm trying to design a controller for a path-planning robot  using Artificial Potnetial Functions [Prof. Howie Choset, Page 6 and 9, $\nabla D(q)$] and had a question about calculating the gradients of the potential functions to generate the force.
From what I understand, the attractive and repulsive force is calculated from the gradient of the attractive and repulsive potentials, i.e. $\nabla f_{a}$ and $\nabla f_{r}$.
$\begin{equation}
f_{r} = \begin{cases} \eta * (1/d(q, q_{o}) - 1/\rho_{o}))^2, && d(q, q_{o}) \leq \rho_{o} \\
0, && d(q, q_{o}) > \rho_{o}
\end{cases}
\tag{1}
\end{equation}$
$\begin{equation}
f_{a} = \epsilon * d(q, q_{g})^2 
\tag{2}
\end{equation}$
$q$, $q_{o}$ and $q_{g}$ are the coordinates of location of the agent, nearest obstacle  and the goal respectively. $\rho_{o}$ is the repulsive radius of each obstacle. $d$ is the Euclidean distance.
This gradient operation on each of the potentials in (1), (2), yields an $x$ and a $y$ component, which are the $x$ and the $y$ components of the gradient vector at that point. We then combine all the horizontal components and the vertical components to calculate the angle, $\theta$, of the resultant force $F_{r}$ with the horizontal.
Is this the right approach or am I misinterpretating the $\nabla$ operations? The $X$ and the $Y$ components of the gradient at each point have to be computed seperately and then combined or am I missing something here?
Any and all help is appreciated!

A:

The role of the gradient here is to get vector information from the potential functions and not just a value. You are right in assuming, that you work separately on the X and Y axes, in the sense that the equations are vector and not scalar equations.
The potential function is a differentiable scalar function. Adding the gradient to the scalar function will return a vector and this operation transforms the operations to a vector function. This is not the only affect of the gradient function. The gradient does not only project to axes, but expresses the (partial) derivates, instead of just projections. This effect is very nicely demonstrated on the Wikipedia page of the gradient on this image.

