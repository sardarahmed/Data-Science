Q:

How much should I expect a Kalman filter to converge?

I am learning about Kalman filters, and implementing the examples from the paper Kalman Filter Applications - Cornell University.
I have implemented example 2, which models a simple water tank, filling at a constant rate. We only measure the tank level, and the Kalman filter is supposed to infer the fill rate.

According to the model, the fill rate is a constant, so I assumed that over time, the Kalman filter would converge more and more accurately (and with less and less noise) on the correct fill rate. However, the amount of noise in the fill rate never seems to reduce after the first few iterations:

This graph shows how the fill rate part of the state vector changes over the course of 1000 iterations of the simulation.
Adjusting the Measurement Variance Matrix seems to have very little effect on the fill rate noise.
Also, the Kalman gain vector and State Variance matrix seem to be constant throughout the simulation. I assumed that the State Variance would reduce as the filter became more and more confident in its state estimate.
Questions:
- Is this graph what I should expect to see?
- Should the Kalman Gain vector and State Variance matrix change over time in this situation?

A:

Let's assume a constant Kalman filter $x_n = Gx_{n-1}+\omega$ with $\omega \sim \mathrm{N}(0, W)$ and $y_n = Fx_n+ \nu$ with $ \nu \sim \mathrm{N}(0,V)$, and initial state $x_0 \sim \mathrm{N}(m_0, C_0)$, where $C_0$ is the initial state variance. Then, the sequence of state covariance matrices $C_0, C_1, \dots, C_n$ converges to a constant covariance matrix $C > 0$ when $n \to \infty$ if the system is observable, and $C$ is a function of the system and measurement covariance matrices $W$ and $V$, respectively (i.e., depends only on the signal-to-noise ratio). The Kalman gain also converges to a constant matrix. This convergence process occurs in just a few steps.
In summary, the state covariance matrix $C_n$ does not reduces indefinitely with time, it reduces until achieve a limiting constant covariance matrix $C>0$. In fact, it is not even sure that $C_n$ will decrease! If $C_0 < C$, the sequence $C_0, C_1, C_2 \dots$ increases until $C$.
See the second chapter of the following reference for a proof: Bayesian forecasting and dynamic models, by West and Harrison

A:

The reason the filter stops converging on a solution is because of the Process Variance Matrix, Q.
This matrix tells the Kalman filter how imperfect the model itself is. If the matrix contains only zeros, you're telling the filter that the model is perfect, but if it contains non-zeros, you're saying that the model can't entirely be trusted, and that more weight should be given to the measurements. This will then limit the filter's convergence.

