Q:

Points in a pointcloud and their distance from camera

Hi.
I wrote a node that gets the pointcloud from the Kinect and converts it from PointCloud2 to the pcl PointXYZ cloud.
Now, for starts, I wanted to take some middle point in the structure ([319][239] for a 640x480 2d pointcloud, i guess) and measure it's distance from the camera, and I did it by accessing points[76241].z, and likewise for other two coordinates, but now I see that it doesn't work like that, because in each frame i get completely different value, not only on the z coordinate, but also on x and y, and they should stay fixed because it's the same point in the 2d matrix.
First of all, if I access it that way, am I really looking at the same point in each frame? Or is it just that the origin of the coordinate system is not at the position of the camera? How would I get for example, that middle point and it's distance from the camera?

Originally posted by kameleon on ROS Answers with karma: 68 on 2012-06-10
Post score: 1

Original comments
Comment by Astronaut on 2013-06-27:
Hello.. I have the same problem. I wont to know the minimum distance from the obstacle to the robot(camera). Any help on this??Thanks
Comment by stark on 2015-01-08:
Have you manage to get the solution for this problem? If yes please let me know the solution even i am facing the same problem.
Comment by skr_robo on 2016-07-15:
Hello, Can you explain the logic of picking 76241 (I do realize that it is 319*239). But shouldn't it be (318 * 480)+238?

A:

I've said this many, many times on ros answers - point.z IS NOT distance from the camera. I don't know where this misconception keeps coming from, but how coordinate frames work is clearly laid out in this REP. To summarize, here's what the fields on a point mean:

x forward
y left
z up

Or in an optical frame

z forward
x right
y down

Note that forward is not the same as depth.
To answer your question, using linear indexing there's no guarantee on the ordering of the points, so trying to access the "middle" point like that could just return some arbitrary point. Additionally, since the Kinect is a noisy sensor, even if you are getting the "same" point, all the values will be slightly different each time. If you actually just want the distance from the center of the camera, you should consider using the depth image instead. In the depth image, each pixel represents a distance in mm, and you can index into it as you would in any image.

Originally posted by Dan Lazewatsky with karma: 9115 on 2012-06-10
This answer was ACCEPTED on the original site
Post score: 3

Original comments
Comment by kameleon on 2012-06-10:
Sorry for a newbie question like this.
Now that you mention it, depth image seems to be something that I could use for my project, but I'll probably need surface reconstruction later and so on... but thanks.
Comment by Dan Lazewatsky on 2012-06-10:
No worries. The fact that the question keeps coming up probably means that it needs to be documented better/more visibly.
Comment by fersarr on 2013-05-04:
Sorry for continuing with the noob questions, but what would forward mean then? If I was using a _optical frame, z is forward, right? Isn't that the transformation distance relative to the _optical frame? wouldn't that make it depth?
Comment by MartinH on 2015-08-18:
I think the misconception of point.z beign depth might come from PCL/openni having the viewing direction along the z-axis by default.

