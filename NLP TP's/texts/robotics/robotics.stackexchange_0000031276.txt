Q:

How do cloud and image indices correspond in tod_training?

Hi, my task is to retrieve 3d points that correspond to 2d points (guess inliers) from a query cloud. There is tod::PCLToPoints which exactly does that. I am wondering though why there is one common scaling factor for both x and y indices from image coordinate space.
int u = float(cloud.width) / image.cols * x
int v = float(cloud.width) / image.cols * y
// such that cloud.at(u, v) corresponds to image(x, y)

What's the reason behind this? I expected that the proportion between y and v is given by dividing cloud height by number of rows in the image, yet experiments show that the approach tod::PCLToPoints is correct (on point clouds and images gathered with tod_training scripts and Kinect camera).

Originally posted by Julius on ROS Answers with karma: 960 on 2011-04-25
Post score: 0

A:

The depth is always 640x480. But the image itself can have a different size/aspect ratio with the Kinect. Hence, you need to rely on the cloud.width to get the right dimensions/ratio.

Originally posted by Vincent Rabaud with karma: 1111 on 2011-06-17
This answer was ACCEPTED on the original site
Post score: 1

