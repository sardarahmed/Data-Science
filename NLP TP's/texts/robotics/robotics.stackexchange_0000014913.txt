Q:

How can I compute the covariance of a camera-based relative pose measurement?

I am trying to compute the relative pose between two cameras using their captured images through the usual way of feature correspondences. I use these feature matches to compute the essential matrix, decomposing which results in the rotation and translation between both the views. I am currently using David Nister's 5 point algorithm to compute the essential matrix and subsequently the relative pose. Once I compute this relative pose:

How can I find the uncertainty of this measurement? Should I try to refine the essential matrix itself (using the epipolar error), which results in the essential matrix's covariance and is it possible to find the pose covariance from this? Or is there another way to find the uncertainty in this pose directly?
There is also another issue in play here: While I am computing the relative pose of camera 2 (call it $P_2$) from camera 1, the pose of camera 1 (say $P_1$) would have its own covariance $\Sigma_1$. How can I consider the effect of this on the covariance of $C_2$ ($\Sigma_2$)?

A:

Typically, when doing this type of pose estimation, the essential matrix would be wrapped inside of RANSAC. i.e., you would have a lot of candidate point correspondences and would randomly sample 5 of them to evaluate with Nister's algorithm to give potential pose solutions, do this several times, and pick the best. If there is such a random process involved, you can repeat it many times to get an empirical estimate of the covariance. 
You can also do the same thing while sampling from the distribution given by $P_1$ and $\Sigma_1$ to incorporate that uncertainty, though you may need to do this a great many times to capture the effects of small terms.

