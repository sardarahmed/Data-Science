Q:

Navigation Stack with omni-wheel Robot

Hi everybody,
I worked through the navigation stack tutorial yesterday and now I am wondering how to get the stack running on my 3-wheeled omni-wheel robot equipped with a Arduino Nano, an old Laptop, and an MS Kinect. The ROS stack is already running as well as freenect. (It may look a bit like Robotino)
It would be great if you would give me some hints how to get it running. I am especially confused how to setup the 3 omni-wheel control.
Thanks very much in advance!
Karamba

Originally posted by karamba on ROS Answers with karma: 41 on 2015-02-08
Post score: 4

A:

I'm assuming by ROS stack running, you mean that you have a base driver that takes in cmd_vel, and spits out odom and tf?
For converting Kinect data into laserscans for navigation, take a look at https://github.com/ros-perception/perception_pcl/blob/indigo-devel/pointcloud_to_laserscan/launch/sample_nodelet.launch. For Kinect, you'll have to use openni/freenect instead of openni2.
There's some sample navigation stuff for another omni-wheel robot here: https://github.com/paulbovbel/nav2_platform/tree/hydro-devel/nav2_navigation/launch
The important distinction is that your robot is holonomic, so you can give it an x, y, theta move command. Diff drive on the other hand would only take x, theta.
If you're gonna reuse the configs there, make sure you tweak speed/acceleration limits, footprint, and anything else robot specific. There's some amcl and gmapping demo bringups in https://github.com/paulbovbel/nav2_platform/tree/hydro-devel/nav2_bringup as well.

Originally posted by paulbovbel with karma: 4518 on 2015-02-21
This answer was ACCEPTED on the original site
Post score: 2

Original comments
Comment by Cyril Jourdan on 2018-11-22:
Hi, I'm also trying to get the navigation stack working on a 3 omni-wheel robot. Do you actually have to write a node that translates changes in motor encoder positions to an odometry message ?

