Q:

How do SLAM algorithms handle a changing environment?

I'm doing some groundwork for a project, and I have a question about the current state of SLAM techniques.
When a SLAM-equipped device detects an object, that object's position is stored. If you look at the point cloud the device is generating, you'll see points for this object, and models generated from it will include geometry here.
If an object is placed in a previously-empty space, it is detected, and points are added. Subsequent models will feature geometry describing this new object.
How does the device react if that object is removed? As far as I've seen, SLAM systems will tend to leave the points in place, resulting in "ghost" geometry. There are algorithms that will disregard lone points caused by transient contacts, but objects that remained long enough to build up a solid model will remain in the device's memory. Are there any systems that are capable of detecting that previously-occupied space is now empty?

A:

That very much depends. Since SLAM is a problem (or at least a technique), not a solution, there is no definitive SLAM algorithm.  Semantically, you have to decide what goes on a "map" of the environment, and that determines how your algorithm should handle transient (aka moving) signals. But that's a digression.
Permanent maps:
Permanent maps should contain enough information to localize yourself with respect to known geometry. Typically used in buildings. Typically human-readable. See Willow-Garage's work. or anything by Thrun in his quite famous textbook. If you lose this map, you have to build it up over time again. 

Removing objects. Yes, the object will appear in a static map for a time. If no measures are taken to remove previously-detected objects, then it will persist. A typical 2D grid-based representation will use each grid cell to represent the probability of an object, so after time the object will "fade".
Adding objects. Same as above.

Local maps:
In reality, SLAM is usually used to localize a robot as it moves, and the map is not kept permanently (or, it is kept permanently, but only the closest Y features are used). Local maps are whatever the robot needs to know to determine how it moved in the last X minutes, where X depends on the application. If you lose the map, you can still fly just fine by using whatever features are in sight right now. 

Batch methods such as Bundle Adjustment using visual features is a very common technique in this direction. Features may be kept over time, and even revisited, but a moving feature is just an unreliable feature, and it will be ignored when trying to figure out where the robot is.
Visual SLAM is exactly this. It is a delta-P (change in pose) estimator, not a map-based localization algorithm. 
In short, as long as most things are not currently moving, it doesn't matter if you remove an object when the robot isn't "looking" at it.

Example
So do this. When you read a SLAM paper, decide the following:

Are they really building a map?
Are they just keeping a list of features and locations?
If so, what "features" go in the map? Lines, points, visual features?
Are these features likely to move? 
If so, how can they handle that?
Finally, sensor noise often "looks" like moving features. How do they handle sensor noise? Because this will often determine what happens to moving features.

You'll get a different answer for each paper / author / book / application. In short, they are typically omitted since they don't help the robot localize much, and can be avoided by simply having a few low-level path planner that only uses local information. 
Good luck, slam is a huge topic.

