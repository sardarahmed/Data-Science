Q:

Can I write the seperation principle for LQG controllers in this state space form?

Look at this picture. This is the seperation principle diagram. 

It is an LQG controller which going to control the real life process. What I want to do, is to create a state space model for this seperation principle system, including the real life process. A LQG controller is a LQR controller together with the Kalmanfilter. Kalmanfilter is also called an observer. 
The LQR controler is a feedback gain matrix L and the kalmanfilter is just a mathematical description of the real life system with a gain matrix K.
r(t) is the reference signal vector which describe how the system's states should hold e.g. temperature or pressure. 
y(t) is the output from the real life process.
$\hat{y}$ is the estimated output from the kalmanfilter.
d(t) is the disturbance vector for the input. That's a bad thing, but the Kalman filter are going to reduce the disturbance and noise. 
u(t) is the in signal vector to the real life system and the kalmanfilter.
n(t) is the noise vector from the measurement tools.
x(t) is the state vector for the system.
$\dot{x}$ is the state vector derivative for the system.
$\hat{x}$ is the estimated state vector for the system.
$\dot{\hat{x}}$ is the estimated state vector derivative for the system.
A is the system matrix. B is the in signal matrix. C is the output matrix. L is the LQR controler gain matrix. K is the kalmanfilter gain matrix. 
So....a lot of people create the state space system as this: $$ $$
For the real life system:
$$ \dot{x} = Ax + Bu + d$$
For the kalmanfilter:
$$\dot{\hat{x}} = A\hat{x} + Bu + Ke$$
But $u(t)$ is:
$$u = r - L\hat{x}$$
And $e(t)$ is:
$$e = y + n - \hat{y} = Cx + n - C\hat{x} $$
And then...for some reason, people says that the state space model should be model by the state estimation error:
$$\dot{\tilde{x}} = \dot{x} - \dot{\hat{x}}  = (Ax + Bu + d) - (A\hat{x} + Bu + Ke)
$$
$$ \dot{\tilde{x}}  = (Ax + Bu + d) - (A\hat{x} + Bu + K(Cx + n - C\hat{x})$$
$$ \dot{\tilde{x}} = Ax - A\hat{x} + d - KCx - Kn + KC\hat{x} $$
And we can say that:
$$\tilde{x} = x - \hat{x} $$
Beacuse:
$$\dot{\tilde{x}} = \dot{x} - \dot{\hat{x}}$$
The kalmanfilter will be:
$$ \dot{\tilde{x}} = (A - KC)\tilde{x} + Kn$$
The real life process will be:
$$ \dot{x} = Ax + Bu + d = Ax + B(r - L\hat{x}) + d = Ax + Br - BL\hat{x} + d$$
But:
$$\tilde{x} = x - \hat{x} \Leftrightarrow  \hat{x} = x - \tilde{x}$$
So this will result for the real life process:
$$ \dot{x} =  Ax + Br - BL(x - \tilde{x}) + d = Ax + Br - BLx + BL\tilde{x} + d$$
So the whole state space model will then be:
$$
\ \begin{bmatrix} \dot{x} \\ \\\dot{\tilde{x}} \end{bmatrix} =\begin{bmatrix} A - BL& BL \\ 0 & A-KC \end{bmatrix} \begin{bmatrix} x\\ \tilde{x} \end{bmatrix}+\begin{bmatrix} B & I & 0\\ 0 & 0 & K \end{bmatrix}\begin{bmatrix} r\\ d\\ n \end{bmatrix}\
$$
Youtube example:
https://youtu.be/H4_hFazBGxU?t=2m13s
$I$ is the identity matrix. But doesn't need to be only ones on digonal form. 
$0$ is the zero matrix.
The Question: $$ $$
If I write the systems on this forms:
$$ \dot{x} = Ax + Bu + d = Ax + B(r - L\hat{x}) + d$$
For the kalmanfilter:
$$\dot{\hat{x}} = A\hat{x} + Bu + Ke = A\hat{x} + B(r - L\hat{x}) + K(y + n - \hat{y})$$
Beacuse $u(t)$ and $e(t)$ is:
$$u = r - L\hat{x}$$
$$e = y + n - \hat{y} = Cx + n - C\hat{x} $$
I get this:
$$\dot{x} = Ax + Br - BL\hat{x} + d$$
$$\dot{\hat{x}} = A\hat{x} + Br - BL\hat{x} + KCx + Kn - KC\hat{x}$$
Why not this state space form:
$$
 \begin{bmatrix} \dot{x} \\ \\\dot{\hat{x}} \end{bmatrix} =\begin{bmatrix} A & -BL \\ KC & [A-BL-KC] \end{bmatrix} \begin{bmatrix} x\\ \hat{x} \end{bmatrix}+\begin{bmatrix} B & I & 0\\ B & 0 & K \end{bmatrix}\begin{bmatrix} r\\ d\\ n \end{bmatrix}\
$$
Youtube example:
https://youtu.be/t_0RmeSnXxY?t=1m44s
Who is best? Does them both works as the LQG diagram shows? Which should I use?

A:

Both state space representations are equivalent. For example the eigenvalues of the two closed-loop system matrices should be the same. However when implementing LQG you only have access to the outputs and the variables you introduced yourself, so either $\hat{x}$ or $\tilde{x}$. But you are still limited by the given dynamics of the states of the system you want to control
$$
\dot{x} = A\,x+B\,u,
$$
where only $u$ can be chosen freely. As stated before, the true state $x$ is not known. This together implies that the matrix entry of the closed-loop system matrix at the first row and first column should remain $A$, as in your last state space form with $\hat{x}$.
However your first state space with $\tilde{x}$ is often used to proof stability. Namely the eigenvalues of the closed-loop system matrix are equal to the combination of the eigenvalues of $A-B\,L$ and $A-K\,C$, which should both be Hurwitz.
As also stated in the answer of Chuck, your incorporation of the reference signal $r$ in the feedback law will in general not give good (steady state) tracking. Namely LQR is only good at controlling the state space to zero. If you do want good reference tracking you should look at Linear Quadratic Integral, or add feed forward.

A:

This is really a lot to look at, but the most glaring issue I noticed off the bat is your definition of the control signal $u(t)$. 
What is the input to your controller? What should be the input to your controller? What is the output of the controller?
State feedback controllers (and LQR controllers) attempt to drive the system states to zero. The control law for these controllers is:
$$
u = -Lx \\
$$
or, in the case of a state estimator (Kalman filter, EKF, etc.), is:
$$
u = -L\hat{x} \\
$$
You are attempting to implement reference tracking by passing an input $r(t)$ to the controller. With a state feedback controller, this is generally done with integral control by adding a state to the state matrix such that:
$$
\dot{e_I} = r(t) - x(n) \\
$$
where $x(n)$ is the state you're trying to control. Consider, for instance, an oscillating system (spring/mass/damper, etc.) where you're trying to control position. The states would all be the same (position/speed/acceleration), so how would your controller "know" that you're attempting to control position and not speed if you're using a control signal of $u(t) = r(t) - Kx$? 
So again, for an oscillator, your system might look like:
$$
\left[\begin{array}{c}
\dot{x} \\
\ddot{x} \\
\end{array}\right] = \left[\begin{array}{cc}
0 & 1 \\
-\omega_n^2 & 0 \\ 
\end{array}\right]\left[\begin{array}{c}
x \\
\dot{x} \\
\end{array}\right]
$$
Your integral controller then appends a state. Now, if you're trying to control position, you get:
$$
\left[\begin{array}{c}
\dot{x} \\
\ddot{x} \\
\dot{e_I} \\
\end{array}\right] = \left[\begin{array}{ccc}
0 & 1 & 0 \\
-\omega_n^2 & 0 & 0\\
-1 & 0 & 0 \\
\end{array}\right]\left[\begin{array}{c}
x \\
\dot{x} \\
e_I \\
\end{array}\right]
$$
Note the $\dot{e_I}$ state - the derivative of an integral error is just the control error. The integral error term $e_I$ doesn't impact either the $x$ or $\dot{x}$ states, but it gives you a new state for control purposes, $u(t) = -Kx$. Now you get a third gain to tune (third pole to place). 
Again, this is all specific for state feedback controllers, but the method works the same for integral control of an LQR system - this is referred to as linear-quadratic-integral control.
Now you can also clearly change which states you're trying to control by altering how the $\dot{e_I}$ term is defined. You can "pick" the position state with $[1,0]$, or the speed state with $[0,1]$, or the acceleration state by recognizing the definition from the state space form: $\ddot{x} = [-w_n^2,0][_{\dot{x}}^{x}]$.

In re-reading your question, it looks like there may be some confusion too on observer setup/design. Observer design is commonly referred to as the "dual" of the controller design because they take similar forms and you can generally do the same steps to get the answer. 
A state feedback controller attempts to drive the states to zero. With an observer, you commented:

And then...for some reason, people says that the state space model should be model by the state estimation error

Correct. The observer "states" are the errors between the modeled states and the actual system states. This is where duality comes in. A state feedback controller and an observer BOTH attempt to drive their states to zero.
In a controller, the states are the actual system states. 
In an observer, the states are the differences between actual and modeled system states. When the observer has done its job, the modeled states should be exactly equal to the actual system states. In order for the observer to have a meaningful use for the controller, the observer should converge faster than the controller such that the controller has the actual system states (or very close to) by the time the controller begins to respond. 
This is the basis for the thumb rule that the observer poles should be "faster" than the control poles (I've read and generally use 5-10 times faster). The observer will cause the modeled states to fluctuate while it converges (hunts) to the correct state measurement, so it's important for that fluctuation to happen faster than the system is able to respond so it doesn't ripple through the controller output. 
SO, that's the reasoning for observer design - it's basically a second controller that exists to drive state error to zero. 

:EDIT: - I removed the previous last section here because I mis-read $\tilde{x}$ in the first set of equations as $\hat{x}$ and then confused myself comparing the two. That said, I think the second form is still a little incorrect (assuming the first form is correct) because of how the exogenous inputs are incorporated. Working entirely from the first form, it starts as:
$$
\ \begin{bmatrix} \dot{x} \\ \\\dot{\tilde{x}} \end{bmatrix} =\begin{bmatrix} A - BL& BL \\ 0 & A-KC \end{bmatrix} \begin{bmatrix} x\\ \tilde{x} \end{bmatrix}+\begin{bmatrix} B & I & 0\\ 0 & 0 & K \end{bmatrix}\begin{bmatrix} r\\ d\\ n \end{bmatrix}\
$$
First, recognize that the state error $\tilde{x} = x-\hat{x}$, and then look just at the first row:
$$
\dot{x} = \left[A-BL\right]x + \left[BL\right]\tilde{x} + Br + Id \\
$$
For the purposes of the rest of this, I'm just going to refer to $Id$ as $d$. So, substitute in the definition of $\tilde{x}$:
$$
\dot{x} = \left[A-BL\right]x + \left[BL\right]\left(x-\hat{x}\right) + Br + d \\
$$
$$
\dot{x} = \left[A-BL\right]x + \left[BL\right]\left(x\right) + \left[BL\right]\left(-\hat{x}\right) + Br + d \\
$$
$$
\dot{x} = \left[A-BL+BL\right]x + \left[-BL\right]\hat{x} + Br + d \\
$$
$$
\boxed{\dot{x} = \left[A\right]x + \left[-BL\right]\hat{x} + Br + d} \\
$$
Great! This is the same first row as the second form. Now, to the second row of the first form:
$$
\dot{\tilde{x}} = [A-KC]\tilde{x} + Kn \\
$$
Again, make the substitutions of $\tilde{x} = x-\hat{x}$ and now also $\dot{\tilde{x}} = \dot{x} - \dot{\hat{x}}$:
$$
\dot{x} - \dot{\hat{x}}= [A-KC]\left(x-\hat{x}\right) + Kn \\
$$
Expand the $x-\hat{x}$ term:
$$
\dot{x} - \dot{\hat{x}}= [A-KC]x - [A-KC]\hat{x} + Kn \\
$$
To get rid of the $\dot{x}$ on the left hand side, just use the definition of $\dot{x}$ from the result of the first row!
$$
\dot{x} = \left[A\right]x + \left[-BL\right]\hat{x} + Br + d
$$
$$
\left[A\right]x + \left[-BL\right]\hat{x} + Br + d - \dot{\hat{x}}= [A-KC]x - [A-KC]\hat{x} + Kn \\
$$
Rearrange:
$$
\left[A\right]x + \left[-BL\right]\hat{x} + Br + d - \left([A-KC]x - [A-KC]\hat{x} + Kn\right) = \dot{\hat{x}}\\
$$
$$
\dot{\hat{x}} = \left[A\right]x + \left[-BL\right]\hat{x} + Br + d - \left([A-KC]x - [A-KC]\hat{x} + Kn\right)\\
$$
Distribute the negative:
$$
\dot{\hat{x}} = \left[A\right]x + \left[-BL\right]\hat{x} + Br + d + \left(-[A-KC]x + [A-KC]\hat{x} - Kn\right)\\
$$
$$
\dot{\hat{x}} = \left[A\right]x + \left[-BL\right]\hat{x} + Br + d - [A-KC]x + [A-KC]\hat{x} - Kn\\
$$
Rearrange again:
$$
\dot{\hat{x}} = \left[A\right]x - [A-KC]x + [A-KC]\hat{x} + \left[-BL\right]\hat{x} + Br + d - Kn\\
$$
Simplify one last time:
$$
\boxed{\dot{\hat{x}} = [KC]x + [A-BL-KC]\hat{x} + Br + d - Kn}\\
$$
This is almost, but not quite what is given as the second row of the second form. This equation has the term $d$ (really $Id$) included and also has $-Kn$ instead of $Kn$. 

