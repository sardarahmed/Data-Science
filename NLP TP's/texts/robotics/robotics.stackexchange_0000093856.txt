Q:

Autonomous docking

Hello,
As a part of my thesis I want to perform autonomous docking for my robot. I saw that xiaomi vacuum cleaner perform autonomous docking only with LIDAR. I am trying to do the same but can't understand the logic. I bought dock station and have LIDAR. Does anybody do something like this and can help me or give any suggestions?
Thanks in advance,
Regards

Originally posted by Yehor on ROS Answers with karma: 166 on 2019-11-18
Post score: 2

Original comments
Comment by MCornelis on 2019-11-18:
Maybe some kind of scan matching method? Does your docking station have some unique features that you could detect with the lidar? Maybe use something like RANSAC to fit the lidar data over a model of the docking station.
It is really hard to help you if you don't provide more information about your use-case. If the location of the docking station is static and your environment is not very dynamic you could also use AMCL to just determine your robot pose in a map and define where the docking station is on that map.
Comment by Yehor on 2019-11-19:
Yes, the dock station has some patter, it has like pits. Can you please describe what is RANSAC and how can I scan matching method? Thank you
Only AMCL is not enough((( I want to use AMCL ro reach the region of the station and then perform docking
Comment by MCornelis on 2019-11-19:
There are many methods to fit/match laser data to features/models. The way RANSAC (RANdom SAmple Consensus) works is by assuming your docking station is in a certain position, then counting how many of your datapoints (laserpoints) are on top of, or close to, your model, this will give you inliers (points that agree with your guess) and outliers (points that don't agree with your guess). You do this for an "n" amount of guesses and then you pick the one with the greatest amount of inliers (better if you also introduce some threshold where inliers/expected inliers > 0.8 or something). It is a really crude "brute-force" type method, but since you already have an initial guess from AMCL it could work. I'm not saying this is the best solution or that it is easy to implement, but it is 1 way of doing things. I would advice you to look online for laser/scan matching methods / laser object detection and see how other people tackle this problem.
Comment by MCornelis on 2019-11-19:
Additionally, if you are free to change the world/docking station/environment as you please (not sure if this is a constraint in your project) then you could consider adding a feature to the docking station yourself. If you are allowed to add something that is very easily detected by a Lidar, why go through the effort of implementing or coming up with a fancy algorithm? Don't solve a problem that you created! If there is an easier solution go for it!
Comment by Yehor on 2019-11-20:
Thank you for you suggestion, I will try to implement laser/scan matching method and if I fail I will try something else.
Thank you, for the idea)))
Comment by bfdmetu on 2020-12-28:
HÄ° Yehor. How you find any good solution? I have the same project. Using Lidar and reflectors to pose our robot. If possible , write a navigation code to autonomous docking with this pose data.
Comment by Yehor on 2020-12-28:
@bfdmetu Hi, I simply moving with navigation stack to the goal in front of the dock station on the map firstly. And then I am looking for cluster with higher intensity in front of the robot. Because, I have attached reflection material on the dock station which has mostly always higher intensity than other things around the dock station.
Comment by bfdmetu on 2020-12-28:\

How can you find higher intensity cluster. How can you use lidar data? I echo topic info /scan but these are meaningless for me :)  is there any package or something ?

Can you get robot pose info or can you update amcl_pose with this way

Comment by Yehor on 2020-12-28:
@bfdmetu

You can go through LaserScan.intensity array and find out the biggest intensity area. And after that you can find where is that area (angle) with respect to the robot.

No with that you can define the direction. However, using trigonometric you can get the x,y of the dock with respect to the robot

Comment by bfdmetu on 2020-12-28:
Where is that array ? Is it in echo /scan outputs ? Is there any documentation to use lidar-laser data.
Thank you for your answer. I will try to find intensity array and try to calculate positions.
Comment by Yehor on 2020-12-28:
@bfdmetu Check documentation about the msg. This is for LaserScan : http://docs.ros.org/en/api/sensor_msgs/html/msg/LaserScan.html
You can get the struct of the msg there. But you will also see the msg struct with echo command.

A:

The xiomi is using reflectors inside the dock
Here is a video I maked to explain it.
https://youtu.be/pZJIKH9PdpM
The magic is you getting not only the distances but also something like remission / intensity values from the sensor.
With the Tape you create unique pattern. So you could get the distance only with the remission and the angle and do not need distance.

Originally posted by duck-development with karma: 1999 on 2019-11-20
This answer was ACCEPTED on the original site
Post score: 0

Original comments
Comment by Yehor on 2019-11-20:
Yes, I saw your video, and actualy thanks you for that!!! I also broke down a station and saw that.
However, as you know the lidar LaserScan msg provides intensity as well as distance, and I have already tried to check it. The intensity was stable 2.0 with my lidar(
Comment by duck-development on 2019-11-20:
Witch lds do you use?
Comment by Yehor on 2019-11-20:
I am using ydlidar x4, this is quite simple lidar
Comment by duck-development on 2019-11-20:
You may close this question if you know how tue dock works
Comment by duck-development on 2019-11-20:
I look at the driver there seems something to bee linke intensity,
Comment by Yehor on 2019-11-26:
Yes it is there, the lidar post intensity within LaserScan msg, but the intensity is stable.

