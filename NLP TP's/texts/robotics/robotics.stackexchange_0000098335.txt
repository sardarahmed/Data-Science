Q:

Bounding Box Error in 3D Perception Stack of Autoware.Auto

Hi,
I was trying to use the 3D Perception Stack in Autoware.Auto to generate bounding boxes and I was facing some errors. Below is the attached rviz2 visualization alongside the simulation.

In the picture above, the blue circle represents the bounding box and the red circle represents filtered point clouds. There are two issues that I am trying to resolve:

The bounding boxes appear to be fish-tailing (rotating inconsistently). I have tried to change the parameters inside the .yaml configuration (param) files for the point_cloud_filter node as well but it does not seem to fix that. I have tried to look at the .urdf for the Lexus vehicle for the robot state publisher and manipulate the parameters according to how the lidar is configured, but that does not seem to fix it either. This can also be seen in the bounding boxes on the right (trees) which constantly keep changing orientation.

The bounding boxes appear to NOT line up longitudinally with the filtered point clouds. I have NOT changed anything in the .yaml config file for the Euclidean cluster, so I am confused why the bounding boxes do not line up with the filtered points. I would understand if the filtered points do not line up with the actual object in the simulation (since they can be calibrated using the sensor_height_m and the static transform parameters) but that is not the case.

The filtered points (output of thepoint_cloud_filter node) seem to lineup with the simulation meaning that the translation parameters are somewhat correct.
Let me know why these issues are happening!
Thanks,
Shlok

Originally posted by shlokgoel on ROS Answers with karma: 23 on 2021-02-17
Post score: 0

A:

Hello, @shlokgoel. To answer your two issues:

Given that we are currently only using a naive Euclidean Clustering algorithm, the orientation of the bounding box for each object can change as points are added and removed from the clusters. Frame-to-frame tracking or an ML-based algorithm would be required to better estimate object orientation. Both of these are on our roadmap for the upcoming Cargo Delivery ODD.

This is likely due to both lidar processing delays and transform delays. If you change the fixed global frame in rviz to base_link, you can offset the second type of delay. Regarding the first, we made a huge performance improvement to the ray_ground_filter just prior to the 1.0.0 release Sunday. Please pull the latest ade images by adding --update to your ade start command and test again.

Originally posted by Josh Whitley with karma: 1766 on 2021-02-23
This answer was ACCEPTED on the original site
Post score: 1

