Q:

How can I compare robot and human hands?

I want a comparison between human hands and robot hands, with respect to grasping and squeezing objects. 
How can I perform such a comparison, using sensors, as one would with a benchmarking database? Is there a standard, or ready-to-use, system?

A:

The only benchmark I know in development is at NIST.  Their Intelligent Systems Division project page should help.  On that page they discuss the specific possible metrics of grasp cycle time, grasp efficiency, finger strength, grasp strength, in-hand manipulation, object pose estimation, slip resistance, split cylinder artifact, touch sensitivity, finger force tracking, and sensor calibration.

A:

You should check out:

The Yale-CMU-Berkeley (YCB) Object and Model set.  It is designed for facilitating benchmarking in robotic manipulation. The set consists of objects of daily life with different shapes, sizes, textures, weight and rigidity, as well as some widely used manipulation tests. The set is associated with a model database which provides mesh models and high-resolution RGB-D scans of the objects for easy incorporation into manipulation and planning software platforms.
The The Yale Human Grasping Data Set.  It is a dataset of human grasping movements in unstructured environments. More than 27 hours of video with grasp, object, and task data from two housekeepers and two machinists are available.
The GraspIt! simulator.  It is a physics based simulator that can accommodate arbitrary hand and robot designs. It has a number of grasp quality metrics, but I don't know if it has a simulated human hand.

