Q:

How to start Kinect Laser data in Turtlebot without gmapping

Hello
We would like to make a program for Turtlebot to avoid obstacle using Kinect Laser data.
We launched the minimal.launch file but the topic /scan is not published.
When we launch the gmapping demo, we found that the topic /scan becomes published and we can get laser data.
The question is whether it is possible to get laser data from Kinect on Turtlebot without having to run gmapping demo.
Any help is appreciated
Thanks
Anis

Originally posted by Anis on ROS Answers with karma: 253 on 2013-04-13
Post score: 1

A:

Thanks Chad and Davesana
We tried the solution of Davesana first by executing the above launch file, but there was a problem that the /scan topic is still not published. However, we run the command
rosrun depthimage_to_laserscan depthimage_to_laserscan image:=/camera/depth/image_raw
in another terminal, and it worked.
We also tried Chad solution.
when running the command
roslaunch turtlebot_bringup 3dsensor.launch
it does not publish the /scan topic. Also, even when we executed the command
rosrun depthimage_to_laserscan depthimage_to_laserscan image:=/camera/depth/image_raw
in another terminal. It did not worked too.
We actually replaced the command 3dsensor.launch by the command
roslaunch openni_launch openni.launch
and it worked with the rosrun depthimage_to_laserscan.
Thanks
Anis

Originally posted by Anis with karma: 253 on 2013-04-14
This answer was ACCEPTED on the original site
Post score: 2

Original comments
Comment by Hemu on 2013-04-14:
Both the solutions work. However, for more accurate data, it is better to use pointcloud_to_laserscan.
Comment by Chad Rockey on 2013-04-15:
Hi Hemu, what issues are you seeing that cause inaccurate data for depthimage_to_laserscan?  It uses MUCH less CPU, and it's strongly recommended that it is used.  Please file an issue with a way to reproduce the inaccurate data: https://github.com/ros-perception/depthimage_to_laserscan
Comment by Hemu on 2013-04-15:
Hi Chad, its true that depth image processing uses much less CPU. The data inaccuracy is specific to the application. For avoiding obstacles, I used (x^2+z^2 ) as the robot mostly moves in the x-z plane (taking kinect as the reference frame) as I wanted to find the distance between robot's base and
Comment by Hemu on 2013-04-15:
an object. The depth image to laserscan provides (x^2+y^2+z^2) which is the distance between the object and the kinect and not the distance between robot's base and object. Apart from this, there is no such inaccuracy.

