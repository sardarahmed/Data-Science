Q:

openni_tracker/Transform depth

I am using the openni_tracker with a Kinect, and I have written a node to receive the transforms from it. Is there a variable that will tell me the distance between the sensor and a joint of a person being tracked?
Update:
In openni_tracker.cpp, this code is found:
void publishTransform(XnUserID const& user, XnSkeletonJoint const& joint, string const& frame_id, string const& child_frame_id) {
    static tf::TransformBroadcaster br;

    XnSkeletonJointPosition joint_position;
    g_UserGenerator.GetSkeletonCap().GetSkeletonJointPosition(user, joint, joint_position);
    double x = joint_position.position.X / 1000.0;
    double y = joint_position.position.Y / 1000.0;
    double z = joint_position.position.Z / 1000.0;

    XnSkeletonJointOrientation joint_orientation;
    g_UserGenerator.GetSkeletonCap().GetSkeletonJointOrientation(user, joint, joint_orientation);

    XnFloat* m = joint_orientation.orientation.elements;
    KDL::Rotation rotation(m[0], m[1], m[2],
                                           m[3], m[4], m[5],
                                           m[6], m[7], m[8]);
    double qx, qy, qz, qw;
    rotation.GetQuaternion(qx, qy, qz, qw);

    tf::Transform transform;
    transform.setOrigin(tf::Vector3(x, y, z));
    transform.setRotation(tf::Quaternion(qx, qy, qz, qw));
    br.sendTransform(tf::StampedTransform(transform, ros::Time::now(), frame_id, child_frame_id));
}

It appears that the Vector3 in transform is equal to joint_position.position which is supposed to be the exact position(not how much the joint has moved). Also, the output data seems to show that this is true. Therefore, I believe the problem has been solved.

Originally posted by qdocehf on ROS Answers with karma: 208 on 2011-07-15
Post score: 0

Original comments
Comment by dornhege on 2011-07-18:
Sorry, I had openni_camera in mind. Never worked with tracker, but you can test that quite easily: rosrun tf view_frames while it is running should give you the /tf tree. This should contain the skeleton frames - and maybe? a kinect frame that you can use.
Comment by qdocehf on 2011-07-18:
In what files are those topics created? I need a bit more information about them to be able to subscribe to them.
Comment by dornhege on 2011-07-15:
You just subscribe to the topics in question and yes, they would give you distances.
Comment by qdocehf on 2011-07-15:
How can I access the pointcloud or depth image topics? Would I be able to use them to measure the distance between the sensor and a person that is being tracked?
Comment by dornhege on 2011-07-15:
What do you mean with "distance from the sensor"? Kinect's distance measurements are not in transformations, but in the pointcloud or depth image topics. The distance of the transform would be the norm of its translation (if that has meaning to you).

A:

It appears that the Vector3 in transform is equal to joint_position.position which is supposed to be the exact position(not how much the joint has moved). Also, the output data seems to show that this is true.

Originally posted by qdocehf with karma: 208 on 2011-07-19
This answer was ACCEPTED on the original site
Post score: 0

