Q:

LIDAR Points as Landmarks

I am currently trying to implement a GraphSLAM/SAM algorithm for LIDAR. From papers I've read, you generate a directed graph from expected LIDAR measurements to landmarks similar to the image below (taken from the Square Root SLAM Paper by Dellaert).

However in practice the point clouds you obtain from LIDAR are similar to this (taken from the KITTI car collected dataset):

It seems algorithms such as SIFT for 3D point clouds aren't as accurate yet. Is there a commonly used technique to efficiently find features in consecutive point clouds to find landmarks for SLAM algorithms without using >30,000 points in a point cloud?

A:

When your exteroceptive sensor is a LiDAR, the most common way to perform graph-based SLAM is to use pose-graphs. The nodes in a pose-graph - as the name implies - are all poses (no features/landmarks). Instead of matching features/landmarks between scans, you match the scans themselves using some variant of ICP (of which there are many). The output of ICP gives you the relative transformation between the poses, which is an edge in the graph.
Any two poses with enough overlap between LiDAR measurements should in theory generate an edge in your pose-graph. That means sequential poses will have them, but the key to getting an accurate map is detecting when non-sequential poses (i.e., you drove the robot in a circle and have returned to the start) have overlap in their measurements. This is called a loop closure and can substantially improve the accuracy of the estimated poses by curtailing the effects of dead-reckoning.
Once you have your accurate pose-graph, you simply generate a map with the LiDAR measurements at each pose. There are many ways to do this as well, the most common being just generating a massive point cloud, or filling in an occupancy grid.

