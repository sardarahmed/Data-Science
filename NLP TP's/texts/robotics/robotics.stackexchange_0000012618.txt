Q:

Find the robot pose from three beacon measurement

At every timestep my robot gets sensor measurements from a scanner that finds three beacons with known poses $B_1 = (B_{1x}, B_{1y})^T, B_2 = (B_{2x}^T, B_{2y}) , B_3 = (B_{3x}, B_{3y})^T$ these measurements include the distance and angle to the beacon, the measuremnt for $B_1$ would be $m_{1t} = (d_{it}, \omega_{1t})^T$. and equivalently for the other beacons.
From these measurements i want to calculate the robots pose containing its position and orientaion $x_t = (p_{xt}, p_{yt}, \Theta_{xt})^T$. 
Calculating the position can be done by trilateration, but I can't seem to find a way to get the orientation of the robot from these measurements. Is there possible a model to calculate both in a single calculation?   If not a solution for finding the orientation of the robot would be great.

A:

If you have already computed the position of the robot, a single(!) angle towards a beacon is sufficient to get its orientation. 
The computation could look roughly like
// b1 is position of beacon1, r is computed position of robot
alpha = atan2(b1.x-r.x, b1.y-r.y)  
So if the robot is looking towards east, the angle-measurement of B1 should alpha. So roughly alpha-b1.w is your robot's orientation. 

