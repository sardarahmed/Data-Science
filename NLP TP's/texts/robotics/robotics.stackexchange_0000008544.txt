Q:

Pole-balancing / inverted-pendulum; is there a need for active control?

Not sure if I am posting this question in the correct community, as it relates primarily to reinforcement learning. Apologies early on if this is not so.
In reinforcement learning many algorithms exist for 'solving' the cart-pole problem; that of balancing a mass on the edge of a stick, connected to a cart on a hinge, which has 1 DoF. There is TD learning, Q-learning and many other on and off-policy methods. There is also the more recent, model-based policy search method PILCO.
What I am really wondering, I suppose is more of a physics question: is there a need for active control? Why is it not possible to find the one point for the cart, which prevents the mass to move, even incrementally, left or right as it sits atop the pole? Why does it always 'fall'?

A:

Your question might be better suited to another forum, but is definitely important for robotics in general. You are basically talking about unstable equilibrium, where there does exist a point where the pole is balanced, but any small perturbation (random unforeseen input like wind or vibration) will cause it to diverge away from that balance.
This is one reason why active control is necessary. But even if the pole were a regular pendulum hanging down (with a stable equilibrium point), you still might want active control to stop it from swinging around too much. In the end, active control is what you need if you want to change the system dynamics -- whether that means going from unstable to stable or just improving the response for desired performance.
When you consider this in the context of reinforcement learning, you can think about active control as the input you still need for certain actions to account for effects that cannot be modelled. For example, driving a car involves learned actions that you simply just "do" without any real active control, like moving your eyes to look in the rear-view mirror. However, it also involves lots of active control to react to the changing environment (otherwise you could drive around blind-folded).
Take a look at combined feedback and feed-forward control, where the former is the active control and the latter representative of learned inputs.

