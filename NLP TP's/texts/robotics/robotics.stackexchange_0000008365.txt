Q:

using a device with os instead of microcontrollers

Im working on a robot that needs image processing to analyze data recieves from cameras. 
As i searched for ARM and AVR libraries i found that there is no dip library for these micros and their limited ram is hard for image data process. i want to know is there any hardware that connects to a win or android or... devices and make it possible to that device connect to actuators and sensors?
thank you or helping.

A:

First you are right, Imageprocessing is nothing for MCUs. Mostly because of their limited hardware ressources. But it is simply not the task of those thingys. So the reason you use MCU is the control you have over the system. You can use hunderds of different interrupts, you can manipulate all timers, you have DMAs, tons of different peripherals and response times within few microseconds. The complexity and controlability of those chips can be seen by looking at the documentation of those chips. An actual one, eg out of the STM32F7-Series has 3000 pages documentation of software and registers. 
If you look at image processing, what you need there? Lot of computation power!!! You need no timers, no DMA, no responses within microseconds.
 So why use MCU for imageprocessing? It's just not there business.
For Imageprocessing you need moderen processors with huge computational power. 
The drawback of modern processors you loose all the nice stuff you like about MCUs. So you cannot control something with high frequencies, you have no access to timer registers,not a lot of peripherial things.
 Given this background and the task to solve a problem with sensors, actors and some conputer vision you can solve this taks with following approaches:
Use dedicated I/O Hardware for normal Computer 
Let your computer vision and all your control algorithms on your computer and use additional hardware to switch IOs use some buses or whatever you have to do. An example hardware would be phidgets, which work with USB; or a PCI card which is directly connected to your computer's bus system.
This provides the advantage that you only have one system, but your system's realtime capability is not given at all. The OS (Windows or Android) have no realtime capabilities at all. with some tweeks and experience you reach a jitter of 0.5ms. This is basically your idea, you mentioned in the question.
Programm MCUs for vision
Well use MCUs for things they are not made for. If you do not need lot of processing on the images this can be a viable option. Depends on your task and on the setup. Most of the algorithms are available and can be implemented with some time investment. The problem is that your are missing the computational power and the memory. So you are limited in computations per timeframe.
Apply a distributed controller concept
Take the best of both worlds. Let the MCU and the computer with decent CPU,Memory and OS what they were designed for. The only problem is communication between those modules. But there are solutions, from serial (USB) over TCP/IP(Ethernet) or even SPI. It's just you need to figure out, what are the requirements and what fits them best.
Use a system which combines the best out of each world
There are not a lot modules which are made for such applications but in the time of early development for the IoT there are some. In my personal opinion the best module is the Beaglebone Black. It has some high performance ARM core out of there cortex A series and a DSP coprocessor for the high responsiveness and low jitter. On this DSP you do not have all features you can access on a normal MCU but it maybe sufficent.
As conclusion the distributed controller is the most sophisticated and can be adjusted to meet almost any need. The other methods depend a lot on the requirements. They might be enought or maybe not. For some "I want to try something quick and dirty and i do not care about performance" use some phidgets, they are relatively cheap and are pretty easy to make them work. 

